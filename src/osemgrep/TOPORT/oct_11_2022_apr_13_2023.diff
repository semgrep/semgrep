diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/app/auth.py /home/pad/yy/cli/src/semgrep/app/auth.py
--- /tmp/semgrep/cli/src/semgrep/app/auth.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/app/auth.py	2023-03-23 12:13:38.739733114 +0100
@@ -2,21 +2,27 @@ import logging
 import sys
 from typing import Optional
 
+from boltons.iterutils import get_path
+
 from semgrep.state import get_state
 
 logger = logging.getLogger(__name__)
 
 
-def is_valid_token(token: str) -> bool:
+def get_deployment_from_token(token: str) -> Optional[str]:
     """
-    Returns true if token is valid
+    Returns the deployment name the token is for, if token is valid
     """
     state = get_state()
     r = state.app_session.get(
         f"{state.env.semgrep_url}/api/agent/deployments/current",
         headers={"Authorization": f"Bearer {token}"},
     )
-    return r.ok
+    if r.ok:
+        data = r.json()
+        return data.get("deployment", {}).get("name")  # type: ignore
+    else:
+        return None
 
 
 def get_deployment_id() -> Optional[int]:
@@ -28,12 +34,16 @@ def get_deployment_id() -> Optional[int]
     state = get_state()
     r = state.app_session.get(f"{state.env.semgrep_url}/api/agent/deployments/current")
 
-    if r.ok:
-        data = r.json()
-        return data.get("deployment", {}).get("id")  # type: ignore
-    else:
+    if not r.ok:
+        return None
+
+    deployment_id = get_path(r.json(), ("deployment", "id"))
+
+    if not isinstance(deployment_id, int):
         return None
 
+    return deployment_id
+
 
 def get_token() -> Optional[str]:
     """
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/app/scans.py /home/pad/yy/cli/src/semgrep/app/scans.py
--- /tmp/semgrep/cli/src/semgrep/app/scans.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/app/scans.py	2023-03-17 09:21:14.519881198 +0100
@@ -2,0 +2,0 @@
@@ -16,19 +18,21 @@ import requests
 from semgrep.parsing_data import ParsingData
 from semgrep.rule import Rule
 from semgrep.rule_match import RuleMatchMap
+from semgrep.semgrep_interfaces.semgrep_output_v1 import FoundDependency
 from semgrep.state import get_state
 from semgrep.verbose_logging import getLogger
 
 
 class ScanHandler:
-    def __init__(self, dry_run: bool) -> None:
+    def __init__(self, dry_run: bool = False, deployment_name: str = "") -> None:
         self._deployment_id: Optional[int] = None
-        self._deployment_name: str = ""
+        self._deployment_name: str = deployment_name
 
         self.scan_id = None
         self.ignore_patterns: List[str] = []
         self._policy_names: List[str] = []
         self._autofix = False
+        self._deepsemgrep = False
         self.dry_run = dry_run
         self._dry_run_rules_url: str = ""
         self._skipped_syntactic_ids: List[str] = []
@@ -46,56 +54,63 @@ class ScanHandler:
     @property
     def deployment_id(self) -> Optional[int]:
         """
-        Seperate property for easy of mocking in test
+        Separate property for easy of mocking in test
         """
         return self._deployment_id
 
     @property
     def deployment_name(self) -> str:
         """
-        Seperate property for easy of mocking in test
+        Separate property for easy of mocking in test
         """
         return self._deployment_name
 
     @property
     def policy_names(self) -> List[str]:
         """
-        Seperate property for easy of mocking in test
+        Separate property for easy of mocking in test
         """
         return self._policy_names
 
     @property
     def autofix(self) -> bool:
         """
-        Seperate property for easy of mocking in test
+        Separate property for easy of mocking in test
         """
         return self._autofix
 
     @property
+    def deepsemgrep(self) -> bool:
+        """
+        Separate property for easy of mocking in test
+        """
+        return self._deepsemgrep
+
+    @property
     def skipped_syntactic_ids(self) -> List[str]:
         """
-        Seperate property for easy of mocking in test
+        Separate property for easy of mocking in test
         """
         return self._skipped_syntactic_ids
 
     @property
     def skipped_match_based_ids(self) -> List[str]:
         """
-        Seperate property for easy of mocking in test
+        Separate property for easy of mocking in test
         """
         return self._skipped_match_based_ids
 
     @property
     def scan_params(self) -> str:
         """
-        Seperate property for easy of mocking in test
+        Separate property for easy of mocking in test
         """
         return self._scan_params
 
     @property
     def rules(self) -> str:
         """
-        Seperate property for easy of mocking in test
+        Separate property for easy of mocking in test
         """
         return self._rules
 
@@ -108,61 +123,21 @@ class ScanHandler:
             raise Exception(
                 f"API server at {state.env.semgrep_url} returned this error: {response.text}"
             )
-        body = response.json()
-        if isinstance(body, dict):
-            return body
-        else:
-            raise Exception(
-                f"API server at {state.env.semgrep_url} returned type '{type(response.json())}'. Expected a dictionary."
-            )
 
-    def fetch_config_and_start_scan_old(self, meta: Dict[str, Any]) -> None:
-        """
-        Get configurations for scan and create scan object
-        Backwards-compatible with versions of semgrep-app from before Aug 2022
-        TODO: Delete once all old, on-prem instances have been deprecated
-        """
-        state = get_state()
+        body = response.json()
 
-        logger.debug("Getting deployment information")
-        get_deployment_url = f"{state.env.semgrep_url}/api/agent/deployments/current"
-        body = self._get_scan_config_from_app(get_deployment_url)
-        self._deployment_id = body["deployment"]["id"]
-        self._deployment_name = body["deployment"]["name"]
-
-        logger.debug("Creating scan")
-        create_scan_url_old = (
-            f"{state.env.semgrep_url}/api/agent/deployments/{self.deployment_id}/scans"
-        )
-        response = state.app_session.post(
-            create_scan_url_old,
-            json={"meta": meta},
-        )
-        try:
-            response.raise_for_status()
-        except requests.RequestException:
+        if not isinstance(body, dict):
             raise Exception(
-                f"API server at {state.env.semgrep_url} returned this error: {response.text}"
+                f"API server at {state.env.semgrep_url} returned type '{type(body)}'. Expected a dictionary."
             )
-        body = response.json()
-        self.scan_id = body["scan"]["id"]
-        self._policy_names = body["policy"]
-        self._autofix = body.get("autofix") or False
-        self._skipped_syntactic_ids = body.get("triage_ignored_syntactic_ids") or []
-        self._skipped_match_based_ids = body.get("triage_ignored_match_based_ids") or []
 
-        logger.debug("Getting rules file")
-        get_rules_url = (
-            f"{state.env.semgrep_url}/api/agent/scans/{self.scan_id}/rules.yaml"
-        )
-        self._rules = get_rules_url
+        return body
 
     def fetch_and_init_scan_config(self, meta: Dict[str, Any]) -> None:
         """
         Get configurations for scan
         """
         state = get_state()
-        logger.debug("Getting scan configurations")
 
         self._scan_params = urlencode(
             {
@@ -173,7 +148,15 @@ class ScanHandler:
                 "semgrep_version": meta.get("semgrep_version", "0.0.0"),
             }
         )
+
+        if self.dry_run:
         app_get_config_url = f"{state.env.semgrep_url}/{DEFAULT_SEMGREP_APP_CONFIG_URL}?{self._scan_params}"
+        else:
+            app_get_config_url = f"{state.env.semgrep_url}/{DEFAULT_SEMGREP_APP_CONFIG_URL}?{self._scan_params}"
+            # TODO: uncomment the line below to replace the old endpoint with the new one once we have the
+            # CLI logic in place to ignore findings that are from old rule versions
+            # app_get_config_url = f"{state.env.semgrep_url}/api/agent/deployments/scans/{self.scan_id}/config"
+
         body = self._get_scan_config_from_app(app_get_config_url)
 
         self._deployment_id = body["deployment_id"]
@@ -181,10 +164,20 @@ class ScanHandler:
         self._policy_names = body["policy_names"]
         self._rules = body["rule_config"]
         self._autofix = body.get("autofix") or False
+        self._deepsemgrep = body.get("deepsemgrep") or False
+        self._dependency_query = body.get("dependency_query") or False
         self._skipped_syntactic_ids = body.get("triage_ignored_syntactic_ids") or []
         self._skipped_match_based_ids = body.get("triage_ignored_match_based_ids") or []
         self.ignore_patterns = body.get("ignored_files") or []
 
+        if state.terminal.is_debug:
+            config = deepcopy(body)
+            try:
+                config["rule_config"] = json.loads(config["rule_config"])
+            except Exception:
+                pass
+            logger.debug(f"Got configuration {json.dumps(config, indent=4)}")
+
     def start_scan(self, meta: Dict[str, Any]) -> None:
         """
         Get scan id and file ignores
@@ -199,7 +192,7 @@ class ScanHandler:
         logger.debug("Starting scan")
         response = state.app_session.post(
             f"{state.env.semgrep_url}/api/agent/deployments/scans",
-            json={"meta": meta, "policy_names": self._policy_names},
+            json={"meta": meta},
         )
 
         if response.status_code == 404:
@@ -253,7 +246,8 @@ class ScanHandler:
         parse_rate: ParsingData,
         total_time: float,
         commit_date: str,
-        lockfile_scan_info: Dict[str, int],
+        lockfile_dependencies: Dict[str, List[FoundDependency]],
+        engine_requested: "EngineType",
     ) -> None:
         """
         commit_date here for legacy reasons. epoch time of latest commit
@@ -266,9 +260,24 @@ class ScanHandler:
             for matches_of_rule in matches_by_rule.values()
             for match in matches_of_rule
         ]
+        # we want date stamps assigned by the app to be assigned such that the
+        # current sort by relevant_since results in findings within a given scan
+        # appear in an intuitive order.  this requires reversed ordering here.
+        all_matches.reverse()
+        sort_order = {  # used only to order rules by severity
+            "EXPERIMENT": 0,
+            "INVENTORY": 1,
+            "INFO": 2,
+            "WARNING": 3,
+            "ERROR": 4,
+        }
+        # NB: sorted guarantees stable sort, so within a given severity level
+        # issues remain sorted as before
+        all_matches = sorted(
+            all_matches, key=lambda match: sort_order[match.severity.value]
+        )
         new_ignored, new_matches = partition(
-            all_matches,
-            lambda match: bool(match.is_ignored),
+            all_matches, lambda match: bool(match.is_ignored)
         )
         findings = [
             match.to_app_finding_format(commit_date).to_json() for match in new_matches
@@ -276,23 +285,36 @@ class ScanHandler:
         ignores = [
             match.to_app_finding_format(commit_date).to_json() for match in new_ignored
         ]
+        token = (
+            # GitHub (cloud)
+            os.getenv("GITHUB_TOKEN")
+            # GitLab.com (cloud)
+            or os.getenv("GITLAB_TOKEN")
+            # Bitbucket Cloud
+            or os.getenv("BITBUCKET_TOKEN")
+        )
+
         findings_and_ignores = {
             # send a backup token in case the app is not available
-            "token": os.getenv("GITHUB_TOKEN"),
-            "gitlab_token": os.getenv("GITLAB_TOKEN"),
+            "token": token,
             "findings": findings,
-            "searched_paths": [str(t) for t in targets],
-            "renamed_paths": [str(rt) for rt in renamed_targets],
+            "searched_paths": [str(t) for t in sorted(targets)],
+            "renamed_paths": [str(rt) for rt in sorted(renamed_targets)],
             "rule_ids": rule_ids,
             "cai_ids": cai_ids,
             "ignores": ignores,
         }
 
+        if any(match.severity == RuleSeverity.EXPERIMENT for match in new_ignored):
+            logger.info("Some experimental rules were run during execution.")
+
         ignored_ext_freqs = Counter(
             [os.path.splitext(path)[1] for path in ignored_targets]
         )
         ignored_ext_freqs.pop("", None)  # don't count files with no extension
 
+        dependency_counts = {k: len(v) for k, v in lockfile_dependencies.items()}
+
         complete = {
             "exit_code": 1
             if any(match.is_blocking and not match.is_ignored for match in all_matches)
@@ -302,7 +324,7 @@ class ScanHandler:
                 "errors": [error.to_dict() for error in errors],
                 "total_time": total_time,
                 "unsupported_exts": dict(ignored_ext_freqs),
-                "lockfile_scan_info": lockfile_scan_info,
+                "lockfile_scan_info": dependency_counts,
                 "parse_rate": {
                     lang: {
                         "targets_parsed": data.num_targets - data.targets_with_errors,
@@ -312,9 +334,18 @@ class ScanHandler:
                     }
                     for (lang, data) in parse_rate.get_errors_by_lang().items()
                 },
+                "engine_requested": engine_requested.name,
             },
         }
 
+        if self._dependency_query:
+            lockfile_dependencies_json = {}
+            for path, dependencies in lockfile_dependencies.items():
+                lockfile_dependencies_json[path] = [
+                    dependency.to_json() for dependency in dependencies
+                ]
+            complete["dependencies"] = lockfile_dependencies_json
+
         if self.dry_run:
             logger.info(
                 f"Would have sent findings and ignores blob: {json.dumps(findings_and_ignores, indent=4)}"
@@ -333,20 +364,7 @@ class ScanHandler:
             f"{state.env.semgrep_url}/api/agent/scans/{self.scan_id}/findings_and_ignores",
             json=findings_and_ignores,
         )
-        # TODO: delete this once all on-prem app instances are gone
-        if (
-            response.status_code == 404
-            and state.env.semgrep_url != "https://semgrep.dev"
-        ):
-            # order matters here - findings sends back errors but ignores doesn't
-            ignores_response = state.app_session.post(
-                f"{state.env.semgrep_url}/api/agent/scans/{self.scan_id}/ignores",
-                json={"findings": ignores},
-            )
-            response = state.app_session.post(
-                f"{state.env.semgrep_url}/api/agent/scans/{self.scan_id}/findings",
-                json=findings_and_ignores,
-            )
+
         try:
             response.raise_for_status()
 
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/app/session.py /home/pad/yy/cli/src/semgrep/app/session.py
--- /tmp/semgrep/cli/src/semgrep/app/session.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/app/session.py	2023-04-12 12:55:37.489634222 +0200
@@ -1,0 +1,0 @@
@@ -77,6 +78,37 @@ class UserAgent:
         return result
 
 
+def enhance_ssl_error_message(
+    err: requests.exceptions.SSLError,
+) -> requests.exceptions.SSLError:
+    """
+    If the provided SSLError wraps a SSL hostname mismatch exception, re-create the SSLError with a more descriptive error message.
+    """
+    inner_err: Optional[Exception] = None
+
+    if err.args:
+        inner_err = err.args[0]
+
+    if isinstance(inner_err, urllib3.connectionpool.MaxRetryError) and inner_err.reason:
+        inner_err = inner_err.reason
+
+    if isinstance(inner_err, requests.exceptions.SSLError) and inner_err.args:
+        inner_err = inner_err.args[0]
+
+    if (
+        isinstance(inner_err, urllib3.connectionpool.CertificateError)
+        and inner_err.args
+        and isinstance(inner_err.args[0], str)
+        and inner_err.args[0].startswith("hostname")
+        and "doesn't match" in inner_err.args[0]
+    ):
+        return requests.exceptions.SSLError(
+            f"SSL certificate error: {inner_err.args[0]}. This error typically occurs when your internet traffic is being routed through a proxy. If this is the case, try setting the REQUESTS_CA_BUNDLE environment variable to the location of your proxy's CA certificate."
+        )
+
+    return err
+
+
 class AppSession(requests.Session):
     """
     Send requests to Semgrep App with this session.
@@ -108,6 +140,10 @@ class AppSession(requests.Session):
         super().__init__(*args, **kwargs)
         self.user_agent = UserAgent()
         self.token: Optional[str] = None
+        if os.getenv("SEMGREP_COOKIES_PATH"):
+            cookies = MozillaCookieJar(os.environ["SEMGREP_COOKIES_PATH"])
+            cookies.load()
+            self.cookies = cookies  # type: ignore
 
         # retry after 4, 8, 16 seconds
         retry_adapter = requests.adapters.HTTPAdapter(
@@ -131,6 +168,10 @@ class AppSession(requests.Session):
         metrics = get_state().metrics
         metrics.add_token(self.token)
 
+    @property
+    def is_authenticated(self) -> bool:
+        return self.token is not None
+
     def request(self, *args: Any, **kwargs: Any) -> requests.Response:
         kwargs.setdefault("timeout", 60)
         kwargs.setdefault("headers", {})
@@ -143,7 +184,11 @@ class AppSession(requests.Session):
         error_handler = get_state().error_handler
         method, url = args
         error_handler.push_request(method, url, **kwargs)
+        try:
         response = super().request(*args, **kwargs)
+        except requests.exceptions.SSLError as err:
+            raise enhance_ssl_error_message(err)
+
         if response.ok:
             error_handler.pop_request()
         else:
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/autofix.py /home/pad/yy/cli/src/semgrep/autofix.py
--- /tmp/semgrep/cli/src/semgrep/autofix.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/autofix.py	2023-01-13 13:09:55.750083251 +0100
@@ -1,4 +1,5 @@
 import re
+from functools import cmp_to_key
 from pathlib import Path
 from typing import Dict
 
@@ -130,6 +134,73 @@ def _write_contents(path: Path, contents
     path.write_text(contents)
 
 
+def matches_compare(x: RuleMatch, y: RuleMatch) -> int:
+    # If paths are not the same, just order based on that.
+    # I just need a total ordering on matches.
+    if x.path < y.path:
+        return -1
+    elif x.path > y.path:
+        return 1
+    else:
+        if x.start.offset < y.start.offset:
+            return -1
+        elif y.start.offset < x.start.offset:
+            return 1
+        else:
+            if x.end.offset < y.end.offset:
+                return -1
+            elif x.end.offset > y.end.offset:
+                return 1
+            else:
+                return 0
+
+
+def matches_overlap(x: RuleMatch, y: RuleMatch) -> bool:
+    if x.path == y.path:
+        if x.start.offset < y.start.offset:
+            return x.end.offset > y.start.offset
+        elif y.start.offset < x.start.offset:
+            return y.end.offset > x.start.offset
+        elif x.start.offset == y.start.offset:
+            return True
+
+    # If they are not from the same file, they cannot overlap.
+    return False
+
+
+def deduplicate_overlapping_matches(
+    rules_and_matches: Iterable[Tuple[Rule, OrderedRuleMatchList]]
+) -> OrderedRuleMatchList:
+
+    final_matches = []
+
+    ordered_matches = sorted(
+        (match for _, matches in rules_and_matches for match in matches),
+        key=cmp_to_key(matches_compare),
+    )
+    acc = None
+
+    for match in ordered_matches:
+        if acc is None:
+            acc = match
+            continue
+
+        if matches_overlap(acc, match):
+            logger.debug(
+                f"Two autofix matches from rules {acc.rule_id} and {match.rule_id} overlap, arbitrarily picking first one"
+            )
+            # Don't do anything, keep `acc` the same, and throw `match` out.`
+
+        else:
+            final_matches.append(acc)
+            acc = match
+
+    if acc is not None:
+        final_matches.append(acc)
+
+    return final_matches
+
+
 def apply_fixes(rule_matches_by_rule: RuleMatchMap, dryrun: bool = False) -> None:
     """
     Modify files in place for all files with findings from rules with an
@@ -137,8 +208,12 @@ def apply_fixes(rule_matches_by_rule: Ru
     """
     modified_files: Set[Path] = set()
     modified_files_offsets: Dict[Path, FileOffsets] = {}
-    for _, rule_matches in rule_matches_by_rule.items():
-        for rule_match in rule_matches:
+
+    nonoverlapping_matches = deduplicate_overlapping_matches(
+        rule_matches_by_rule.items()
+    )
+
+    for rule_match in nonoverlapping_matches:
             fix = rule_match.fix
             fix_regex = rule_match.fix_regex
             filepath = rule_match.path
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/cli.py /home/pad/yy/cli/src/semgrep/cli.py
--- /tmp/semgrep/cli/src/semgrep/cli.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/cli.py	2023-03-17 09:21:14.519881198 +0100
@@ -1,2 +1,2 @@
 
     subcommand: str = (
@@ -67,6 +82,7 @@ def cli(ctx: click.Context) -> None:
     state.app_session.authenticate()
     state.app_session.user_agent.tags.add(f"command/{subcommand}")
     state.metrics.add_feature("subcommand", subcommand)
+    state.command.set_subcommand(subcommand)
 
     maybe_set_git_safe_directories()
 
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/commands/ci.py /home/pad/yy/cli/src/semgrep/commands/ci.py
--- /tmp/semgrep/cli/src/semgrep/commands/ci.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/commands/ci.py	2023-04-12 12:55:37.489634222 +0200
@@ -1,25 +1,33 @@
+import atexit
 import os
 import sys
 import time
 from collections import defaultdict
-from contextlib import contextmanager
 from pathlib import Path
 from typing import Iterable
-from typing import Iterator
 from typing import List
 from typing import Optional
 from typing import Sequence
 from typing import Tuple
 
 import click
+from rich.padding import Padding
+from rich.progress import Progress
+from rich.progress import SpinnerColumn
+from rich.progress import TextColumn
+from rich.table import Table
 
 import semgrep.semgrep_main
 from semgrep.app import auth
 from semgrep.app.scans import ScanHandler
+from semgrep.commands.install import run_install_semgrep_pro
 from semgrep.commands.scan import CONTEXT_SETTINGS
 from semgrep.commands.scan import scan_options
 from semgrep.commands.wrapper import handle_command_errors
+from semgrep.console import console
+from semgrep.console import Title
 from semgrep.constants import OutputFormat
+from semgrep.engine import EngineType
 from semgrep.error import FATAL_EXIT_CODE
 from semgrep.error import INVALID_API_KEY_EXIT_CODE
 from semgrep.error import SemgrepError
@@ -71,8 +79,7 @@ def yield_exclude_paths(requested_patter
     yield from patterns
 
 
-@contextmanager
-def fix_head_if_github_action(metadata: GitMeta) -> Iterator[None]:
+def fix_head_if_github_action(metadata: GitMeta) -> None:
     """
     GHA can checkout the incorrect commit for a PR (it will create a fake merge commit),
     so we need to reset the head to the actual PR branch head before continuing.
@@ -80,7 +87,9 @@ def fix_head_if_github_action(metadata:
     Assumes cwd is a valid git project and that if we are in github-actions pull_request,
     metadata.head_branch_hash point to head commit of current branch
     """
-    if isinstance(metadata, GithubMeta) and metadata.is_pull_request_event:
+    if not (isinstance(metadata, GithubMeta) and metadata.is_pull_request_event):
+        return
+
         assert metadata.head_branch_hash is not None  # Not none when github action PR
         assert metadata.base_branch_hash is not None
 
@@ -90,19 +99,10 @@ def fix_head_if_github_action(metadata:
         stashed_rev = git_check_output(["git", "rev-parse", "HEAD"])
         logger.debug(f"stashed_rev: {stashed_rev}")
 
-        logger.info(
-            f"Not on head ref: {metadata.head_branch_hash}; checking that out now."
-        )
+    logger.info(f"Not on head ref: {metadata.head_branch_hash}; checking that out now.")
         git_check_output(["git", "checkout", metadata.head_branch_hash])
 
-        try:
-            yield
-        finally:
-            logger.info(f"Returning to original head revision {stashed_rev}")
-            git_check_output(["git", "checkout", stashed_rev])
-    else:
-        # Do nothing
-        yield
+    atexit.register(git_check_output, ["git", "checkout", stashed_rev])
 
 
 @click.command(context_settings=CONTEXT_SETTINGS)
@@ -169,29 +169,25 @@ def ci(
     config: Optional[Tuple[str, ...]],
     debug: bool,
     dry_run: bool,
-    emacs: bool,
     enable_nosem: bool,
     enable_version_check: bool,
     exclude: Optional[Tuple[str, ...]],
     exclude_rule: Optional[Tuple[str, ...]],
     suppress_errors: bool,
     force_color: bool,
-    gitlab_sast: bool,
-    gitlab_secrets: bool,
     include: Optional[Tuple[str, ...]],
     jobs: int,
-    json: bool,
-    junit_xml: bool,
     max_chars_per_line: int,
     max_lines_per_finding: int,
-    max_memory: int,
+    max_memory: Optional[int],
     max_target_bytes: int,
     metrics: Optional[MetricsState],
     metrics_legacy: Optional[MetricsState],
     optimizations: str,
-    dataflow_traces: bool,
+    dataflow_traces: Optional[bool],
     output: Optional[str],
-    sarif: bool,
+    output_format: OutputFormat,
+    requested_engine: EngineType,
     quiet: bool,
     rewrite_rule_ids: bool,
     supply_chain: bool,
@@ -199,9 +195,9 @@ def ci(
     time_flag: bool,
     timeout_threshold: int,
     timeout: int,
+    interfile_timeout: Optional[int],
     use_git_ignore: bool,
     verbose: bool,
-    vim: bool,
 ) -> None:
     """
     The recommended way to run semgrep in CI
@@ -216,7 +212,11 @@ def ci(
     """
     state = get_state()
     state.terminal.configure(
-        verbose=verbose, debug=debug, quiet=quiet, force_color=force_color
+        verbose=verbose,
+        debug=debug,
+        quiet=quiet,
+        force_color=force_color,
+        output_format=output_format,
     )
 
     state.metrics.configure(metrics, metrics_legacy)
@@ -230,7 +230,7 @@ def ci(
         sys.exit(INVALID_API_KEY_EXIT_CODE)
     elif not token and config:
         # Not logged in but has explicit config
-        logger.info(f"Running `semgrep ci` without API token but with configs {config}")
+        pass
     elif token and config:
         # Logged in but has explicit config
         logger.info(
@@ -238,31 +238,16 @@ def ci(
         )
         sys.exit(FATAL_EXIT_CODE)
     elif token:
-        if not auth.is_valid_token(token):
+        deployment_name = auth.get_deployment_from_token(token)
+        if not deployment_name:
             logger.info(
                 "API token not valid. Try to run `semgrep logout` and `semgrep login` again.",
             )
             sys.exit(INVALID_API_KEY_EXIT_CODE)
-        scan_handler = ScanHandler(dry_run)
+        scan_handler = ScanHandler(dry_run=dry_run, deployment_name=deployment_name)
     else:  # impossible stateâ€¦ until we break the code above
         raise RuntimeError("The token and/or config are misconfigured")
 
-    output_format = OutputFormat.TEXT
-    if json:
-        output_format = OutputFormat.JSON
-    elif gitlab_sast:
-        output_format = OutputFormat.GITLAB_SAST
-    elif gitlab_secrets:
-        output_format = OutputFormat.GITLAB_SECRETS
-    elif junit_xml:
-        output_format = OutputFormat.JUNIT_XML
-    elif sarif:
-        output_format = OutputFormat.SARIF
-    elif emacs:
-        output_format = OutputFormat.EMACS
-    elif vim:
-        output_format = OutputFormat.VIM
-
     output_settings = OutputSettings(
         output_format=output_format,
         output_destination=output,
@@ -275,44 +260,58 @@ def ci(
     output_handler = OutputHandler(output_settings)
     metadata = generate_meta_from_environment(baseline_commit)
 
-    logger.info("Scan environment:")
-    logger.info(
-        f"  versions    - semgrep {semgrep.__VERSION__} on python {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}"
-    )
-    logger.info(
-        f"  environment - running in environment {metadata.environment}, triggering event is {metadata.event_name}"
+    console.print(Title("Debugging Info"))
+    debugging_table = Table.grid(padding=(0, 1))
+    debugging_table.add_row(
+        "versions",
+        "-",
+        f"semgrep [bold]{semgrep.__VERSION__}[/bold] on python [bold]{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}[/bold]",
+    )
+    debugging_table.add_row(
+        "environment",
+        "-",
+        f"running in environment [bold]{metadata.environment}[/bold], triggering event is [bold]{metadata.event_name}[/bold]",
     )
-    if scan_handler:
-        logger.info(f"  server      - {state.env.semgrep_url}")
-    if supply_chain:
-        logger.info("  running a supply chain scan")
-    logger.info("")
 
-    try:
-        with fix_head_if_github_action(metadata):
-            if scan_handler:
-                metadata_dict = metadata.to_dict()
-                metadata_dict["is_sca_scan"] = supply_chain
+    console.print(Title("Scan Environment", order=2))
+    console.print(debugging_table, markup=True)
+
+    fix_head_if_github_action(metadata)
 
             try:
-                logger.info("Fetching configuration from semgrep.dev")
                 # Note this needs to happen within fix_head_if_github_action
                 # so that metadata of current commit is correct
                 if scan_handler:
-                    try:
+            console.print(Title("Connection", order=2))
+            metadata_dict = metadata.to_dict()
+            metadata_dict["is_sca_scan"] = supply_chain
                         proj_config = ProjectConfig.load_all()
                         metadata_dict = {**metadata_dict, **proj_config.to_dict()}
-                        scan_handler.fetch_and_init_scan_config(metadata_dict)
+            with Progress(
+                TextColumn("  {task.description}"),
+                SpinnerColumn(spinner_name="simpleDotsScrolling"),
+                console=console,
+            ) as progress_bar:
+                at_url_maybe = (
+                    f" at [bold]{state.env.semgrep_url}[/bold]"
+                    if state.env.semgrep_url != "https://semgrep.dev"
+                    else ""
+                )
+
+                start_scan_task = progress_bar.add_task(
+                    f"Reporting start of scan for [bold]{scan_handler.deployment_name}[/bold]"
+                )
                         scan_handler.start_scan(metadata_dict)
-                    except Exception as e:
-                        if (
-                            state.env.semgrep_url != "https://semgrep.dev"
-                        ):  # support old on-prem apps
-                            scan_handler.fetch_config_and_start_scan_old(metadata_dict)
-                        else:
-                            raise e
-                    logger.info(f"Authenticated as {scan_handler.deployment_name}")
+                progress_bar.update(start_scan_task, completed=100)
+
+                connection_task = progress_bar.add_task(
+                    f"Fetching configuration from Semgrep Cloud Platform{at_url_maybe}"
+                )
+                scan_handler.fetch_and_init_scan_config(metadata_dict)
+                progress_bar.update(connection_task, completed=100)
+
                     config = (scan_handler.rules,)
+
             except Exception as e:
                 import traceback
 
@@ -320,32 +319,56 @@ def ci(
                 logger.info(f"Could not start scan {e}")
                 sys.exit(FATAL_EXIT_CODE)
 
-            # Append ignores configured on semgrep.dev
-            requested_excludes = scan_handler.ignore_patterns if scan_handler else []
-            if requested_excludes:
-                logger.info(
-                    f"Adding ignore patterns configured on semgrep.dev as `--exclude` options: {exclude}"
+    engine_type = EngineType.decide_engine_type(
+        requested_engine=requested_engine,
+        scan_handler=scan_handler,
+        git_meta=metadata,
+    )
+
+    # set default settings for selected engine type
+    if dataflow_traces is None:
+        dataflow_traces = engine_type.has_dataflow_traces
+
+    if max_memory is None:
+        max_memory = engine_type.default_max_memory
+
+    if interfile_timeout is None:
+        interfile_timeout = engine_type.default_interfile_timeout
+
+    if engine_type.is_pro:
+        console.print(Padding(Title("Engine", order=2), (1, 0, 0, 0)))
+        if engine_type.check_if_installed():
+            console.print(
+                f"Using Semgrep Pro Version: [bold]{engine_type.get_pro_version()}[/bold]",
+                markup=True,
+            )
+            console.print(
+                f"Installed at [bold]{engine_type.get_binary_path()}[/bold]",
+                markup=True,
                 )
+        else:
+            run_install_semgrep_pro()
+
+    try:
+        excludes_from_app = scan_handler.ignore_patterns if scan_handler else []
 
             assert exclude is not None  # exclude is default empty tuple
-            exclude = (*exclude, *yield_exclude_paths(requested_excludes))
+        exclude = (*exclude, *yield_exclude_paths(excludes_from_app))
             assert config  # Config has to be defined here. Helping mypy out
             start = time.time()
             (
                 filtered_matches_by_rule,
                 semgrep_errors,
-                all_targets,
                 renamed_targets,
                 ignore_log,
                 filtered_rules,
                 profiler,
-                profiling_data,
-                parsing_data,
-                _explanations,
+            output_extra,
                 shown_severities,
-                lockfile_scan_info,
+            dependencies,
             ) = semgrep.semgrep_main.main(
                 core_opts_str=core_opts,
+            engine_type=engine_type,
                 output_handler=output_handler,
                 target=[os.curdir],  # semgrep ci only scans cwd
                 pattern=None,
@@ -365,10 +388,12 @@ def ci(
                 no_git_ignore=(not use_git_ignore),
                 timeout=timeout,
                 max_memory=max_memory,
+            interfile_timeout=interfile_timeout,
                 timeout_threshold=timeout_threshold,
                 skip_unknown_extensions=(not scan_unknown_extensions),
                 optimizations=optimizations,
                 baseline_commit=metadata.merge_base_ref,
+            baseline_commit_is_mergebase=True,
             )
     except SemgrepError as e:
         output_handler.handle_semgrep_errors([e])
@@ -423,13 +448,8 @@ def ci(
                 cai_matches_by_rule
                 if "r2c-internal-cai" in rule.id
                 else blocking_matches_by_rule
-                # if an SCA finding is unreachable, it always goes in non-blocking
-                if rule.is_blocking
-                and (
-                    match.extra["sca_info"].reachable
-                    if "sca_info" in match.extra
-                    else True
-                )
+                # note that SCA findings are always non-blocking
+                if rule.is_blocking and "sca_info" not in match.extra
                 else nonblocking_matches_by_rule
             )
             applicable_result_set[rule].append(match)
@@ -439,12 +459,15 @@ def ci(
 
     output_handler.output(
         {**blocking_matches_by_rule, **nonblocking_matches_by_rule},
-        all_targets=all_targets,
+        all_targets=output_extra.all_targets,
         ignore_log=ignore_log,
         profiler=profiler,
         filtered_rules=filtered_rules,
-        profiling_data=profiling_data,
+        profiling_data=output_extra.profiling_data,
         severities=shown_severities,
+        is_ci_invocation=True,
+        rules_by_engine=output_extra.rules_by_engine,
+        engine_type=engine_type,
     )
 
     logger.info("CI scan completed successfully.")
@@ -452,18 +475,27 @@ def ci(
         f"  Found {unit_str(num_blocking_findings + num_nonblocking_findings, 'finding')} ({num_blocking_findings} blocking) from {unit_str(len(blocking_rules) + len(nonblocking_rules), 'rule')}."
     )
     if scan_handler:
-        logger.info("  Uploading findings to Semgrep App.")
+        logger.info("  Uploading findings.")
         scan_handler.report_findings(
             filtered_matches_by_rule,
             semgrep_errors,
             filtered_rules,
-            all_targets,
+            output_extra.all_targets,
             renamed_targets,
             ignore_log.unsupported_lang_paths,
-            parsing_data,
+            output_extra.parsing_data,
             total_time,
             metadata.commit_datetime,
-            lockfile_scan_info,
+            dependencies,
+            engine_type,
+        )
+        logger.info("  View results in Semgrep App:")
+        logger.info(
+            f"    https://semgrep.dev/orgs/{scan_handler.deployment_name}/findings"
+        )
+        if "r2c-internal-project-depends-on" in scan_handler.rules:
+            logger.info(
+                f"    https://semgrep.dev/orgs/{scan_handler.deployment_name}/supply-chain"
         )
 
     audit_mode = metadata.event_name in audit_on
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/commands/install.py /home/pad/yy/cli/src/semgrep/commands/install.py
--- /tmp/semgrep/cli/src/semgrep/commands/install.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/commands/install.py	2023-03-17 09:21:14.529881197 +0100
@@ -1,4 +1,5 @@
 import os
+import platform
 import shutil
 import stat
 import subprocess
@@ -6,34 +7,28 @@ import sys
 from pathlib import Path
 
 import click
-from tqdm import tqdm
+from rich.progress import BarColumn
+from rich.progress import DownloadColumn
+from rich.progress import Progress
+from rich.progress import TextColumn
+from rich.progress import TimeRemainingColumn
+from rich.progress import TransferSpeedColumn
 
+from semgrep import __VERSION__
 from semgrep.commands.wrapper import handle_command_errors
+from semgrep.console import console
 from semgrep.error import FATAL_EXIT_CODE
 from semgrep.error import INVALID_API_KEY_EXIT_CODE
 from semgrep.semgrep_core import SemgrepCore
 from semgrep.state import get_state
+from semgrep.util import abort
 from semgrep.util import sub_check_output
 from semgrep.verbose_logging import getLogger
 
 logger = getLogger(__name__)
 
 
-@click.command(hidden=True)
-@handle_command_errors
-def install_deep_semgrep() -> None:
-    """
-    Install the DeepSemgrep binary (Experimental)
-
-    The binary is installed in the same directory that semgrep-core
-    is installed in. Does not reinstall if existing binary already exists.
-
-    Must be logged in and have access to DeepSemgrep beta
-    Visit https://semgrep.dev/deep-semgrep-beta for more information
-    """
-    state = get_state()
-    state.terminal.configure(verbose=False, debug=False, quiet=False, force_color=False)
-
+def determine_semgrep_pro_path() -> Path:
     core_path = SemgrepCore.path()
     if core_path is None:
         logger.info(
@@ -42,27 +37,46 @@ def install_deep_semgrep() -> None:
         logger.info("There is something wrong with your semgrep installtation")
         sys.exit(FATAL_EXIT_CODE)
 
-    deep_semgrep_path = Path(core_path).parent / "deep-semgrep"
-    if deep_semgrep_path.exists():
-        logger.info(
-            f"Overwriting DeepSemgrep binary already installed in {deep_semgrep_path}"
-        )
+    semgrep_pro_path = Path(core_path).parent / "semgrep-core-proprietary"
+    return semgrep_pro_path
+
+
+def run_install_semgrep_pro() -> None:
+    state = get_state()
+    state.terminal.configure(verbose=False, debug=False, quiet=False, force_color=False)
+
+    semgrep_pro_path = determine_semgrep_pro_path()
+
+    # TODO This is a temporary solution to help offline users
+    logger.info(f"Semgrep Pro Engine will be installed in {semgrep_pro_path}")
+
+    if semgrep_pro_path.exists():
+        logger.info(f"Overwriting Semgrep Pro Engine already installed!")
 
     if state.app_session.token is None:
         logger.info("run `semgrep login` before using `semgrep install`")
         sys.exit(INVALID_API_KEY_EXIT_CODE)
 
     if sys.platform.startswith("darwin"):
-        platform = "osx"
+        # arm64 is possible. Dunno if other arms are, so let's just check a prefix.
+        if platform.machine().startswith("arm"):
+            platform_kind = "osx-arm64"
+        else:
+            platform_kind = "osx-x86_64"
     elif sys.platform.startswith("linux"):
-        platform = "manylinux"
+        platform_kind = "manylinux"
     else:
-        platform = "manylinux"
+        platform_kind = "manylinux"
         logger.info(
             "Running on potentially unsupported platform. Installing linux compatible binary"
         )
 
-    url = f"{state.env.semgrep_url}/api/agent/deployments/deepbinary/{platform}"
+    url = f"{state.env.semgrep_url}/api/agent/deployments/deepbinary/{platform_kind}?version={__VERSION__}"
+
+    # Download the binary into a temporary location, check it, then install it.
+    # This should prevent bad installations.
+
+    semgrep_pro_path_tmp = semgrep_pro_path.with_suffix(".tmp_download")
 
     with state.app_session.get(url, timeout=60, stream=True) as r:
         if r.status_code == 401:
@@ -71,33 +85,80 @@ def install_deep_semgrep() -> None:
             )
             sys.exit(INVALID_API_KEY_EXIT_CODE)
         if r.status_code == 403:
-            logger.info("Logged in deployment does not have access to DeepSemgrep beta")
-            logger.info(
+            logger.warning(
+                "Logged in deployment does not have access to Semgrep Pro Engine beta"
+            )
+            # FIXME: Needs to be updated before launch Feb 2023
+            logger.warning(
                 "Visit https://semgrep.dev/deep-semgrep-beta for more information."
             )
             sys.exit(FATAL_EXIT_CODE)
         r.raise_for_status()
 
+        # Make sure no such binary exists. We have had weird situations when the
+        # downloaded binary was corrupted, and overwriting it did not fix it, but
+        # it was necessary to `rm -f` it.
+        if semgrep_pro_path_tmp.exists():
+            semgrep_pro_path_tmp.unlink()
+
         file_size = int(r.headers.get("Content-Length", 0))
 
-        with open(deep_semgrep_path, "wb") as f:
-            with tqdm.wrapattr(r.raw, "read", total=file_size) as r_raw:
+        with Progress(
+            TextColumn("{task.description}"),
+            BarColumn(),
+            DownloadColumn(),
+            TransferSpeedColumn(),
+            TimeRemainingColumn(),
+            console=console,
+        ) as progress, semgrep_pro_path_tmp.open("wb") as f, progress.wrap_file(
+            r.raw, total=file_size, description="Downloading..."
+        ) as r_raw:
                 shutil.copyfileobj(r_raw, f)
 
     # THINK: Do we need to give exec permissions to everybody? Can this be a security risk?
     #        The binary should not have setuid or setgid rights, so letting others
     #        execute it should not be a problem.
-    # nosemgrep: python.lang.security.audit.insecure-file-permissions.insecure-file-permissions
+    # nosemgrep: tests.precommit_dogfooding.python.lang.security.audit.insecure-file-permissions.insecure-file-permissions
     os.chmod(
-        deep_semgrep_path,
-        os.stat(deep_semgrep_path).st_mode | stat.S_IEXEC | stat.S_IXGRP | stat.S_IXOTH,
+        semgrep_pro_path_tmp,
+        os.stat(semgrep_pro_path_tmp).st_mode
+        | stat.S_IEXEC
+        | stat.S_IXGRP
+        | stat.S_IXOTH,
     )
-    logger.info(f"Installed deepsemgrep to {deep_semgrep_path}")
 
+    # Get Pro version, it serves as a simple check that the binary works
+    try:
     version = sub_check_output(
-        [str(deep_semgrep_path), "--version"],
+            [str(semgrep_pro_path_tmp), "-pro_version"],
         timeout=10,
         encoding="utf-8",
-        stderr=subprocess.DEVNULL,
+            stderr=subprocess.STDOUT,
     ).rstrip()
-    logger.info(f"DeepSemgrep Version Info: ({version})")
+    except subprocess.CalledProcessError:
+        if semgrep_pro_path_tmp.exists():
+            semgrep_pro_path_tmp.unlink()
+        abort(
+            "Downloaded binary failed version check, try again or contact support@r2c.dev"
+        )
+
+    # Version check worked so we now install the binary
+    if semgrep_pro_path.exists():
+        semgrep_pro_path.unlink()
+    semgrep_pro_path_tmp.rename(semgrep_pro_path)
+    logger.info(f"Successfully installed Semgrep Pro Engine (version {version})!")
+
+
+@click.command()
+@handle_command_errors
+def install_semgrep_pro() -> None:
+    """
+    Install the Semgrep Pro Engine
+
+    The binary is installed in the same directory that semgrep-core
+    is installed in.
+
+    Must be logged in and have access to Semgrep Pro Engine beta
+    Visit https://semgrep.dev/deep-semgrep-beta for more information
+    """
+    run_install_semgrep_pro()
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/commands/login.py /home/pad/yy/cli/src/semgrep/commands/login.py
--- /tmp/semgrep/cli/src/semgrep/commands/login.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/commands/login.py	2023-03-17 09:21:14.529881197 +0100
@@ -31,7 +31,7 @@ def login() -> NoReturn:
     """
     Obtain and save credentials for semgrep.dev
 
-    Looks for an semgrep.dev API token in the environment variable SEMGREP_API_TOKEN_SETTINGS_KEY.
+    Looks for an semgrep.dev API token in the environment variable SEMGREP_APP_TOKEN.
     If not defined and running in a TTY, prompts interactively.
     Once token is found, saves it to global settings file
     """
@@ -97,7 +97,7 @@ def login() -> NoReturn:
 
 def save_token(login_token: Optional[str], echo_token: bool) -> bool:
     state = get_state()
-    if login_token is not None and auth.is_valid_token(login_token):
+    if login_token is not None and auth.get_deployment_from_token(login_token):
         auth.set_token(login_token)
         click.echo(
             f"Saved login token\n\n\t{login_token if echo_token else '<redacted>'}\n\nin {state.settings.path}."
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/commands/scan.py /home/pad/yy/cli/src/semgrep/commands/scan.py
--- /tmp/semgrep/cli/src/semgrep/commands/scan.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/commands/scan.py	2023-03-28 09:18:43.349887686 +0200
@@ -1,4 +1,3 @@
-import multiprocessing
 import os
 import tempfile
 from itertools import chain
@@ -24,6 +23,7 @@ from semgrep import __VERSION__
 from semgrep import bytesize
 from semgrep.app.registry import list_current_public_rulesets
 from semgrep.app.version import get_no_findings_msg
+from semgrep.commands.install import determine_semgrep_pro_path
 from semgrep.commands.wrapper import handle_command_errors
 from semgrep.constants import Colors
 from semgrep.constants import DEFAULT_MAX_CHARS_PER_LINE
@@ -36,6 +36,7 @@ from semgrep.constants import OutputForm
 from semgrep.constants import RuleSeverity
 from semgrep.core_runner import CoreRunner
 from semgrep.dump_ast import dump_parsed_ast
+from semgrep.engine import EngineType
 from semgrep.error import SemgrepError
 from semgrep.metrics import MetricsState
 from semgrep.notifications import possibly_notify_user
@@ -44,6 +45,7 @@ from semgrep.output import OutputSetting
 from semgrep.project import get_project_url
 from semgrep.rule import Rule
 from semgrep.rule_match import RuleMatchMap
+from semgrep.semgrep_core import SemgrepCore
 from semgrep.semgrep_types import LANGUAGE
 from semgrep.state import get_state
 from semgrep.target_manager import write_pipes_to_disk
@@ -57,13 +59,6 @@ logger = getLogger(__name__)
 ScanReturn = Optional[Tuple[RuleMatchMap, List[SemgrepError], List[Rule], Set[Path]]]
 
 
-def __get_cpu_count() -> int:
-    try:
-        return multiprocessing.cpu_count()
-    except NotImplementedError:
-        return 1  # CPU count is not implemented on Windows
-
-
 def __validate_lang(option: str, lang: Optional[str]) -> str:
     if lang is None:
         abort(f"{option} and -l/--lang must both be specified")
@@ -254,7 +249,7 @@ _scan_options: List[Callable] = [
             Filter files or directories by path. The argument is a
             glob-style pattern such as 'foo.*' that must match the path.
             This is an extra filter in addition to other applicable filters.
-            For example, specifying the language with '-l javascript' migh
+            For example, specifying the language with '-l javascript' might
             preselect files 'src/foo.jsx' and 'lib/bar.js'. Specifying one of
             '--include=src', '--include=*.jsx', or '--include=src/foo.*'
             will restrict the selection to the single file 'src/foo.jsx'.
@@ -293,10 +288,11 @@ _scan_options: List[Callable] = [
     optgroup.option(
         "--scan-unknown-extensions/--skip-unknown-extensions",
         is_flag=True,
-        default=True,
+        default=False,
         help="""
             If true, explicit files will be scanned using the language specified in
-            --lang. If --skip-unknown-extensions, these files will not be scanned
+            --lang. If --skip-unknown-extensions, these files will not be scanned.
+            Defaults to false.
         """,
     ),
     optgroup.group("Performance and memory options"),
@@ -314,19 +310,18 @@ _scan_options: List[Callable] = [
         "-j",
         "--jobs",
         type=int,
-        default=__get_cpu_count(),
         help="""
             Number of subprocesses to use to run checks in parallel. Defaults to the
-            number of cores on the system.
+            number of cores on the system (1 if using --pro).
         """,
     ),
     optgroup.option(
         "--max-memory",
         type=int,
-        default=0,
         help="""
-            Maximum system memory to use running a rule on a single file in MB. If set to
-            0 will not have memory limit. Defaults to 0.
+            Maximum system memory to use running a rule on a single file in MiB. If set to
+            0 will not have memory limit. Defaults to 0 for all CLI scans. For CI scans
+            that use the pro engine, it defaults to 5000 MiB
         """,
     ),
     optgroup.option(
@@ -355,6 +350,16 @@ _scan_options: List[Callable] = [
             skipped. If set to 0 will not have limit. Defaults to 3.
         """,
     ),
+    # TODO: Move to Semgrep Pro Engine group ?
+    optgroup.option(
+        "--interfile-timeout",
+        type=int,
+        help=f"""
+            Maximum time to spend on interfile analysis. If set to 0 will not have
+            time limit. Defaults to 0 s for all CLI scans. For CI scans, it defaults
+            to 3 hours.
+        """,
+    ),
     optgroup.option(
         "--core-opts",
         hidden=True,
@@ -396,8 +401,9 @@ _scan_options: List[Callable] = [
     ),
     optgroup.option(
         "--dataflow-traces",
+        default=None,
         is_flag=True,
-        help="Explain how non-local values reach the location of a finding (only affects text output).",
+        help="Explain how non-local values reach the location of a finding (only affects text and SARIF output).",
     ),
     optgroup.option(
         "-o",
@@ -450,32 +456,91 @@ _scan_options: List[Callable] = [
         help="Uses ASCII output if no format specified.",
     ),
     optgroup.option(
+        "--text",
+        "output_format",
+        type=OutputFormat,
+        flag_value=OutputFormat.TEXT,
+        default=True,
+        help="Output results in Emacs single-line format.",
+    ),
+    optgroup.option(
         "--emacs",
-        is_flag=True,
+        "output_format",
+        type=OutputFormat,
+        flag_value=OutputFormat.EMACS,
         help="Output results in Emacs single-line format.",
     ),
     optgroup.option(
-        "--json", is_flag=True, help="Output results in Semgrep's JSON format."
+        "--json",
+        "output_format",
+        type=OutputFormat,
+        flag_value=OutputFormat.JSON,
+        help="Output results in Semgrep's JSON format.",
     ),
     optgroup.option(
         "--gitlab-sast",
-        is_flag=True,
+        "output_format",
+        type=OutputFormat,
+        flag_value=OutputFormat.GITLAB_SAST,
         help="Output results in GitLab SAST format.",
     ),
     optgroup.option(
         "--gitlab-secrets",
-        is_flag=True,
+        "output_format",
+        type=OutputFormat,
+        flag_value=OutputFormat.GITLAB_SECRETS,
         help="Output results in GitLab Secrets format.",
     ),
     optgroup.option(
-        "--junit-xml", is_flag=True, help="Output results in JUnit XML format."
+        "--junit-xml",
+        "output_format",
+        type=OutputFormat,
+        flag_value=OutputFormat.JUNIT_XML,
+        help="Output results in JUnit XML format.",
+    ),
+    optgroup.option(
+        "--sarif",
+        "output_format",
+        type=OutputFormat,
+        flag_value=OutputFormat.SARIF,
+        help="Output results in SARIF format.",
     ),
-    optgroup.option("--sarif", is_flag=True, help="Output results in SARIF format."),
     optgroup.option(
         "--vim",
-        is_flag=True,
+        "output_format",
+        type=OutputFormat,
+        flag_value=OutputFormat.VIM,
         help="Output results in vim single-line format.",
     ),
+    optgroup.group("Semgrep Pro Engine options"),
+    optgroup.option(
+        "--pro",
+        "requested_engine",
+        type=EngineType,
+        flag_value=EngineType.PRO_INTERFILE,
+        help="Inter-file analysis and Pro languages (currently just Apex). Requires Semgrep Pro Engine, contact support@r2c.dev for more information on this.",
+    ),
+    optgroup.option(
+        "--pro-intrafile",
+        "requested_engine",
+        type=EngineType,
+        flag_value=EngineType.PRO_INTRAFILE,
+        help="Intra-file inter-procedural taint analysis. Implies --pro-languages. Requires Semgrep Pro Engine, contact support@r2c.dev for more information on this.",
+    ),
+    optgroup.option(
+        "--pro-languages",
+        "requested_engine",
+        type=EngineType,
+        flag_value=EngineType.PRO_LANG,
+        help="Enable Pro languages (currently just Apex). Requires Semgrep Pro Engine, contact support@r2c.dev for more information on this.",
+    ),
+    optgroup.option(
+        "--oss-only",
+        "requested_engine",
+        type=EngineType,
+        flag_value=EngineType.OSS,
+        help="Run using only OSS features, even if the Semgrep Pro toggle is on.",
+    ),
 ]
 
 
@@ -595,8 +660,7 @@ def scan_options(func: Callable) -> Call
 # rely on their existence, or their output being stable
 @click.option("--dump-command-for-core", "-d", is_flag=True, hidden=True)
 @click.option(
-    "--deep",
-    "-x",
+    "--dump-engine-path",
     is_flag=True,
     hidden=True
     # help="contact support@r2c.dev for more information on this"
@@ -610,38 +674,34 @@ def scan(
     config: Optional[Tuple[str, ...]],
     core_opts: Optional[str],
     debug: bool,
-    deep: bool,
+    dump_engine_path: bool,
+    requested_engine: Optional[EngineType],
     dryrun: bool,
     dump_ast: bool,
     dump_command_for_core: bool,
-    emacs: bool,
     enable_nosem: bool,
     enable_version_check: bool,
     error_on_findings: bool,
     exclude: Optional[Tuple[str, ...]],
     exclude_rule: Optional[Tuple[str, ...]],
     force_color: bool,
-    gitlab_sast: bool,
-    gitlab_secrets: bool,
     include: Optional[Tuple[str, ...]],
-    jobs: int,
-    json: bool,
-    junit_xml: bool,
+    jobs: Optional[int],
     lang: Optional[str],
     max_chars_per_line: int,
     max_lines_per_finding: int,
-    max_memory: int,
+    max_memory: Optional[int],
     max_target_bytes: int,
     metrics: Optional[MetricsState],
     metrics_legacy: Optional[MetricsState],
     optimizations: str,
     dataflow_traces: bool,
     output: Optional[str],
+    output_format: OutputFormat,
     pattern: Optional[str],
     quiet: bool,
     replacement: Optional[str],
     rewrite_rule_ids: bool,
-    sarif: bool,
     scan_unknown_extensions: bool,
     severity: Optional[Tuple[str, ...]],
     show_supported_languages: bool,
@@ -652,11 +712,11 @@ def scan(
     time_flag: bool,
     timeout: int,
     timeout_threshold: int,
+    interfile_timeout: Optional[int],
     use_git_ignore: bool,
     validate: bool,
     verbose: bool,
     version: bool,
-    vim: bool,
 ) -> ScanReturn:
     """
     Run semgrep rules on files
@@ -673,7 +733,7 @@ def scan(
     For more information about Semgrep, go to https://semgrep.dev.
 
     NOTE: By default, Semgrep will report pseudonymous usage metrics to its server if you pull your configuration from
-    the Semgrep registy. To learn more about how and why these metrics are collected, please see
+    the Semgrep registry. To learn more about how and why these metrics are collected, please see
     https://semgrep.dev/docs/metrics. To modify this behavior, see the --metrics option below.
     """
 
@@ -689,10 +749,26 @@ def scan(
         click.echo(LANGUAGE.show_suppported_languages_message())
         return None
 
+    engine_type = EngineType.decide_engine_type(requested_engine=requested_engine)
+
+    if dump_engine_path:
+        if engine_type == EngineType.OSS:
+            print(SemgrepCore.path())
+        else:
+            print(determine_semgrep_pro_path())
+        return None
+
+    if dataflow_traces is None:
+        dataflow_traces = engine_type.has_dataflow_traces
+
     state = get_state()
     state.metrics.configure(metrics, metrics_legacy)
     state.terminal.configure(
-        verbose=verbose, debug=debug, quiet=quiet, force_color=force_color
+        verbose=verbose,
+        debug=debug,
+        quiet=quiet,
+        force_color=force_color,
+        output_format=output_format,
     )
 
     if include and exclude:
@@ -711,6 +787,12 @@ def scan(
             "Cannot create auto config when metrics are off. Please allow metrics or run with a specific config."
         )
 
+    # People have more flexibility on local scans so --max-memory and --pro-timeout is set to unlimited
+    if not max_memory:
+        max_memory = 0  # unlimited
+    if not interfile_timeout:
+        interfile_timeout = 0  # unlimited
+
     output_time = time_flag
 
     # Note this must be after the call to `terminal.configure` so that verbosity is respected
@@ -721,22 +803,6 @@ def scan(
         semgrep.config_resolver.adjust_for_docker()
         targets = (os.curdir,)
 
-    output_format = OutputFormat.TEXT
-    if json:
-        output_format = OutputFormat.JSON
-    elif gitlab_sast:
-        output_format = OutputFormat.GITLAB_SAST
-    elif gitlab_secrets:
-        output_format = OutputFormat.GITLAB_SECRETS
-    elif junit_xml:
-        output_format = OutputFormat.JUNIT_XML
-    elif sarif:
-        output_format = OutputFormat.SARIF
-    elif emacs:
-        output_format = OutputFormat.EMACS
-    elif vim:
-        output_format = OutputFormat.VIM
-
     output_settings = OutputSettings(
         output_format=output_format,
         output_destination=output,
@@ -758,9 +824,9 @@ def scan(
             config=config,
             test_ignore_todo=test_ignore_todo,
             strict=strict,
-            json=json,
+            json=output_format == OutputFormat.JSON,
             optimizations=optimizations,
-            deep=deep,
+            engine_type=engine_type,
         )
 
     run_has_findings = False
@@ -769,12 +835,25 @@ def scan(
     # 'managed_output'. Output depends on file contents so we cannot have
     # already deleted the temporary stdin file.
     with tempfile.TemporaryDirectory() as pipes_dir:
+        # mostly repeating the loop in write_pipes_to_disk to detect if we
+        # need --scan-unknown-extensions.
+        for t in targets:
+            if t == "-" or Path(t).is_fifo():
+                logger.debug("stdin or piped targets, adding --scan-unknown-extensions")
+                scan_unknown_extensions = True
+
         targets = write_pipes_to_disk(targets, Path(pipes_dir))
+
         output_handler = OutputHandler(output_settings)
         return_data: ScanReturn = None
 
         if dump_ast:
-            dump_parsed_ast(json, __validate_lang("--dump-ast", lang), pattern, targets)
+            dump_parsed_ast(
+                output_format == OutputFormat.JSON,
+                __validate_lang("--dump-ast", lang),
+                pattern,
+                targets,
+            )
         elif validate:
             if not (pattern or lang or config):
                 logger.error(
@@ -790,9 +869,11 @@ def scan(
                     try:
                         metacheck_errors = CoreRunner(
                             jobs=jobs,
+                            engine_type=engine_type,
                             timeout=timeout,
                             max_memory=max_memory,
                             timeout_threshold=timeout_threshold,
+                            interfile_timeout=interfile_timeout,
                             optimizations=optimizations,
                             core_opts_str=core_opts,
                         ).validate_configs(config)
@@ -815,20 +896,17 @@ def scan(
                 (
                     filtered_matches_by_rule,
                     semgrep_errors,
-                    all_targets,
-                    _,
+                    _renamed_targets,
                     ignore_log,
                     filtered_rules,
                     profiler,
-                    profiling_data,
-                    _,
-                    explanations,
+                    output_extra,
                     shown_severities,
-                    _,
+                    _dependencies,
                 ) = semgrep.semgrep_main.main(
                     core_opts_str=core_opts,
                     dump_command_for_core=dump_command_for_core,
-                    deep=deep,
+                    engine_type=engine_type,
                     output_handler=output_handler,
                     target=targets,
                     pattern=pattern,
@@ -849,6 +927,7 @@ def scan(
                     timeout=timeout,
                     max_memory=max_memory,
                     timeout_threshold=timeout_threshold,
+                    interfile_timeout=interfile_timeout,
                     skip_unknown_extensions=(not scan_unknown_extensions),
                     severity=severity,
                     optimizations=optimizations,
@@ -861,14 +940,16 @@ def scan(
 
             output_handler.output(
                 filtered_matches_by_rule,
-                all_targets=all_targets,
+                all_targets=output_extra.all_targets,
                 ignore_log=ignore_log,
                 profiler=profiler,
                 filtered_rules=filtered_rules,
-                profiling_data=profiling_data,
-                explanations=explanations,
+                profiling_data=output_extra.profiling_data,
+                explanations=output_extra.explanations,
+                rules_by_engine=output_extra.rules_by_engine,
                 severities=shown_severities,
                 print_summary=True,
+                engine_type=engine_type,
             )
 
             run_has_findings = any(filtered_matches_by_rule.values())
@@ -877,7 +958,7 @@ def scan(
                 filtered_matches_by_rule,
                 semgrep_errors,
                 filtered_rules,
-                all_targets,
+                output_extra.all_targets,
             )
 
     if enable_version_check:
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/commands/wrapper.py /home/pad/yy/cli/src/semgrep/commands/wrapper.py
--- /tmp/semgrep/cli/src/semgrep/commands/wrapper.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/commands/wrapper.py	2023-02-20 09:00:19.780001987 +0100
@@ -40,6 +40,11 @@ def handle_command_errors(func: Callable
             logger.exception(e)
             exit_code = FATAL_EXIT_CODE
         except SystemExit as e:
+            if e.code is None:
+                exit_code = 0
+            elif isinstance(e.code, str):
+                exit_code = FATAL_EXIT_CODE
+            else:
             exit_code = e.code
         except:  # noqa: B001
             exit_code = FATAL_EXIT_CODE
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/config_resolver.py /home/pad/yy/cli/src/semgrep/config_resolver.py
--- /tmp/semgrep/cli/src/semgrep/config_resolver.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/config_resolver.py	2023-03-31 09:10:01.999997015 +0200
@@ -18,10 +18,12 @@ from urllib.parse import urlparse
 
 import requests
 import ruamel.yaml
+from rich import progress
 from ruamel.yaml import YAMLError
 
 from semgrep import __VERSION__
 from semgrep.app import auth
+from semgrep.console import console
 from semgrep.constants import CLI_RULE_ID
 from semgrep.constants import Colors
 from semgrep.constants import DEFAULT_CONFIG_FILE
@@ -88,7 +90,12 @@ class ConfigLoader:
     _project_url = None
     _extra_headers: Dict[str, str] = {}
 
-    def __init__(self, config_str: str, project_url: Optional[str] = None) -> None:
+    def __init__(
+        self,
+        config_str: str,
+        project_url: Optional[str] = None,
+        config_str_for_jsonnet: Optional[str] = None,
+    ) -> None:
         """
         Mutates Metrics state!
         Takes a user's inputted config_str and transforms it into the appropriate
@@ -106,7 +113,7 @@ class ConfigLoader:
             self._config_path = config_str
         elif is_policy_id(config_str):
             state.metrics.add_feature("config", "policy")
-            self._config_path = url_for_policy(config_str)
+            self._config_path = url_for_policy()
         elif is_supply_chain(config_str):
             state.metrics.add_feature("config", "sca")
             self._config_path = url_for_supply_chain()
@@ -122,6 +129,11 @@ class ConfigLoader:
         else:
             state.metrics.add_feature("config", "local")
             self._origin = ConfigType.LOCAL
+            # For local imports, use the jsonnet config str
+            # if it exists
+            config_str = (
+                config_str_for_jsonnet if config_str_for_jsonnet else config_str
+            )
             self._config_path = str(Path(config_str).expanduser())
 
         if self.is_registry_url():
@@ -249,7 +261,14 @@ def parse_config_files(
     but is None for registry rules
     """
     config = {}
-    for (config_id, contents, config_path) in loaded_config_infos:
+    for config_id, contents, config_path in progress.track(
+        loaded_config_infos,
+        description=f"  parsing {len(loaded_config_infos)} rules",
+        transient=True,
+        # expected to take just 2-3 seconds with less than 500
+        disable=len(loaded_config_infos) < 500,
+        console=console,
+    ):
         try:
             if not config_id:  # registry rules don't have config ids
                 config_id = "remote-url"
@@ -521,7 +539,7 @@ def indent(msg: str) -> str:
     return "\n".join(["\t" + line for line in msg.splitlines()])
 
 
-def import_callback(base: str, path: str) -> Tuple[str, str]:
+def import_callback(base: str, path: str) -> Tuple[str, bytes]:
     """
     Instructions to jsonnet for how to resolve
     import expressions (`local $NAME = $PATH`).
@@ -531,8 +549,27 @@ def import_callback(base: str, path: str
     use it when resolving imports. By implementing
     this callback, we support yaml files (jsonnet
     can otherwise only build against json files)
-    and config specifiers like `p/python`.
+    and config specifiers like `p/python`. We also
+    support a library path
     """
+
+    # If the library path is absolute, assume that's
+    # the intended path. But if it's relative, assume
+    # it's relative to the path semgrep was called from.
+    # This follows the semantics of `jsonnet -J`
+    library_path = os.environ.get("R2C_INTERNAL_JSONNET_LIB")
+
+    if library_path and not os.path.isabs(library_path):
+        library_path = os.path.join(os.curdir, library_path)
+
+    # Assume the path is the library path if it exists,
+    # otherwise try it without the library. This way,
+    # jsonnet will give an error for the path the user
+    # likely expects
+    # TODO throw an error if neither exists?
+    if library_path and os.path.exists(os.path.join(library_path, path)):
+        final_path = os.path.join(library_path, path)
+    else:
     final_path = os.path.join(base, path)
     logger.debug(f"import_callback for {path}, base = {base}, final = {final_path}")
 
@@ -548,31 +585,21 @@ def import_callback(base: str, path: str
             data = yaml.load(fpi)
         contents = json.dumps(data)
         filename = final_path
-        return filename, contents
-
-    # This could be handled by ConfigLoader below (and its _load_config_from_local_path() helper)
-    # but this would not handle the 'base' argument yet, so better to be explicit about
-    # jsonnet handling here.
-    if final_path and (
-        final_path.split(".")[-1] == "jsonnet"
-        or final_path.split(".")[-1] == "libsonnet"
-    ):
-        logger.debug(f"loading jsonnet file {final_path}")
-        contents = Path(final_path).read_text()
-        return final_path, contents
+        return filename, contents.encode()
 
     logger.debug(f"defaulting to the config resolver for {path}")
     # Registry-aware import!
     # Can now do 'local x = import "p/python";'!!
-    # TODO? should we pass also base?
-    config_infos = ConfigLoader(path, None).load_config()
+    # Will also handle `.jsonnet` and `.libsonnet` files
+    # implicitly, since they will be resolved as local files
+    config_infos = ConfigLoader(path, None, final_path).load_config()
     if len(config_infos) == 0:
         raise SemgrepError(f"No valid configs imported")
     elif len(config_infos) > 1:
         raise SemgrepError(f"Currently configs cannot be imported from a directory")
     else:
         (_config_id, contents, config_path) = config_infos[0]
-        return config_path, contents
+        return config_path, contents.encode()
 
 
 def parse_config_string(
@@ -675,7 +702,7 @@ def registry_id_to_url(registry_id: str)
     return f"{env.semgrep_url}/{registry_id}"
 
 
-def url_for_policy(config_str: str) -> str:
+def url_for_policy() -> str:
     """
     Return url to download a policy for a given repo_name
 
Only in /home/pad/yy/cli/src/semgrep: console.py
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/constants.py /home/pad/yy/cli/src/semgrep/constants.py
--- /tmp/semgrep/cli/src/semgrep/constants.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/constants.py	2023-02-20 09:00:19.780001987 +0100
@@ -15,6 +14,8 @@ DEFAULT_CONFIG_FOLDER = f".{DEFAULT_SEMG
 DEFAULT_SEMGREP_APP_CONFIG_URL = "api/agent/deployments/scans/config"
 
 DEFAULT_TIMEOUT = 30  # seconds
+DEFAULT_PRO_TIMEOUT_CI = 10800  # seconds
+DEFAULT_MAX_MEMORY_PRO_CI = 5000  # MiB
 
 SETTINGS_FILENAME = "settings.yml"
 
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/core_output.py /home/pad/yy/cli/src/semgrep/core_output.py
--- /tmp/semgrep/cli/src/semgrep/core_output.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/core_output.py	2023-03-17 09:21:14.529881197 +0100
@@ -14,2 +14,2 @@ from typing import Set
 from typing import Tuple
 
@@ -77,10 +78,15 @@ def core_error_to_semgrep_error(err: cor
         or isinstance(err.error_type.value, core.LexicalError)
         or isinstance(err.error_type.value, core.PartialParsing)
     ):
-        code = 3
+        code = TARGET_PARSE_FAILURE_EXIT_CODE
         err = replace(err, rule_id=None)  # Rule id not important for parse errors
+    elif isinstance(err.error_type.value, core.PatternParseError):
+        # TODO This should probably be RULE_PARSE_FAILURE_EXIT_CODE
+        # but we have been exiting with FATAL_EXIT_CODE, so we need
+        # to be deliberate about changing it
+        code = FATAL_EXIT_CODE
     else:
-        code = 2
+        code = FATAL_EXIT_CODE
 
     return SemgrepCoreError(code, level, spans, err)
 
@@ -106,7 +112,7 @@ def core_matches_to_rule_matches(
     Convert core_match objects into RuleMatch objects that the rest of the codebase
     interacts with.
 
-    For now assumes that all matches encapsulated by this object are from the same rulee
+    For now assumes that all matches encapsulated by this object are from the same rule
     """
     rule_table = {rule.id: rule for rule in rules}
 
@@ -158,8 +164,10 @@ def core_matches_to_rule_matches(
         message = interpolate(rule.message, matched_values, propagated_values)
         if match.extra.rendered_fix:
             fix = match.extra.rendered_fix
+            logger.debug(f"Using AST-based autofix rendered in semgrep-core: `{fix}`")
         elif rule.fix:
             fix = interpolate(rule.fix, matched_values, propagated_values)
+            logger.debug(f"Using text-based autofix rendered in cli: `{fix}`")
         else:
             fix = None
         fix_regex = None
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/core_runner.py /home/pad/yy/cli/src/semgrep/core_runner.py
--- /tmp/semgrep/cli/src/semgrep/core_runner.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/core_runner.py	2023-04-12 12:55:37.489634222 +0200
@@ -22,32 +26,47 @@ from typing import Tuple
 from typing import TYPE_CHECKING
 
 from attr import field
 from attr import frozen
+from boltons.iterutils import get_path
+from rich import box
+from rich.columns import Columns
+from rich.padding import Padding
+from rich.progress import BarColumn
+from rich.progress import Progress
+from rich.progress import TaskID
+from rich.progress import TaskProgressColumn
+from rich.progress import TextColumn
+from rich.progress import TimeElapsedColumn
+from rich.table import Table
 from ruamel.yaml import YAML
-from tqdm import tqdm
 
 import semgrep.fork_subprocess as fork_subprocess
 import semgrep.output_from_core as core
+from semgrep.app import auth
 from semgrep.config_resolver import Config
+from semgrep.console import console
 from semgrep.constants import Colors
 from semgrep.constants import PLEASE_FILE_ISSUE_TEXT
 from semgrep.core_output import core_error_to_semgrep_error
 from semgrep.core_output import core_matches_to_rule_matches
 from semgrep.core_output import parse_core_output
+from semgrep.engine import EngineType
 from semgrep.error import SemgrepCoreError
 from semgrep.error import SemgrepError
 from semgrep.error import with_color
+from semgrep.output_extra import OutputExtra
 from semgrep.parsing_data import ParsingData
 from semgrep.profiling import ProfilingData
 from semgrep.profiling import Times
 from semgrep.rule import Rule
+from semgrep.rule import RuleProduct
 from semgrep.rule_match import OrderedRuleMatchList
 from semgrep.rule_match import RuleMatchMap
 from semgrep.semgrep_core import SemgrepCore
+from semgrep.semgrep_interfaces.semgrep_output_v1 import Ecosystem
 from semgrep.semgrep_types import Language
 from semgrep.state import get_state
 from semgrep.target_manager import TargetManager
-from semgrep.util import sub_check_output
 from semgrep.util import unit_str
 from semgrep.verbose_logging import getLogger
 
@@ -81,9 +101,16 @@ def setrlimits_preexec_fn() -> None:
     Note this is intended to run as a preexec_fn before semgrep-core in a subprocess
     so all code here runs in a child fork before os switches to semgrep-core binary
     """
+    # since this logging is inside the child core processes,
+    # which have their own output requirements so that CLI can parse its stdout,
+    # we use a different logger than the usual "semgrep" one
+    core_logger = getLogger("semgrep_core")
+
     # Get current soft and hard stack limits
     old_soft_limit, hard_limit = resource.getrlimit(resource.RLIMIT_STACK)
-    logger.info(f"Existing stack limits: Soft: {old_soft_limit}, Hard: {hard_limit}")
+    core_logger.info(
+        f"Existing stack limits: Soft: {old_soft_limit}, Hard: {hard_limit}"
+    )
 
     # Have candidates in case os unable to set certain limit
     potential_soft_limits = [
@@ -105,17 +132,19 @@ def setrlimits_preexec_fn() -> None:
     potential_soft_limits.sort(reverse=True)
     for soft_limit in potential_soft_limits:
         try:
-            logger.info(f"Trying to set soft limit to {soft_limit}")
+            core_logger.info(f"Trying to set soft limit to {soft_limit}")
             resource.setrlimit(resource.RLIMIT_STACK, (soft_limit, hard_limit))
-            logger.info(f"Successfully set stack limit to {soft_limit}, {hard_limit}")
+            core_logger.info(
+                f"Successfully set stack limit to {soft_limit}, {hard_limit}"
+            )
             return
         except Exception as e:
-            logger.info(
+            core_logger.info(
                 f"Failed to set stack limit to {soft_limit}, {hard_limit}. Trying again."
             )
-            logger.verbose(str(e))
+            core_logger.verbose(str(e))
 
-    logger.info("Failed to change stack limits")
+    core_logger.info("Failed to change stack limits")
 
 
 # This is used only to dedup errors from validate_configs(). For dedupping errors
@@ -161,7 +190,7 @@ class StreamingSemgrepCore:
     expediency in integrating
     """
 
-    def __init__(self, cmd: List[str], total: int) -> None:
+    def __init__(self, cmd: List[str], total: int, engine_type: EngineType) -> None:
         """
         cmd: semgrep-core command to run
         total: how many rules to run / how many "." we expect to see a priori
@@ -171,7 +200,9 @@ class StreamingSemgrepCore:
         self._total = total
         self._stdout = ""
         self._stderr = ""
-        self._progress_bar: Optional[tqdm] = None  # type: ignore
+        self._progress_bar: Optional[Progress] = None
+        self._progress_bar_task_id: Optional[TaskID] = None
+        self._engine_type: EngineType = engine_type
 
         # Map from file name to contents, to be checked before the real
         # file system when servicing requests from semgrep-core.
@@ -208,9 +239,58 @@ class StreamingSemgrepCore:
             [asyncio.StreamReader], Coroutine[Any, Any, bytes]
         ] = lambda s: s.readexactly(2)
         reading_json = False
+        # Read ".\n" repeatedly until we reach the JSON output.
+        # TODO: read progress from one channel and JSON data from another.
+        # or at least write/read progress as a stream of JSON objects so that
+        # we don't have to hack a parser together.
+        has_started = False
         while True:
             # blocking read if buffer doesnt contain any lines or EOF
+            try:
             line_bytes = await get_input(stream)
+            except asyncio.IncompleteReadError:
+                # happens if the data that follows a sequence of zero
+                # or more ".\n" has fewer than two bytes, such as:
+                # "", "3", ".\n.\n3", ".\n.\n.\n.", etc.
+
+                # Hack: the exact wording of parts this message may be used in metrics queries
+                # that are looking for it. Make sure `semgrep-core exited with unexpected output`
+                # and `interfile analysis` are both in the message, or talk to Emma.
+                raise SemgrepError(
+                    f"""
+                    You are seeing this because the engine was killed.
+
+                    The most common reason this happens is because it used too much memory.
+                    If your repo is large (~10k files or more), you have three options:
+                    1. Increase the amount of memory available to semgrep
+                    2. Reduce the number of jobs semgrep runs with via `-j <jobs>`. We
+                        recommend using 1 job if you are running out of memory.
+                    3. Scan the repo in parts (contact us for help)
+
+                    Otherwise, it is likely that semgrep is hitting the limit on only some
+                    files. In this case, you can try to set the limit on the amount of memory
+                    semgrep can use on each file with `--max-memory <memory>`. We recommend
+                    lowering this to a limit 70% of the available memory. For CI runs with
+                    interfile analysis, the default max-memory is 5000MB. Without, the default
+                    is unlimited.
+
+                    The last thing you can try if none of these work is to raise the stack
+                    limit with `ulimit -s <limit>`.
+
+                    If you have tried all these steps and still are seeing this error, please
+                    contact us.
+
+                       Error: semgrep-core exited with unexpected output
+                    """
+                )
+
+            if (
+                not has_started
+                and self._progress_bar
+                and self._progress_bar_task_id is not None
+            ):
+                has_started = True
+                self._progress_bar.start_task(self._progress_bar_task_id)
 
             # read returns empty when EOF
             if not line_bytes:
@@ -218,16 +298,31 @@ class StreamingSemgrepCore:
                 break
 
             if line_bytes == b".\n" and not reading_json:
-                num_scanned_targets += 1
-                if self._progress_bar:
-                    self._progress_bar.update()
+                # We expect to see 3 dots for each target, when running interfile analysis:
+                # - once when finishing phase 4, name resolution, on that target
+                # - once when finishing phase 5, taint configs, on that target
+                # - once when finishing analysis on that target as usual
+                #
+                # However, for regular OSS Semgrep, we only print one dot per
+                # target, that being the last bullet point listed above.
+                #
+                # So a dot counts as 1 progress if running Pro, but 3 progress if
+                # running the OSS engine.
+                advanced_targets = 1 if self._engine_type.is_interfile else 3
+
+                if self._progress_bar and self._progress_bar_task_id is not None:
+                    self._progress_bar.update(
+                        self._progress_bar_task_id, advance=advanced_targets
+                    )
             elif chr(line_bytes[0]).isdigit() and not reading_json:
                 if not line_bytes.endswith(b"\n"):
                     line_bytes = line_bytes + await stream.readline()
                 extra_targets = int(line_bytes)
                 num_total_targets += extra_targets
-                if self._progress_bar:
-                    self._progress_bar.total += extra_targets
+                if self._progress_bar and self._progress_bar_task_id is not None:
+                    self._progress_bar.update(
+                        self._progress_bar_task_id, total=num_total_targets
+                    )
             else:
                 stdout_lines.append(line_bytes)
                 # Once we see a non-"." char it means we are reading a large json blob
@@ -377,17 +472,24 @@ class StreamingSemgrepCore:
         open_and_ignore("/tmp/core-runner-semgrep-BEGIN")
 
         terminal = get_state().terminal
-        if (
-            sys.stderr.isatty()
-            and self._total > 1
-            and not terminal.is_quiet
-            and not terminal.is_debug
-        ):
-            # cf. for bar_format: https://tqdm.github.io/docs/tqdm/
-            self._progress_bar = tqdm(  # typing: ignore
-                total=self._total,
-                file=sys.stderr,
-                bar_format="  {l_bar}{bar}|{n_fmt}/{total_fmt} tasks",
+        with Progress(
+            # align progress bar to output by indenting 2 spaces
+            # (the +1 space comes from column gap)
+            TextColumn(" "),
+            BarColumn(),
+            TaskProgressColumn(),
+            TimeElapsedColumn(),
+            console=console,
+            disable=(
+                not sys.stderr.isatty()
+                or self._total <= 1
+                or terminal.is_quiet
+                or terminal.is_debug
+            ),
+        ) as progress_bar:
+            self._progress_bar = progress_bar
+            self._progress_bar_task_id = self._progress_bar.add_task(
+                "", total=self._total, start=False
             )
 
         if SemgrepCore.using_bridge_module():
@@ -395,9 +497,6 @@ class StreamingSemgrepCore:
         else:
             rc = asyncio.run(self._stream_exec_subprocess())
 
-        if self._progress_bar:
-            self._progress_bar.close()
-
         open_and_ignore("/tmp/core-runner-semgrep-END")
         return rc
 
@@ -406,19 +505,34 @@ class StreamingSemgrepCore:
 class Task:
     path: str = field(converter=str)
     language: Language
-    rule_nums: Sequence[int]
+    # a rule_num is the rule's index in the rule ID list
+    rule_nums: Tuple[int, ...]
+
+    @property
+    def language_label(self) -> str:
+        return (
+            "<multilang>"
+            if self.language in {Language("regex"), Language("generic")}
+            else self.language
+        )
 
 
 class TargetMappings(List[Task]):
     @property
     def rule_count(self) -> int:
-        return len({rule for task in self for rule in task.rule_nums})
+        return len({rule_num for task in self for rule_num in task.rule_nums})
 
     @property
     def file_count(self) -> int:
         return len(self)
 
 
+@define
+class TaskCounts:
+    files: int = 0
+    rules: int = 0
+
+
 class Plan:
     """
     Saves and displays knowledge of what will be run
@@ -427,73 +541,211 @@ class Plan:
     log: outputs a summary of how many files will be scanned for each file
     """
 
-    def __init__(self, mappings: List[Task], rule_ids: List[str]):
+    def __init__(
+        self,
+        mappings: List[Task],
+        rules: List[Rule],
+        *,
+        lockfiles_by_ecosystem: Optional[Dict[Ecosystem, FrozenSet[Path]]] = None,
+    ):
         self.target_mappings = TargetMappings(mappings)
         # important: this is a list of rule_ids, not a set
         # target_mappings relies on the index of each rule_id in rule_ids
-        self.rule_ids = rule_ids
+        self.rules = rules
+        self.lockfiles_by_ecosystem = lockfiles_by_ecosystem
 
+    # TODO: make this counts_by_lang_label, returning TaskCounts
     def split_by_lang_label(self) -> Dict[str, "TargetMappings"]:
         result: Dict[str, TargetMappings] = collections.defaultdict(TargetMappings)
         for task in self.target_mappings:
-            label = (
-                "<multilang>"
-                if task.language in {Language("regex"), Language("generic")}
-                else task.language
-            )
-            result[label].append(task)
+            result[task.language_label].append(task)
+        return result
+
+    @lru_cache(maxsize=1000)  # caching this saves 60+ seconds on mid-sized repos
+    def ecosystems_by_rule_nums(self, rule_nums: Tuple[int]) -> Set[Ecosystem]:
+        return {
+            ecosystem
+            for rule_num in rule_nums
+            for ecosystem in self.rules[rule_num].ecosystems
+        }
+
+    def counts_by_ecosystem(
+        self,
+    ) -> Mapping[Ecosystem, TaskCounts]:
+        result: DefaultDict[Ecosystem, TaskCounts] = collections.defaultdict(TaskCounts)
+
+        # if a pypi rule does reachability analysis on *.json files,
+        # when the user has no .json files, then there is no task for it,
+        # but we should still print it as a reachability rule we used
+        # so we get rule counts by looking at all rules
+        for rule in self.rules:
+            for ecosystem in rule.ecosystems:
+                result[ecosystem].rules += 1
+
+        # one .json file could determine the reachability of libraries from pypi and npm at the same time
+        # so one task might need increase counts for multiple ecosystems (unlike when splitting by lang)
+        for task in self.target_mappings:
+            for ecosystem in self.ecosystems_by_rule_nums(task.rule_nums):
+                result[ecosystem].files += 1
+
+        # if a rule scans npm and maven, but we only have npm lockfiles,
+        # then we skip mentioning maven in debug info by deleting maven's counts
+        if self.lockfiles_by_ecosystem is not None:
+            unused_ecosystems = {
+                ecosystem
+                for ecosystem in result
+                if not self.lockfiles_by_ecosystem.get(ecosystem)
+            }
+            for ecosystem in unused_ecosystems:
+                del result[ecosystem]
+
         return result
 
     def to_json(self) -> Dict[str, Any]:
         return {
             "target_mappings": [asdict(task) for task in self.target_mappings],
-            "rule_ids": self.rule_ids,
+            "rule_ids": [rule.id for rule in self.rules],
         }
 
     @property
     def num_targets(self) -> int:
         return len(self.target_mappings)
 
-    def log(self) -> None:
+    def table_by_language(self) -> Table:
+        table = Table(box=box.SIMPLE_HEAD, show_edge=False)
+        table.add_column("Language")
+        table.add_column("Rules", justify="right")
+        table.add_column("Files", justify="right")
+
+        plans_by_language = sorted(
+            self.split_by_lang_label().items(),
+            key=lambda x: (x[1].file_count, x[1].rule_count),
+            reverse=True,
+        )
+        for language, plan in plans_by_language:
+            table.add_row(language, str(plan.rule_count), str(plan.file_count))
+
+        return table
+
+    def table_by_ecosystem(self) -> Table:
+        table = Table(box=box.SIMPLE_HEAD, show_edge=False)
+        table.add_column("Ecosystem")
+        table.add_column("Rules", justify="right")
+        table.add_column("Files", justify="right")
+        table.add_column("Lockfiles")
+
+        counts_by_ecosystem = self.counts_by_ecosystem()
+
+        for ecosystem, plan in sorted(
+            counts_by_ecosystem.items(),
+            key=lambda x: (x[1].files, x[1].rules),
+            reverse=True,
+        ):
+            if self.lockfiles_by_ecosystem is not None:
+                lockfile_paths = ", ".join(
+                    str(lockfile)
+                    for lockfile in self.lockfiles_by_ecosystem.get(ecosystem, [])
+                )
+            else:
+                lockfile_paths = "N/A"
+
+            table.add_row(
+                ecosystem.kind,
+                str(plan.rules),
+                str(plan.files),
+                lockfile_paths,
+            )
+
+        return table
+
+    def table_by_origin(self) -> Table:
+        table = Table(box=box.SIMPLE_HEAD, show_edge=False)
+        table.add_column("Origin")
+        table.add_column("Rules", justify="right")
+
+        origin_counts = collections.Counter(
+            get_path(
+                rule.metadata, ("semgrep.dev", "rule", "origin"), default="unknown"
+            )
+            for rule in self.rules
+            if rule.product == RuleProduct.sast
+        )
+
+        for origin, count in sorted(
+            origin_counts.items(), key=lambda x: x[1], reverse=True
+        ):
+            origin_name = origin.replace("_", " ").capitalize()
+
+            table.add_row(origin_name, str(count))
+
+        return table
+
+    def table_by_sca_analysis(self) -> Table:
+        table = Table(box=box.SIMPLE_HEAD, show_edge=False)
+        table.add_column("Analysis")
+        table.add_column("Rules", justify="right")
+
+        SCA_ANALYSIS_NAMES = {
+            "reachable": "Reachability",
+            "legacy": "Basic",
+            "malicious": "Basic",
+            "upgrade-only": "Basic",
+        }
+
+        sca_analysis_counts = collections.Counter(
+            SCA_ANALYSIS_NAMES.get(rule.metadata.get("sca-kind", ""), "Unknown")
+            for rule in self.rules
+            if rule.product == RuleProduct.sca
+        )
+
+        for sca_analysis, count in sorted(
+            sca_analysis_counts.items(), key=lambda x: x[1], reverse=True
+        ):
+            sca_analysis_name = sca_analysis.replace("_", " ").title()
+
+            table.add_row(sca_analysis_name, str(count))
+
+        return table
+
+    def record_metrics(self) -> None:
         metrics = get_state().metrics
 
+        for language in self.split_by_lang_label():
+            metrics.add_feature("language", language)
+
+    def print(self, *, with_tables_for: RuleProduct) -> None:
         if self.target_mappings.rule_count == 0:
-            logger.info("Nothing to scan.")
+            console.print("Nothing to scan.")
             return
 
         if self.target_mappings.rule_count == 1:
-            logger.info(f"Scanning {unit_str(len(self.target_mappings), 'file')}.")
+            console.print(f"Scanning {unit_str(len(self.target_mappings), 'file')}.")
             return
 
-        plans_by_language = sorted(
-            self.split_by_lang_label().items(),
-            key=lambda x: (x[1].file_count, x[1].rule_count),
-            reverse=True,
-        )
-        if len(plans_by_language) == 1:
-            logger.info(
-                f"Scanning {unit_str(self.target_mappings.file_count, 'file')} with {unit_str(self.target_mappings.rule_count, f'{plans_by_language[0][0]} rule')}."
+        if len(self.split_by_lang_label()) == 1:
+            console.print(
+                f"Scanning {unit_str(self.target_mappings.file_count, 'file')} with {unit_str(self.target_mappings.rule_count, f'{list(self.split_by_lang_label())[0]} rule')}."
             )
             return
 
-        logger.info("\nScanning across multiple languages:")
-        for language, plan in plans_by_language:
-            metrics.add_feature("language", language)
+        if with_tables_for == RuleProduct.sast:
+            tables = [
+                self.table_by_language(),
+                self.table_by_origin(),
+            ]
+        elif with_tables_for == RuleProduct.sca:
+            tables = [
+                self.table_by_ecosystem(),
+                self.table_by_sca_analysis(),
+            ]
+        else:
+            tables = []
 
-            lang_chars = max(len(lang) for lang, _ in plans_by_language)
-            rules_chars = max(
-                len(str(plan.rule_count)) for _, plan in plans_by_language
-            ) + len(" rules")
-            files_chars = max(
-                len(str(plan.file_count)) for _, plan in plans_by_language
-            ) + len(" files")
-
-            lang_field = language.rjust(lang_chars)
-            rules_field = unit_str(plan.rule_count, "rule", pad=True).rjust(rules_chars)
-            files_field = unit_str(plan.file_count, "file", pad=True).rjust(files_chars)
+        columns = Columns(tables, padding=(1, 8))
 
-            logger.info(f"    {lang_field} | {rules_field} Ã— {files_field}")
-        logger.info("")
+        # rich tables are 2 spaces indented by default
+        # deindent only by 1 to align the content, instead of the invisible table border
+        console.print(Padding(columns, (1, 0)), deindent=1)
 
     def __str__(self) -> str:
         return f"<Plan of {len(self.target_mappings)} tasks for {list(self.split_by_lang_label())}>"
@@ -508,17 +760,22 @@ class CoreRunner:
 
     def __init__(
         self,
-        jobs: int,
+        jobs: Optional[int],
+        engine_type: EngineType,
         timeout: int,
         max_memory: int,
         timeout_threshold: int,
+        interfile_timeout: int,
         optimizations: str,
         core_opts_str: Optional[str],
     ):
-        self._jobs = jobs
+        self._binary_path = engine_type.get_binary_path()
+        self._jobs = jobs or engine_type.default_jobs
+        self._engine_type = engine_type
         self._timeout = timeout
         self._max_memory = max_memory
         self._timeout_threshold = timeout_threshold
+        self._interfile_timeout = interfile_timeout
         self._optimizations = optimizations
         self._core_opts = shlex.split(core_opts_str) if core_opts_str else []
 
@@ -660,8 +917,22 @@ class CoreRunner:
             }
             profiling_data.set_file_times(Path(t.path), rule_timings, t.run_time)
 
-    def _plan_core_run(
-        self, rules: List[Rule], target_manager: TargetManager, all_targets: Set[Path]
+    def _add_max_memory_bytes(
+        self, profiling_data: ProfilingData, max_memory_bytes: int
+    ) -> None:
+        """
+        This represents the maximum amount of memory used by the OCaml side of
+        Semgrep during its execution.
+
+        This is useful for telemetry purposes.
+        """
+        profiling_data.set_max_memory_bytes(max_memory_bytes)
+
+    @staticmethod
+    def plan_core_run(
+        rules: List[Rule],
+        target_manager: TargetManager,
+        all_targets: Optional[Set[Path]] = None,
     ) -> Plan:
         """
         Gets the targets to run for each rule
@@ -669,7 +940,7 @@ class CoreRunner:
         Returns this information as a list of rule ids and a list of targets with
         language + index of the rule ids for the rules to run each target on.
         Semgrep-core will use this to determine what to run (see Input_to_core.atd).
-        Also updates all_targets, used by core_runner
+        Also updates all_targets if set, used by core_runner
 
         Note: this is a list because a target can appear twice (e.g. Java + Generic)
         """
@@ -677,7 +948,17 @@ class CoreRunner:
             list
         )
 
-        for i, rule in enumerate(rules):
+        lockfiles = target_manager.get_all_lockfiles()
+
+        rules = [
+            rule
+            for rule in rules
+            # filter out SCA rules with no relevant lockfiles
+            if rule.product != RuleProduct.sca
+            or any(lockfiles[ecosystem] for ecosystem in rule.ecosystems)
+        ]
+
+        for rule_num, rule in enumerate(rules):
             for language in rule.languages:
                 targets = list(
                     target_manager.get_files_for_rule(
@@ -686,35 +967,31 @@ class CoreRunner:
                 )
 
                 for target in targets:
+                    if all_targets is not None:
                     all_targets.add(target)
-                    target_info[target, language].append(i)
+                    target_info[target, language].append(rule_num)
 
         return Plan(
             [
                 Task(
                     path=target,
                     language=language,
-                    rule_nums=target_info[target, language],
+                    # tuple conversion makes rule_nums hashable, so usable as cache key
+                    rule_nums=tuple(target_info[target, language]),
                 )
                 for target, language in target_info
             ],
-            [rule.id for rule in rules],
+            rules,
+            lockfiles_by_ecosystem=lockfiles,
         )
 
-    def _run_rules_direct_to_semgrep_core(
+    def _run_rules_direct_to_semgrep_core_helper(
         self,
         rules: List[Rule],
         target_manager: TargetManager,
         dump_command_for_core: bool,
-        deep: bool,
-    ) -> Tuple[
-        RuleMatchMap,
-        List[SemgrepError],
-        Set[Path],
-        ProfilingData,
-        ParsingData,
-        Optional[List[core.MatchingExplanation]],
-    ]:
+        engine: EngineType,
+    ) -> Tuple[RuleMatchMap, List[SemgrepError], OutputExtra,]:
         state = get_state()
         logger.debug(f"Passing whole rules directly to semgrep_core")
 
@@ -723,6 +1000,7 @@ class CoreRunner:
         all_targets: Set[Path] = set()
         file_timeouts: Dict[Path, int] = collections.defaultdict(lambda: 0)
         max_timeout_files: Set[Path] = set()
+        # TODO this is a quick fix, refactor this logic
 
         profiling_data: ProfilingData = ProfilingData()
         parsing_data: ParsingData = ParsingData()
@@ -746,9 +1024,9 @@ class CoreRunner:
         )
 
         with exit_stack:
+            plan = self.plan_core_run(rules, target_manager, all_targets)
+            plan.record_metrics()
 
-            plan = self._plan_core_run(rules, target_manager, all_targets)
-            plan.log()
             parsing_data.add_targets(plan)
             target_file_contents = json.dumps(plan.to_json())
             target_file.write(target_file_contents)
@@ -768,7 +1046,8 @@ class CoreRunner:
             }
 
             # Run semgrep
-            cmd = [SemgrepCore.path()] + [
+            cmd = [
+                str(self._binary_path),
                 "-json",
                 "-rules",
                 rule_file.name,
@@ -796,53 +1075,37 @@ class CoreRunner:
 
             # TODO: use exact same command-line arguments so just
             # need to replace the SemgrepCore.path() part.
-            if deep:
+            if engine.is_pro:
+                if auth.get_token() is None:
                 logger.error("!!!This is a proprietary extension of semgrep.!!!")
                 logger.error("!!!You must be logged in to access this extension!!!")
+                else:
+                    if engine is EngineType.PRO_INTERFILE:
+                        logger.error(
+                            "Semgrep Pro Engine may be slower and show different results than Semgrep OSS."
+                        )
+
+                if engine is EngineType.PRO_INTERFILE:
                 targets = target_manager.targets
                 if len(targets) == 1 and targets[0].path.is_dir():
                     root = str(targets[0].path)
                 else:
+                        # TODO: This is no longer true...
                     raise SemgrepError("deep mode needs a single target (root) dir")
-
-                deep_path = SemgrepCore.deep_path()
-                if deep_path is None:
-                    raise SemgrepError(
-                        "Could not run deep analysis: DeepSemgrep not installed. Run `semgrep install-deep-semgrep`"
-                    )
-
-                logger.info(f"Using DeepSemgrep installed in {deep_path}")
-                version = sub_check_output(
-                    [deep_path, "--version"],
-                    timeout=10,
-                    encoding="utf-8",
-                    stderr=subprocess.DEVNULL,
-                ).rstrip()
-                logger.info(f"DeepSemgrep Version Info: ({version})")
-
-                cmd = [deep_path] + [
-                    "--json",
-                    "--rules",
-                    rule_file.name,
-                    "-j",
-                    str(self._jobs),
-                    "--targets",
-                    target_file.name,
-                    "--root",
-                    root,
-                    # "--timeout",
-                    # str(self._timeout),
-                    # "--timeout_threshold",
-                    # str(self._timeout_threshold),
-                    # "--max_memory",
-                    # str(self._max_memory),
+                    cmd += ["-deep_inter_file"]
+                    cmd += [
+                        "-timeout_for_interfile_analysis",
+                        str(self._interfile_timeout),
                 ]
+                    cmd += [root]
+                elif engine is EngineType.PRO_INTRAFILE:
+                    cmd += ["-deep_intra_file"]
 
             stderr: Optional[int] = subprocess.PIPE
             if state.terminal.is_debug:
                 cmd += ["--debug"]
 
-            logger.debug("Running semgrep-core with command:")
+            logger.debug("Running Semgrep engine with command:")
             logger.debug(" ".join(cmd))
 
             if dump_command_for_core:
@@ -851,11 +1114,13 @@ class CoreRunner:
                 # to copy+paste it to a shell.  (The real command is
                 # still visible in the log message above.)
                 printed_cmd = cmd.copy()
-                printed_cmd[0] = SemgrepCore.executable_path()
+                printed_cmd[0] = str(self._binary_path)
                 print(" ".join(printed_cmd))
                 sys.exit(0)
 
-            runner = StreamingSemgrepCore(cmd, plan.num_targets)
+            # Multiplied by three, because we have three places in Pro Engine to
+            # report progress, versus one for OSS Engine.
+            runner = StreamingSemgrepCore(cmd, plan.num_targets * 3, engine)
             runner.vfs_map = vfs_map
             returncode = runner.execute()
 
@@ -871,6 +1136,9 @@ class CoreRunner:
 
             if ("time" in output_json) and core_output.time:
                 self._add_match_times(profiling_data, core_output.time)
+                self._add_max_memory_bytes(
+                    profiling_data, core_output.time.max_memory_bytes
+                )
 
             # end with tempfile.NamedTemporaryFile(...) ...
             outputs = core_matches_to_rule_matches(rules, core_output)
@@ -899,14 +1167,61 @@ class CoreRunner:
                     parsing_data.add_error(err)
             errors.extend(parsed_errors)
 
-        return (
-            outputs,
-            errors,
+        output_extra = OutputExtra(
             all_targets,
             profiling_data,
             parsing_data,
             core_output.explanations,
+            core_output.rules_by_engine,
+        )
+
+        return (
+            outputs,
+            errors,
+            output_extra,
+        )
+
+    def _run_rules_direct_to_semgrep_core(
+        self,
+        rules: List[Rule],
+        target_manager: TargetManager,
+        dump_command_for_core: bool,
+        engine: EngineType,
+    ) -> Tuple[RuleMatchMap, List[SemgrepError], OutputExtra,]:
+        """
+        Sometimes we may run into synchronicity issues with the latest DeepSemgrep binary.
+        These issues may possibly cause a failure if a user, for instance, updates their
+        version of Semgrep, but does not update to the latest version of DeepSemgrep.
+
+        A short bandaid solution for now is to suggest that a user updates to the latest
+        version, if the DeepSemgrep binary crashes for any reason.
+        """
+        try:
+            return self._run_rules_direct_to_semgrep_core_helper(
+                rules, target_manager, dump_command_for_core, engine
+            )
+        except SemgrepError as e:
+            # Handle Semgrep errors normally
+            raise e
+        except Exception as e:
+            # Unexpected error, output a warning that the engine might be out of date
+            if engine.is_pro:
+                logger.error(
+                    f"""
+
+Semgrep Pro crashed during execution (unknown reason).
+This can sometimes happen because either Semgrep Pro or Semgrep is out of date.
+
+Try updating your version of Semgrep Pro (`semgrep install-semgrep-pro`) or your version of Semgrep (`pip install semgrep/brew install semgrep`).
+If both are up-to-date and the crash persists, please contact support to report an issue!
+When reporting the issue, please re-run the semgrep command with the
+`--debug` flag so as to print more details about what happened, if you can.
+
+Exception raised: `{e}`
+                    """
         )
+                sys.exit(2)
+            raise e
 
     # end _run_rules_direct_to_semgrep_core
 
@@ -915,15 +1230,8 @@ class CoreRunner:
         target_manager: TargetManager,
         rules: List[Rule],
         dump_command_for_core: bool,
-        deep: bool,
-    ) -> Tuple[
-        RuleMatchMap,
-        List[SemgrepError],
-        Set[Path],
-        ProfilingData,
-        ParsingData,
-        Optional[List[core.MatchingExplanation]],
-    ]:
+        engine: EngineType,
+    ) -> Tuple[RuleMatchMap, List[SemgrepError], OutputExtra,]:
         """
         Takes in rules and targets and retuns object with findings
         """
@@ -932,16 +1240,13 @@ class CoreRunner:
         (
             findings_by_rule,
             errors,
-            all_targets,
-            profiling_data,
-            parsing_data,
-            explanations,
+            output_extra,
         ) = self._run_rules_direct_to_semgrep_core(
-            rules, target_manager, dump_command_for_core, deep
+            rules, target_manager, dump_command_for_core, engine
         )
 
         logger.debug(
-            f"semgrep ran in {datetime.now() - start} on {len(all_targets)} files"
+            f"semgrep ran in {datetime.now() - start} on {len(output_extra.all_targets)} files"
         )
         by_severity = collections.defaultdict(list)
         for rule, findings in findings_by_rule.items():
@@ -955,13 +1260,13 @@ class CoreRunner:
         return (
             findings_by_rule,
             errors,
-            all_targets,
-            profiling_data,
-            parsing_data,
-            explanations,
+            output_extra,
         )
 
     def validate_configs(self, configs: Tuple[str, ...]) -> Sequence[SemgrepError]:
+        if self._binary_path is None:  # should never happen, doing this for mypy
+            raise SemgrepError("semgrep engine not found.")
+
         metachecks = Config.from_config_list(["p/semgrep-rule-lints"], None)[
             0
         ].get_rules(True)
@@ -969,33 +1274,27 @@ class CoreRunner:
         parsed_errors = []
 
         with tempfile.NamedTemporaryFile("w", suffix=".yaml") as rule_file:
-
             yaml = YAML()
             yaml.dump(
                 {"rules": [metacheck._raw for metacheck in metachecks]}, rule_file
             )
             rule_file.flush()
 
-            cmd = (
-                [SemgrepCore.path()]
-                + [
+            cmd = [
+                str(self._binary_path),
                     "-json",
                     "-check_rules",
                     rule_file.name,
+                *configs,
                 ]
-                + list(configs)
-            )
 
-            runner = StreamingSemgrepCore(cmd, 1)  # only scanning combined rules
+            # only scanning combined rules
+            runner = StreamingSemgrepCore(cmd, 1, self._engine_type)
             returncode = runner.execute()
 
             # Process output
             output_json = self._extract_core_output(
-                metachecks,
-                returncode,
-                " ".join(cmd),
-                runner.stdout,
-                runner.stderr,
+                metachecks, returncode, " ".join(cmd), runner.stdout, runner.stderr
             )
             core_output = parse_core_output(output_json)
 
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/dependency_aware_rule.py /home/pad/yy/cli/src/semgrep/dependency_aware_rule.py
--- /tmp/semgrep/cli/src/semgrep/dependency_aware_rule.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/dependency_aware_rule.py	2023-02-21 17:27:23.810182962 +0100
@@ -1,9 +1,11 @@
 
 import semgrep.output_from_core as core
-from semdep.find_lockfiles import find_single_lockfile
+from semdep.external.packaging.specifiers import InvalidSpecifier  # type: ignore
+from semdep.external.packaging.specifiers import SpecifierSet  # type: ignore
 from semdep.package_restrictions import dependencies_range_match_any
+from semdep.parse_lockfile import parse_lockfile_path
 from semgrep.error import SemgrepError
 from semgrep.rule import Rule
 from semgrep.rule_match import RuleMatch
 
 SCA_FINDING_SCHEMA = 20220913
@@ -40,14 +47,24 @@ def parse_depends_on_yaml(entries: List[
         semver_range = entry.get("version")
         if semver_range is None:
             raise SemgrepError(f"project-depends-on is missing `version`")
+        try:
+            SpecifierSet(semver_range)
+        except InvalidSpecifier:
+            raise SemgrepError(f"invalid semver range {semver_range}")
+
         yield DependencyPattern(
             ecosystem=ecosystem, package=package, semver_range=semver_range
         )
 
 
 def generate_unreachable_sca_findings(
-    rule: Rule, target_manager: TargetManager, exlcude: Set[Path]
-) -> Tuple[List[RuleMatch], List[SemgrepError], Set[Path]]:
+    rule: Rule,
+    target_manager: TargetManager,
+    already_reachable: Callable[[Path, FoundDependency], bool],
+) -> Tuple[List[RuleMatch], List[SemgrepError]]:
+    """
+    Returns matches to a only a rule's sca-depends-on patterns; ignoring any reachabiliy patterns it has
+    """
     depends_on_keys = rule.project_depends_on
     dep_rule_errors: List[SemgrepError] = []
 
@@ -55,17 +72,17 @@ def generate_unreachable_sca_findings(
     ecosystems = list(rule.ecosystems)
 
     non_reachable_matches = []
-    targeted_lockfiles = set()
     for ecosystem in ecosystems:
-        lockfile_data = target_manager.get_lockfile_dependencies(ecosystem)
-        for lockfile_path, deps in lockfile_data.items():
-            if lockfile_path in exlcude:
-                continue
-            targeted_lockfiles.add(lockfile_path)
+        lockfile_paths = target_manager.get_lockfiles(ecosystem)
+
+        for lockfile_path in lockfile_paths:
+            deps = parse_lockfile_path(lockfile_path)
             dependency_matches = list(
                 dependencies_range_match_any(depends_on_entries, list(deps))
             )
             for dep_pat, found_dep in dependency_matches:
+                if already_reachable(lockfile_path, found_dep):
+                    continue
                 dep_match = DependencyMatch(
                     dependency_pattern=dep_pat,
                     found_dependency=found_dep,
@@ -86,7 +103,10 @@ def generate_unreachable_sca_findings(
                         ),
                         # TODO: we need to define the fields below in
                         # Output_from_core.atd so we can reuse core.MatchExtra
-                        extra=core.CoreMatchExtra(metavars=core.Metavars({})),
+                        extra=core.CoreMatchExtra(
+                            metavars=core.Metavars({}),
+                            engine_kind=core.EngineKind(core.OSS()),
+                        ),
                     ),
                     extra={
                         "sca_info": ScaInfo(
@@ -98,12 +118,25 @@ def generate_unreachable_sca_findings(
                     },
                 )
                 non_reachable_matches.append(match)
-    return non_reachable_matches, dep_rule_errors, targeted_lockfiles
+    return non_reachable_matches, dep_rule_errors
+
+
+@lru_cache(maxsize=100_000)
+def transivite_dep_is_also_direct(
+    package: str, deps: Tuple[Tuple[str, Transitivity], ...]
+) -> bool:
+    """
+    Assumes that [dep] is transitive
+    Checks if there is a direct version of the transitive dependency [dep]
+    """
+    return (package, Transitivity(Direct())) in deps
 
 
 def generate_reachable_sca_findings(
     matches: List[RuleMatch], rule: Rule, target_manager: TargetManager
-) -> Tuple[List[RuleMatch], List[SemgrepError], Set[Path]]:
+) -> Tuple[
+    List[RuleMatch], List[SemgrepError], Callable[[Path, FoundDependency], bool]
+]:
     depends_on_keys = rule.project_depends_on
     dep_rule_errors: List[SemgrepError] = []
 
@@ -112,24 +145,34 @@ def generate_reachable_sca_findings(
 
     # Reachability rule
     reachable_matches = []
-    matched_lockfiles = set()
+    reachable_deps = set()
     for ecosystem in ecosystems:
         for match in matches:
             try:
-                lockfile_data = find_single_lockfile(match.path, ecosystem)
-                if lockfile_data is None:
+                lockfile_path = target_manager.find_single_lockfile(
+                    match.path, ecosystem
+                )
+                if lockfile_path is None:
                     continue
-                lockfile_path, deps = lockfile_data
-                if str(lockfile_path) not in target_manager.lockfile_scan_info:
-                    # If the lockfile is not part of the actual targets or we just haven't parsed this lockfile yet
-                    target_manager.lockfile_scan_info[str(lockfile_path)] = len(deps)
+                deps = parse_lockfile_path(lockfile_path)
+                frozen_deps = tuple((dep.package, dep.transitivity) for dep in deps)
 
                 dependency_matches = list(
                     dependencies_range_match_any(depends_on_entries, deps)
                 )
-                if dependency_matches:
-                    matched_lockfiles.add(lockfile_path)
                 for dep_pat, found_dep in dependency_matches:
+                    if found_dep.transitivity == Transitivity(
+                        Transitive()
+                    ) and transivite_dep_is_also_direct(found_dep.package, frozen_deps):
+                        continue
+                    reachable_deps.add(
+                        (
+                            lockfile_path,
+                            found_dep.package,
+                            found_dep.version,
+                            found_dep.transitivity,
+                        )
+                    )
                     dep_match = DependencyMatch(
                         dependency_pattern=dep_pat,
                         found_dependency=found_dep,
@@ -144,4 +187,8 @@ def generate_reachable_sca_findings(
                     reachable_matches.append(match)
             except SemgrepError as e:
                 dep_rule_errors.append(e)
-    return reachable_matches, dep_rule_errors, matched_lockfiles
+    return (
+        reachable_matches,
+        dep_rule_errors,
+        (lambda p, d: (p, d.package, d.version, d.transitivity) in reachable_deps),
+    )
Only in /home/pad/yy/cli/src/semgrep: engine.py
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/env.py /home/pad/yy/cli/src/semgrep/env.py
--- /tmp/semgrep/cli/src/semgrep/env.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/env.py	2023-03-31 09:10:01.999997015 +0200
@@ -90,6 +90,8 @@ class Env:
     shouldafound_no_email: bool = field()
     min_fetch_depth: int = field()
 
+    r2c_internal_jsonnet_lib: Path = field()
+
     @version_check_timeout.default
     def version_check_timeout_default(self) -> int:
         value = os.getenv("SEMGREP_VERSION_CHECK_TIMEOUT", "2")
@@ -155,3 +157,12 @@ class Env:
     def min_fetch_depth_default(self) -> int:
         value = os.getenv("SEMGREP_GHA_MIN_FETCH_DEPTH", "0")
         return int(value)
+
+    # R2C_INTERNAL_JSONNET
+    @r2c_internal_jsonnet_lib.default
+    def r2c_internal_jsonnet_lib_default(self) -> Path:
+        value = os.getenv("R2C_INTERNAL_JSONNET_LIB")
+        if value:
+            return Path(value)
+        # TODO what should the default path be?
+        return Path.home()
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/error_handler.py /home/pad/yy/cli/src/semgrep/error_handler.py
--- /tmp/semgrep/cli/src/semgrep/error_handler.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/error_handler.py	2023-03-23 12:13:38.739733114 +0100
@@ -80,10 +80,12 @@ class ErrorHandler:
         }
         if sys.exc_info()[0] is not None:
             self.payload["error"] = traceback.format_exc()
+
         try:
             requests.post(
                 state.env.fail_open_url, headers=headers, json=self.payload, timeout=3
             )
         except Exception as e:
-            logger.error(f"Error sending to fail-open endpoint: {e}")
+            logger.debug(f"Error sending to fail-open endpoint: {e}")
+
         return 0
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/error.py /home/pad/yy/cli/src/semgrep/error.py
--- /tmp/semgrep/cli/src/semgrep/error.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/error.py	2023-03-17 09:21:14.529881197 +0100
@@ -15,2 +15,2 @@ from typing import Tuple
 import attr  # TODO: update to next-gen API with @define; difficult cause these subclass of Exception
 
@@ -28,10 +28,10 @@ logger = getLogger(__name__)
 OK_EXIT_CODE = 0
 FINDINGS_EXIT_CODE = 1
 FATAL_EXIT_CODE = 2
-# the commented one below are not used anymore
-# INVALID_CODE_EXIT_CODE = 3
-INVALID_PATTERN_EXIT_CODE = 4
+TARGET_PARSE_FAILURE_EXIT_CODE = 3
+RULE_PARSE_FAILURE_EXIT_CODE = 4
 UNPARSEABLE_YAML_EXIT_CODE = 5
+# the commented one below are not used anymore
 # NEED_ARBITRARY_CODE_EXEC_EXIT_CODE = 6
 MISSING_CONFIG_EXIT_CODE = 7
 INVALID_LANGUAGE_EXIT_CODE = 8
@@ -113,3 +113,3 @@ class SemgrepCoreError(SemgrepError):
             return "Syntax error"
         if isinstance(type_.value, core.PatternParseError):
             return "Pattern parse error"
@@ -136,6 +136,20 @@ class SemgrepCoreError(SemgrepError):
 
         return base
 
+    @property
+    def is_special_interfile_analysis_error(self) -> bool:
+        """
+        These errors indicate that multifile analysis did not
+        successfully ran, but we were able to get results anyway.
+        They should not block, but they are still errors so that
+        they display as errors
+
+        TODO remove this when we remove the interfile specific errors
+        """
+        return isinstance(
+            self.core.error_type.value, core.OutOfMemoryDuringInterfile
+        ) or isinstance(self.core.error_type.value, core.TimeoutDuringInterfile)
+
     def is_timeout(self) -> bool:
         """
         Return if this error is a match timeout
@@ -384,7 +398,7 @@ class ErrorWithSpan(SemgrepError):
 
 @attr.s(frozen=True, eq=True)
 class InvalidRuleSchemaError(ErrorWithSpan):
-    code = INVALID_PATTERN_EXIT_CODE
+    code = RULE_PARSE_FAILURE_EXIT_CODE
     level = Level.ERROR
 
 
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/formatter/base.py /home/pad/yy/cli/src/semgrep/formatter/base.py
--- /tmp/semgrep/cli/src/semgrep/formatter/base.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/formatter/base.py	2022-12-06 09:32:02.560492621 +0100
@@ -6,3 +6,3 @@ from typing import Iterable
 from typing import Mapping
 from typing import Sequence
 
@@ -22,6 +22,7 @@ class BaseFormatter(abc.ABC):
         cli_output_extra: out.CliOutputExtra,
         extra: Mapping[str, Any],
         shown_severities: Collection[RuleSeverity],
+        is_ci_invocation: bool,
     ) -> str:
         filtered_rules = (r for r in rules if r.severity in shown_severities)
         filtered_matches = (m for m in rule_matches if m.severity in shown_severities)
@@ -31,6 +32,7 @@ class BaseFormatter(abc.ABC):
             semgrep_structured_errors,
             cli_output_extra,
             extra,
+            is_ci_invocation,
         )
 
     @abc.abstractmethod
@@ -41,6 +43,7 @@ class BaseFormatter(abc.ABC):
         semgrep_structured_errors: Sequence[SemgrepError],
         cli_output_extra: out.CliOutputExtra,
         extra: Mapping[str, Any],
+        is_ci_invocation: bool,
     ) -> str:
         raise NotImplementedError
 
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/formatter/emacs.py /home/pad/yy/cli/src/semgrep/formatter/emacs.py
--- /tmp/semgrep/cli/src/semgrep/formatter/emacs.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/formatter/emacs.py	2022-12-06 09:32:02.560492621 +0100
@@ -3,3 +3,3 @@ from typing import Iterable
 from typing import Mapping
 from typing import Sequence
 
@@ -37,6 +37,7 @@ class EmacsFormatter(BaseFormatter):
         semgrep_structured_errors: Sequence[SemgrepError],
         cli_output_extra: out.CliOutputExtra,
         extra: Mapping[str, Any],
+        is_ci_invocation: bool,
     ) -> str:
         sorted_matches = sorted(rule_matches, key=lambda r: (r.path, r.rule_id))
         return "\n".join(":".join(self._get_parts(rm)) for rm in sorted_matches)
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/formatter/gitlab_sast.py /home/pad/yy/cli/src/semgrep/formatter/gitlab_sast.py
--- /tmp/semgrep/cli/src/semgrep/formatter/gitlab_sast.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/formatter/gitlab_sast.py	2023-02-28 13:23:46.240097814 +0100
@@ -37,9 +39,8 @@ def _construct_semgrep_rule_url(rule_id:
 
 class GitlabSastFormatter(BaseFormatter):
     def _format_rule_match(self, rule_match: RuleMatch) -> Mapping[str, Any]:
-        # create UUID from sha256 hash
-        return {
-            "id": str(rule_match.uuid),
+        result: Dict[str, Any] = {
+            "id": str(rule_match.uuid),  # create UUID from sha256 hash
             "category": "sast",
             # CVE is a required field from Gitlab schema.
             # It also is part of the determination for uniqueness
@@ -55,9 +56,6 @@ class GitlabSastFormatter(BaseFormatter)
             ),
             "message": rule_match.message,
             "severity": _to_gitlab_severity(rule_match.severity),
-            # Semgrep is designed to be a low-FP tool by design.
-            # Does hard-coding confidence make sense here?
-            "confidence": "High",
             "scanner": {
                 "id": "semgrep",
                 "name": "Semgrep",
@@ -77,8 +75,46 @@ class GitlabSastFormatter(BaseFormatter)
                     "url": _construct_semgrep_rule_url(rule_match.rule_id),
                 }
             ],
+            "flags": [],
+            "details": {},
         }
 
+        confidence = rule_match.metadata.get("confidence")
+        if confidence:
+            result["details"]["confidence"] = {
+                "type": "text",
+                "name": "confidence",
+                "value": confidence,
+            }
+            if confidence == "LOW":
+                result["flags"].append(
+                    {
+                        "type": "flagged-as-likely-false-positive",
+                        "origin": "Semgrep",
+                        "description": "This finding is from a low confidence rule.",
+                    }
+                )
+
+        if rule_match.exposure_type:
+            result["details"]["exposure"] = {
+                "type": "text",
+                "name": "exposure",
+                "value": rule_match.exposure_type,
+            }
+            if rule_match.exposure_type == "unreachable":
+                result["flags"].append(
+                    {
+                        "type": "flagged-as-likely-false-positive",
+                        "origin": "Semgrep Supply Chain",
+                        "description": (
+                            "Semgrep found no way to reach this vulnerability "
+                            "while scanning your code."
+                        ),
+                    }
+                )
+
+        return result
+
     def format(
         self,
         rules: Iterable[Rule],
@@ -86,6 +122,7 @@ class GitlabSastFormatter(BaseFormatter)
         semgrep_structured_errors: Sequence[SemgrepError],
         cli_output_extra: out.CliOutputExtra,
         extra: Mapping[str, Any],
+        is_ci_invocation: bool,
     ) -> str:
         """
         Format matches in GitLab SAST report compliant JSON.
@@ -97,9 +134,33 @@ class GitlabSastFormatter(BaseFormatter)
         - Schema:
             https://gitlab.com/gitlab-org/security-products/security-report-schemas/-/blob/master/dist/sast-report-format.json
         """
+        metrics = get_state().metrics
         output_dict = {
             "$schema": "https://gitlab.com/gitlab-org/security-products/security-report-schemas/-/blob/master/dist/sast-report-format.json",
-            "version": "14.1.2",
+            "version": "15.0.4",
+            "scan": {
+                "start_time": metrics.payload["started_at"].isoformat(
+                    timespec="seconds"
+                ),
+                "end_time": datetime.now().isoformat(timespec="seconds"),
+                "analyzer": {
+                    "id": "semgrep",
+                    "name": "Semgrep",
+                    "url": "https://semgrep.dev",
+                    "version": metrics.payload["environment"]["version"],
+                    "vendor": {"name": "Semgrep"},
+                },
+                "scanner": {
+                    "id": "semgrep",
+                    "name": "Semgrep",
+                    "url": "https://semgrep.dev",
+                    "version": metrics.payload["environment"]["version"],
+                    "vendor": {"name": "Semgrep"},
+                },
+                "version": metrics.payload["environment"]["version"],
+                "status": "success",
+                "type": "sast",
+            },
             "vulnerabilities": [
                 self._format_rule_match(rule_match)
                 for rule_match in rule_matches
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/formatter/json.py /home/pad/yy/cli/src/semgrep/formatter/json.py
--- /tmp/semgrep/cli/src/semgrep/formatter/json.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/formatter/json.py	2023-03-17 09:21:14.529881197 +0100
@@ -4,3 +4,3 @@ from typing import Iterable
 from typing import Mapping
 from typing import Sequence
 
@@ -30,6 +30,7 @@ class JsonFormatter(BaseFormatter):
             lines="".join(rule_match.lines).rstrip(),
             metavars=rule_match.match.extra.metavars,
             dataflow_trace=rule_match.dataflow_trace,
+            engine_kind=rule_match.match.extra.engine_kind,
         )
 
         if rule_match.extra.get("sca_info"):
@@ -42,6 +43,8 @@ class JsonFormatter(BaseFormatter):
             extra.fix_regex = rule_match.fix_regex
         if rule_match.is_ignored is not None:
             extra.is_ignored = rule_match.is_ignored
+        if rule_match.extra.get("extra_extra"):
+            extra.extra_extra = out.RawJson(rule_match.extra.get("extra_extra"))
 
         return out.CliMatch(
             check_id=out.RuleId(rule_match.rule_id),
@@ -58,11 +61,12 @@ class JsonFormatter(BaseFormatter):
         semgrep_structured_errors: Sequence[SemgrepError],
         cli_output_extra: out.CliOutputExtra,
         extra: Mapping[str, Any],
+        is_ci_invocation: bool,
     ) -> str:
         # Note that extra is not used here! Every part of the JSON output should
-        # be specified in Semgrep_output_xxx.atd and be part of CliOutputExtra
+        # be specified in semgrep_output_v1.atd and be part of CliOutputExtra
         output = out.CliOutput(
-            version=out.Semver(__VERSION__),
+            version=out.Version(__VERSION__),
             results=[
                 self._rule_match_to_CliMatch(rule_match) for rule_match in rule_matches
             ],
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/formatter/junit_xml.py /home/pad/yy/cli/src/semgrep/formatter/junit_xml.py
--- /tmp/semgrep/cli/src/semgrep/formatter/junit_xml.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/formatter/junit_xml.py	2022-12-06 09:32:02.560492621 +0100
@@ -4,3 +4,3 @@ from typing import Iterable
 from typing import Mapping
 from typing import Sequence
 
@@ -37,6 +37,7 @@ class JunitXmlFormatter(BaseFormatter):
         semgrep_structured_errors: Sequence[SemgrepError],
         cli_output_extra: out.CliOutputExtra,
         extra: Mapping[str, Any],
+        is_ci_invocation: bool,
     ) -> str:
         test_cases = [
             self._rule_match_to_test_case(rule_match) for rule_match in rule_matches
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/formatter/sarif.py /home/pad/yy/cli/src/semgrep/formatter/sarif.py
--- /tmp/semgrep/cli/src/semgrep/formatter/sarif.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/formatter/sarif.py	2023-02-28 13:23:46.240097814 +0100
 
 class SarifFormatter(BaseFormatter):
     @staticmethod
-    def _rule_match_to_sarif(rule_match: RuleMatch) -> Mapping[str, Any]:
-        rule_match_sarif = {
+    def _taint_source_to_thread_flow_location_sarif(rule_match: RuleMatch) -> Any:
+        dataflow_trace = rule_match.dataflow_trace
+        if not dataflow_trace:
+            return None
+        taint_source = dataflow_trace.taint_source
+        if not taint_source:
+            return None
+        if isinstance(taint_source.value, out.CliCall):
+            logger.error(
+                "Emitting SARIF output for unsupported dataflow trace (source is a call)"
+            )
+            return None
+        elif isinstance(taint_source.value, out.CliLoc):
+            location = taint_source.value.value[0]
+            content = "".join(taint_source.value.value[1]).strip()
+            source_message_text = f"Source: '{content}' @ '{str(location.path)}:{str(location.start.line)}'"
+
+            taint_source_location_sarif = {
+                "location": {
+                    "message": {"text": source_message_text},
+                    "physicalLocation": {
+                        "artifactLocation": {"uri": str(rule_match.path)},
+                        "region": {
+                            "startLine": location.start.line,
+                            "startColumn": location.start.col,
+                            "endLine": location.end.line,
+                            "endColumn": location.end.col,
+                            "snippet": {"text": content},
+                            "message": {"text": source_message_text},
+                        },
+                    },
+                }
+            }
+            return taint_source_location_sarif
+
+    @staticmethod
+    def _intermediate_vars_to_thread_flow_location_sarif(rule_match: RuleMatch) -> Any:
+        dataflow_trace = rule_match.dataflow_trace
+        if not dataflow_trace:
+            return None
+        intermediate_vars = dataflow_trace.intermediate_vars
+        if not intermediate_vars:
+            return None
+        intermediate_var_locations = []
+        for intermediate_var in intermediate_vars:
+            location = intermediate_var.location
+            content = "".join(intermediate_var.content).strip()
+            propagation_message_text = f"Propagator : '{content}' @ '{str(location.path)}:{str(location.start.line)}'"
+
+            intermediate_vars_location_sarif = {
+                "location": {
+                    "message": {"text": propagation_message_text},
+                    "physicalLocation": {
+                        "artifactLocation": {"uri": str(rule_match.path)},
+                        "region": {
+                            "startLine": location.start.line,
+                            "startColumn": location.start.col,
+                            "endLine": location.end.line,
+                            "endColumn": location.end.col,
+                            "snippet": {"text": content},
+                            "message": {"text": propagation_message_text},
+                        },
+                    },
+                }
+            }
+            intermediate_var_locations.append(intermediate_vars_location_sarif)
+        return intermediate_var_locations
+
+    @staticmethod
+    def _sink_to_thread_flow_location_sarif(rule_match: RuleMatch) -> Any:
+        content = "".join(rule_match.get_lines()).strip()
+        sink_message_text = (
+            f"Sink: '{content}' @ '{str(rule_match.path)}:{str(rule_match.start.line)}'"
+        )
+
+        sink_location_sarif = {
+            "location": {
+                "message": {"text": sink_message_text},
+                "physicalLocation": {
+                    "artifactLocation": {"uri": str(rule_match.path)},
+                    "region": {
+                        "startLine": rule_match.start.line,
+                        "startColumn": rule_match.start.col,
+                        "endLine": rule_match.end.line,
+                        "endColumn": rule_match.end.col,
+                        "snippet": {"text": "".join(rule_match.lines).rstrip()},
+                        "message": {"text": sink_message_text},
+                    },
+                },
+            }
+        }
+        return sink_location_sarif
+
+    @staticmethod
+    def _dataflow_trace_to_thread_flows_sarif(rule_match: RuleMatch) -> Any:
+        thread_flows = []
+        locations = []
+
+        dataflow_trace = rule_match.dataflow_trace
+        if not dataflow_trace:
+            return None
+        taint_source = dataflow_trace.taint_source
+        # TODO: deal with taint sink
+        intermediate_vars = dataflow_trace.intermediate_vars
+
+        if taint_source:
+            locations.append(
+                SarifFormatter._taint_source_to_thread_flow_location_sarif(rule_match)
+            )
+
+        if intermediate_vars:
+            intermediate_var_locations = (
+                SarifFormatter._intermediate_vars_to_thread_flow_location_sarif(
+                    rule_match
+                )
+            )
+            if intermediate_var_locations:
+                for intermediate_var_location in intermediate_var_locations:
+                    locations.append(intermediate_var_location)
+
+        locations.append(SarifFormatter._sink_to_thread_flow_location_sarif(rule_match))
+
+        thread_flows.append({"locations": locations})
+        return thread_flows
+
+    @staticmethod
+    def _dataflow_trace_to_codeflow_sarif(
+        rule_match: RuleMatch,
+    ) -> Optional[Mapping[str, Any]]:
+        dataflow_trace = rule_match.dataflow_trace
+        if not dataflow_trace:
+            return None
+        taint_source = dataflow_trace.taint_source
+        if not taint_source:
+            return None
+
+        # TODO: handle rule_match.taint_sink
+        if isinstance(taint_source.value, out.CliCall):
+            logger.error(
+                "Emitting SARIF output for unsupported dataflow trace (source is a call)"
+            )
+            return None
+        elif isinstance(taint_source.value, out.CliLoc):
+            location = taint_source.value.value[0]
+            code_flow_message = f"Untrusted dataflow from {str(location.path)}:{str(location.start.line)} to {str(rule_match.path)}:{str(rule_match.start.line)}"
+            code_flow_sarif = {
+                "message": {"text": code_flow_message},
+            }
+            thread_flows = SarifFormatter._dataflow_trace_to_thread_flows_sarif(
+                rule_match
+            )
+            if thread_flows:
+                code_flow_sarif["threadFlows"] = thread_flows
+
+            return code_flow_sarif
+
+    @staticmethod
+    def _rule_match_to_sarif(
+        rule_match: RuleMatch, dataflow_traces: bool
+    ) -> Mapping[str, Any]:
+        rule_match_sarif: Dict[str, Any] = {
             "ruleId": rule_match.rule_id,
             "message": {"text": rule_match.message},
             "locations": [
@@ -38,7 +201,15 @@ class SarifFormatter(BaseFormatter):
                     }
                 }
             ],
+            "fingerprints": {"matchBasedId/v1": rule_match.match_based_id},
+            "properties": {},
         }
+
+        if dataflow_traces and rule_match.dataflow_trace:
+            code_flows = SarifFormatter._dataflow_trace_to_codeflow_sarif(rule_match)
+            if code_flows:
+                rule_match_sarif["codeFlows"] = [code_flows]
+
         if rule_match.is_ignored:
             rule_match_sarif["suppressions"] = [{"kind": "inSource"}]
 
@@ -47,6 +218,9 @@ class SarifFormatter(BaseFormatter):
         if fix is not None:
             rule_match_sarif["fixes"] = [fix]
 
+        if rule_match.exposure_type:
+            rule_match_sarif["properties"]["exposure"] = rule_match.exposure_type
+
         return rule_match_sarif
 
     @staticmethod
@@ -84,6 +257,21 @@ class SarifFormatter(BaseFormatter):
     def _rule_to_sarif(rule: Rule) -> Mapping[str, Any]:
         severity = SarifFormatter._rule_to_sarif_severity(rule)
         tags = SarifFormatter._rule_to_sarif_tags(rule)
+        security_severity = rule.metadata.get("security-severity")
+        if security_severity is not None:
+            rule_json = {
+                "id": rule.id,
+                "name": rule.id,
+                "shortDescription": {"text": rule.message},
+                "fullDescription": {"text": rule.message},
+                "defaultConfiguration": {"level": severity},
+                "properties": {
+                    "precision": "very-high",
+                    "tags": tags,
+                    "security-severity": security_severity,
+                },
+            }
+        else:
         rule_json = {
             "id": rule.id,
             "name": rule.id,
@@ -130,6 +318,7 @@ class SarifFormatter(BaseFormatter):
         if "cwe" in rule.metadata:
             cwe = rule.metadata["cwe"]
             result.extend(cwe if isinstance(cwe, list) else [cwe])
+            result.append("security")
         if "owasp" in rule.metadata:
             owasp = rule.metadata["owasp"]
             result.extend(
@@ -137,11 +326,19 @@ class SarifFormatter(BaseFormatter):
                 if isinstance(owasp, list)
                 else [f"OWASP-{owasp}"]
             )
+        if (
+            "semgrep.policy" in rule.metadata
+            and "slug" in rule.metadata["semgrep.policy"]
+        ):
+            # https://github.com/returntocorp/semgrep-app/blob/8d2e6187b7daa2b20c49839a4fcb67e560202aa8/frontend/src/pages/ruleBoard/constants/constants.tsx#L74
+            # this should be "rule-board-audit", "rule-board-block", or "rule-board-pr-comments"
+            slug = rule.metadata["semgrep.policy"]["slug"]
+            result.append(slug)
 
         for tags in rule.metadata.get("tags", []):
             result.append(tags)
 
-        return result
+        return sorted(set(result))
 
     @staticmethod
     def _semgrep_error_to_sarif_notification(error: SemgrepError) -> Mapping[str, Any]:
@@ -178,6 +375,7 @@ class SarifFormatter(BaseFormatter):
         semgrep_structured_errors: Sequence[SemgrepError],
         cli_output_extra: out.CliOutputExtra,
         extra: Mapping[str, Any],
+        is_ci_invocation: bool,
     ) -> str:
         """
         Format matches in SARIF v2.1.0 formatted JSON.
@@ -203,7 +401,7 @@ class SarifFormatter(BaseFormatter):
                         }
                     },
                     "results": [
-                        self._rule_match_to_sarif(rule_match)
+                        self._rule_match_to_sarif(rule_match, extra["dataflow_traces"])
                         for rule_match in rule_matches
                     ],
                     "invocations": [
@@ -220,4 +418,4 @@ class SarifFormatter(BaseFormatter):
         }
 
         # Sort keys for predictable output. This helps with snapshot tests, etc.
-        return json.dumps(output_dict, sort_keys=True)
+        return json.dumps(output_dict, sort_keys=True, indent=2)
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/formatter/text.py /home/pad/yy/cli/src/semgrep/formatter/text.py
--- /tmp/semgrep/cli/src/semgrep/formatter/text.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/formatter/text.py	2023-03-25 17:10:17.159558674 +0100
@@ -1,7 +1,10 @@
 
 import click
 import colorama
+from rich.console import Console
 
+from semgrep.console import console
+from semgrep.console import Title
 from semgrep.constants import CLI_RULE_ID
 from semgrep.constants import Colors
 from semgrep.constants import ELLIPSIS_STRING
@@ -24,6 +30,7 @@ from semgrep.error import SemgrepCoreErr
 from semgrep.error import SemgrepError
 from semgrep.formatter.base import BaseFormatter
 from semgrep.rule import Rule
+from semgrep.rule import RuleProduct
 from semgrep.rule_match import RuleMatch
 from semgrep.semgrep_types import LANGUAGE
 from semgrep.semgrep_types import Language
@@ -35,6 +42,8 @@ from semgrep.util import with_color
 
 MAX_TEXT_WIDTH = 120
 
+BASE_INDENT = 8
+
 terminal_size = get_terminal_size((MAX_TEXT_WIDTH, 1))[0]
 if terminal_size <= 0:
     terminal_size = MAX_TEXT_WIDTH
@@ -47,16 +56,24 @@ else:
 FINDINGS_INDENT_DEPTH = 10
 
 
-class TextFormatter(BaseFormatter):
-    @staticmethod
-    def _color_line(
+GROUP_TITLES: Dict[Tuple[RuleProduct, str], str] = {
+    (RuleProduct.sca, "unreachable"): "Unreachable Supply Chain Finding",
+    (RuleProduct.sca, "undetermined"): "Undetermined Supply Chain Finding",
+    (RuleProduct.sca, "reachable"): "Reachable Supply Chain Finding",
+    (RuleProduct.sast, "nonblocking"): "Non-blocking Code Finding",
+    (RuleProduct.sast, "blocking"): "Blocking Code Finding",
+    (RuleProduct.sast, "merged"): "Code Finding",
+}
+
+
+def color_line(
         line: str,
         line_number: int,
         start_line: int,
         start_col: int,
         end_line: int,
         end_col: int,
-    ) -> str:
+) -> str:
         """
         Assumes column start and end numbers are 1-indexed
         """
@@ -74,8 +91,8 @@ class TextFormatter(BaseFormatter):
         )
         return line
 
-    @staticmethod
-    def _format_lines(
+
+def format_lines(
         path: Path,
         start_line: int,
         start_col: int,
@@ -86,7 +103,8 @@ class TextFormatter(BaseFormatter):
         per_finding_max_lines_limit: Optional[int],
         per_line_max_chars_limit: Optional[int],
         show_separator: bool,
-    ) -> Iterator[str]:
+    show_path: bool,
+) -> Iterator[str]:
         trimmed = 0
         stripped = False
 
@@ -111,7 +129,7 @@ class TextFormatter(BaseFormatter):
             line_number = ""
             if start_line:
                 if color_output:
-                    line = TextFormatter._color_line(
+                line = color_line(
                         line,
                         start_line + i,
                         start_line,
@@ -128,9 +146,7 @@ class TextFormatter(BaseFormatter):
                     is_first_line = i == 0
                     if is_first_line:
                         line = (
-                            line[
-                                start_col - 1 : start_col - 1 + per_line_max_chars_limit
-                            ]
+                        line[start_col - 1 : start_col - 1 + per_line_max_chars_limit]
                             + ELLIPSIS_STRING
                         )
                         if start_col > 1:
@@ -140,29 +156,39 @@ class TextFormatter(BaseFormatter):
                     # while stripping a string, the ANSI code for resetting color might also get stripped.
                     line = line + colorama.Style.RESET_ALL
 
+        # plus one because we want this to be slightly separated from the intervening messages
+        if i == 0 and show_path:
+            yield f" " * (
+                BASE_INDENT + 1
+            ) + f"{with_color(Colors.cyan, f'{path}', bold=False)}"
+
             yield f" " * (
                 11 - len(line_number)
             ) + f"{line_number}â”† {line}" if line_number else f"{line}"
 
         if stripped:
-            stripped_str = f"[shortened a long line from output, adjust with {MAX_CHARS_FLAG_NAME}]"
+        stripped_str = (
+            f"[shortened a long line from output, adjust with {MAX_CHARS_FLAG_NAME}]"
+        )
             yield " " * FINDINGS_INDENT_DEPTH + stripped_str
 
         if per_finding_max_lines_limit != 1:
             if trimmed > 0:
-                trimmed_str = f" [hid {trimmed} additional lines, adjust with {MAX_LINES_FLAG_NAME}] "
+            trimmed_str = (
+                f" [hid {trimmed} additional lines, adjust with {MAX_LINES_FLAG_NAME}] "
+            )
                 yield " " * FINDINGS_INDENT_DEPTH + trimmed_str
             elif lines and show_separator:
                 yield f" " * FINDINGS_INDENT_DEPTH + f"â‹®â”†" + f"-" * 40
 
-    @staticmethod
-    def _finding_to_line(
+
+def finding_to_line(
         rule_match: RuleMatch,
         color_output: bool,
         per_finding_max_lines_limit: Optional[int],
         per_line_max_chars_limit: Optional[int],
         show_separator: bool,
-    ) -> Iterator[str]:
+) -> Iterator[str]:
         path = rule_match.path
         start_line = rule_match.start.line
         end_line = rule_match.end.line
@@ -170,7 +196,7 @@ class TextFormatter(BaseFormatter):
         end_col = rule_match.end.col
         if path:
             lines = rule_match.extra.get("fixed_lines") or rule_match.lines
-            yield from TextFormatter._format_lines(
+        yield from format_lines(
                 path,
                 start_line,
                 start_col,
@@ -181,48 +207,145 @@ class TextFormatter(BaseFormatter):
                 per_finding_max_lines_limit,
                 per_line_max_chars_limit,
                 show_separator,
+            False,
+        )
+
+
+def match_to_lines(
+    ref_path: Path,
+    location: out.Location,
+    content: str,
+    color_output: bool,
+    per_finding_max_lines_limit: Optional[int],
+    per_line_max_chars_limit: Optional[int],
+) -> Iterator[str]:
+    path = Path(location.path)
+    is_same_file = path == ref_path
+    lines = get_lines(path, location.start.line, location.end.line)
+    yield from format_lines(
+        path,
+        location.start.line,
+        location.start.col,
+        location.end.line,
+        location.end.col,
+        lines,
+        color_output,
+        per_finding_max_lines_limit,
+        per_line_max_chars_limit,
+        False,
+        not is_same_file,
+    )
+
+
+def call_trace_to_lines(
+    ref_path: Path,
+    call_trace: out.CliMatchCallTrace,
+    color_output: bool,
+    per_finding_max_lines_limit: Optional[int],
+    per_line_max_chars_limit: Optional[int],
+) -> Iterator[str]:
+    trace = call_trace.value
+    if isinstance(trace, out.CliLoc):
+        yield from match_to_lines(
+            ref_path,
+            trace.value[0],
+            trace.value[1],
+            color_output,
+            per_finding_max_lines_limit,
+            per_line_max_chars_limit,
+        )
+
+    elif isinstance(trace, out.CliCall):
+        data, intermediate_vars, call_trace = trace.value
+
+        yield from match_to_lines(
+            ref_path,
+            data[0],
+            data[1],
+            color_output,
+            per_finding_max_lines_limit,
+            per_line_max_chars_limit,
+        )
+
+        if intermediate_vars and len(intermediate_vars) > 0:
+            # TODO change this message based on rule kind if we ever use
+            # dataflow traces for more than just taint
+            yield (
+                BASE_INDENT * " " + "Taint flows through these intermediate variables:"
+            )
+            prev_path = ref_path
+            for var in intermediate_vars:
+                loc = var.location
+                path = Path(loc.path)
+                lines = get_lines(Path(loc.path), loc.start.line, loc.end.line)
+                is_same_file = path == prev_path
+                yield from format_lines(
+                    Path(loc.path),
+                    loc.start.line,
+                    loc.start.col,
+                    loc.end.line,
+                    loc.end.col,
+                    lines,
+                    color_output,
+                    per_finding_max_lines_limit,
+                    per_line_max_chars_limit,
+                    False,
+                    not is_same_file,
             )
+                prev_path = path
 
-    @staticmethod
-    def _dataflow_trace_to_lines(
+        if isinstance(call_trace.value, out.CliCall):
+            yield (BASE_INDENT * " " + "then call to:")
+        elif isinstance(call_trace.value, out.CliLoc):
+            yield (BASE_INDENT * " " + "then reaches:")
+        yield from call_trace_to_lines(
+            ref_path,
+            call_trace,
+            color_output,
+            per_finding_max_lines_limit,
+            per_line_max_chars_limit,
+        )
+
+
+def dataflow_trace_to_lines(
+    rule_match_path: Path,
         dataflow_trace: Optional[out.CliMatchDataflowTrace],
         color_output: bool,
         per_finding_max_lines_limit: Optional[int],
         per_line_max_chars_limit: Optional[int],
-    ) -> Iterator[str]:
+    show_separator: bool,
+) -> Iterator[str]:
         if dataflow_trace:
             source = dataflow_trace.taint_source
             intermediate_vars = dataflow_trace.intermediate_vars
+        sink = dataflow_trace.taint_sink
+
             if source:
-                yield (8 * " " + "Taint comes from:")
-                path = Path(source.location.path)
-                lines = get_lines(
-                    path, source.location.start.line, source.location.end.line
-                )
-                # TODO with DeepSemgrep is it possible for the source (and
-                # intermediate vars) to be in a different file? If so, make sure
-                # we print the filename when needed.
-                yield from TextFormatter._format_lines(
-                    path,
-                    source.location.start.line,
-                    source.location.start.col,
-                    source.location.end.line,
-                    source.location.end.col,
-                    lines,
+            yield ""
+            yield (BASE_INDENT * " " + "Taint comes from:")
+            yield from call_trace_to_lines(
+                rule_match_path,
+                source,
                     color_output,
                     per_finding_max_lines_limit,
                     per_line_max_chars_limit,
-                    False,
                 )
+
             if intermediate_vars and len(intermediate_vars) > 0:
                 # TODO change this message based on rule kind of we ever use
                 # dataflow traces for more than just taint
-                yield (8 * " " + "Taint flows through these intermediate variables:")
+            yield ""
+            yield (
+                BASE_INDENT * " " + "Taint flows through these intermediate variables:"
+            )
+            prev_path = rule_match_path
                 for var in intermediate_vars:
                     loc = var.location
-                    lines = get_lines(Path(loc.path), loc.start.line, loc.end.line)
-                    yield from TextFormatter._format_lines(
-                        Path(loc.path),
+                path = Path(loc.path)
+                lines = get_lines(path, loc.start.line, loc.end.line)
+                is_same_file = path == prev_path
+                yield from format_lines(
+                    path,
                         loc.start.line,
                         loc.start.col,
                         loc.end.line,
@@ -232,21 +355,36 @@ class TextFormatter(BaseFormatter):
                         per_finding_max_lines_limit,
                         per_line_max_chars_limit,
                         False,
+                    not is_same_file,
+                )
+                prev_path = path
+
+        if sink:
+            yield ""
+            yield (BASE_INDENT * " " + "This is how taint reaches the sink:")
+            yield from call_trace_to_lines(
+                rule_match_path,
+                sink,
+                color_output,
+                per_finding_max_lines_limit,
+                per_line_max_chars_limit,
                     )
+            yield ""
 
-    @staticmethod
-    def _get_details_shortlink(rule_match: RuleMatch) -> Optional[str]:
+        if source and show_separator:
+            yield f" " * BASE_INDENT + f"â‹®â”†" + f"-" * 40
+
+
+def get_details_shortlink(rule_match: RuleMatch) -> Optional[str]:
         source_url = rule_match.metadata.get("shortlink")
         if not source_url:
             return ""
         return f"Details: {source_url}"
 
-    @staticmethod
-    def _build_summary(
-        time_data: out.CliTiming,
-        error_output: Sequence[SemgrepError],
-        color_output: bool,
-    ) -> Iterator[str]:
+
+def print_time_summary(
+    time_data: out.CliTiming, error_output: Sequence[SemgrepError]
+) -> None:
         items_to_show = 5
         col_lim = 50
 
@@ -255,9 +393,7 @@ class TextFormatter(BaseFormatter):
         # Compute summary timings
         rule_parsing_time = time_data.rules_parse_time
         rule_match_timings = {
-            rule.id.value: sum(
-                t.match_times[i] for t in targets if t.match_times[i] >= 0
-            )
+        rule.id.value: sum(t.match_times[i] for t in targets if t.match_times[i] >= 0)
             for i, rule in enumerate(time_data.rules)
         }
         file_parsing_time = sum(
@@ -329,30 +465,41 @@ class TextFormatter(BaseFormatter):
         core_time = time_data.profiling_times.get("core_time", 0.0)
         # time_data.profiling_times.get("ignores_time", 0.0)
 
-        yield f"\n============================[ summary ]============================"
+    console.print(
+        "\n============================[ summary ]============================"
+    )
 
-        yield f"Total time: {total_time:.4f}s Config time: {config_time:.4f}s Core time: {core_time:.4f}s"
+    console.print(
+        f"Total time: {total_time:.4f}s Config time: {config_time:.4f}s Core time: {core_time:.4f}s"
+    )
 
         # Output semgrep-core information
-        yield f"\nSemgrep-core time:"
-        yield f"Total CPU time: {all_total_time:.4f}s  File parse time: {file_parsing_time:.4f}s" f"  Rule parse time: {rule_parsing_time:.4f}s  Match time: {total_matching_time:.4f}s"
+    console.print("\nSemgrep-core time:")
+    console.print(
+        f"Total CPU time: {all_total_time:.4f}s  File parse time: {file_parsing_time:.4f}s"
+        f"  Rule parse time: {rule_parsing_time:.4f}s  Match time: {total_matching_time:.4f}s"
+    )
 
-        yield f"Slowest {items_to_show}/{len(file_timings)} files"
-        slowest_file_times = sorted(
-            file_timings.items(), key=lambda x: x[1], reverse=True
-        )[:items_to_show]
+    console.print(f"Slowest {items_to_show}/{len(file_timings)} files")
+    slowest_file_times = sorted(file_timings.items(), key=lambda x: x[1], reverse=True)[
+        :items_to_show
+    ]
         for file_name, (parse_time, run_time) in slowest_file_times:
             num_bytes = f"({format_bytes(Path(file_name).resolve().stat().st_size)}):"
             file_name = truncate(file_name, col_lim)
-            yield f"{with_color(Colors.green, f'{file_name:<50}')} {num_bytes:<8} {run_time:.3f}s ({parse_time:.3f}s to parse)"
+        console.print(
+            f"{with_color(Colors.green, f'{file_name:<50}')} {num_bytes:<8} {run_time:.3f}s ({parse_time:.3f}s to parse)"
+        )
 
-        yield f"Slowest {items_to_show} rules to match"
+    console.print(f"Slowest {items_to_show} rules to match")
         slowest_rule_times = sorted(rule_match_timings.items(), reverse=True)[
             :items_to_show
         ]
         for rule_id, match_time in slowest_rule_times:
             rule_id = truncate(rule_id, col_lim) + ":"
-            yield f"{with_color(Colors.yellow, f'{rule_id:<59}')} {match_time:.3f}s"
+        console.print(
+            f"{with_color(Colors.yellow, f'{rule_id:<59}')} {match_time:.3f}s"
+        )
 
         # Output other file information
         ANALYZED = "Analyzed:"
@@ -370,13 +517,13 @@ class TextFormatter(BaseFormatter):
                 first = False
             return returned
 
-        yield ""
+    console.print()
 
         by_lang = [
             f"{ lang_counts[lang] } { lang } files ({ format_bytes(lang_bytes[lang]) } in {(lang_times[lang]):.3f} seconds)"
             for lang in langs
         ]
-        yield from add_heading(ANALYZED, by_lang)
+    console.print("\n".join(add_heading(ANALYZED, by_lang)))
 
         # Output errors
         def if_exists(num_errors: int, msg: str) -> str:
@@ -390,20 +537,17 @@ class TextFormatter(BaseFormatter):
         error_lines = [error_msg] + [
             f"{type} ({num} files)" for (type, num) in error_types.items()
         ]
+    console.print("\n".join(add_heading(FAILED, error_lines)))
+    console.print()
 
-        yield from add_heading(FAILED, error_lines)
-
-        yield ""
 
-    @staticmethod
-    def _build_text_output(
+def print_text_output(
         rule_matches: Iterable[RuleMatch],
         color_output: bool,
         per_finding_max_lines_limit: Optional[int],
         per_line_max_chars_limit: Optional[int],
         dataflow_traces: bool,
-    ) -> Iterator[str]:
-
+) -> None:
         last_file = None
         last_message = None
         sorted_rule_matches = sorted(rule_matches, key=lambda r: (r.path, r.rule_id))
@@ -408,25 +552,25 @@ class TextFormatter(BaseFormatter):
         last_message = None
         sorted_rule_matches = sorted(rule_matches, key=lambda r: (r.path, r.rule_id))
         for rule_index, rule_match in enumerate(sorted_rule_matches):
-
             current_file = rule_match.path
             rule_id = rule_match.rule_id
             message = rule_match.message
             fix = rule_match.fix
-            if "sca_info" in rule_match.extra and (
-                rule_match.extra["sca_info"].reachable
-            ):
+        if "sca_info" in rule_match.extra and (rule_match.extra["sca_info"].reachable):
                 lockfile = rule_match.extra["sca_info"].dependency_match.lockfile
             else:
                 lockfile = None
             if last_file is None or last_file != current_file:
                 if last_file is not None:
-                    yield ""
-                yield f"\n{with_color(Colors.cyan, f'  {current_file} ', bold=False)}" + (
+                console.print()
+            console.print(
+                f"\n{with_color(Colors.cyan, f'  {current_file} ', bold=False)}"
+                + (
                     f"with lockfile {with_color(Colors.cyan, f'{lockfile}')}"
                     if lockfile
                     else ""
                 )
+            )
                 last_message = None
             # don't display the rule line if the check is empty
             if (
@@ -434,7 +578,7 @@ class TextFormatter(BaseFormatter):
                 and rule_id != CLI_RULE_ID
                 and (last_message is None or last_message != message)
             ):
-                shortlink = TextFormatter._get_details_shortlink(rule_match)
+            shortlink = get_details_shortlink(rule_match)
                 shortlink_text = (8 * " " + shortlink + "\n") if shortlink else ""
                 rule_id_text = click.wrap_text(
                     f"{with_color(Colors.foreground, rule_id, bold=True)}",
@@ -443,10 +587,8 @@ class TextFormatter(BaseFormatter):
                     5 * " ",
                     False,
                 )
-                message_text = click.wrap_text(
-                    f"{message}", width, 8 * " ", 8 * " ", True
-                )
-                yield f"{rule_id_text}\n{message_text}\n{shortlink_text}"
+            message_text = click.wrap_text(f"{message}", width, 8 * " ", 8 * " ", True)
+            console.print(f"{rule_id_text}\n{message_text}\n{shortlink_text}")
 
             last_file = current_file
             last_message = message
@@ -457,30 +599,54 @@ class TextFormatter(BaseFormatter):
             )
             autofix_tag = with_color(Colors.green, "         â–¶â–¶â”† Autofix â–¶")
             if fix:
-                yield f"{autofix_tag} {fix}"
+            console.print(f"{autofix_tag} {fix}")
             elif rule_match.fix_regex:
                 fix_regex = rule_match.fix_regex
-                yield f"{autofix_tag} s/{fix_regex.regex}/{fix_regex.replacement}/{fix_regex.count or 'g'}"
+            console.print(
+                f"{autofix_tag} s/{fix_regex.regex}/{fix_regex.replacement}/{fix_regex.count or 'g'}"
+            )
 
             is_same_file = (
                 next_rule_match.path == rule_match.path if next_rule_match else False
             )
-            yield from TextFormatter._finding_to_line(
+        for line in finding_to_line(
                 rule_match,
                 color_output,
                 per_finding_max_lines_limit,
                 per_line_max_chars_limit,
-                is_same_file,
-            )
+            # if we have dataflow traces on, then we should print the separator,
+            # because otherwise it is easy to mistake taint traces as belonging
+            # to a different finding
+            is_same_file and not (dataflow_traces and rule_match.dataflow_trace),
+        ):
+            console.print(line)
 
             if dataflow_traces:
-                yield from TextFormatter._dataflow_trace_to_lines(
+            for line in dataflow_trace_to_lines(
+                rule_match.path,
                     rule_match.dataflow_trace,
                     color_output,
                     per_finding_max_lines_limit,
                     per_line_max_chars_limit,
-                )
+                is_same_file,
+            ):
+                console.print("  " + line)
 
+
+@contextmanager
+def force_quiet_off(console: Console) -> Iterator[None]:
+    """
+    Force the console to be not quiet, even if it was set to be quiet before.
+    """
+    was_quiet = console.quiet
+    console.quiet = False
+    try:
+        yield
+    finally:
+        console.quiet = was_quiet
+
+
+class TextFormatter(BaseFormatter):
     def format(
         self,
         rules: Iterable[Rule],
@@ -488,125 +654,84 @@ class TextFormatter(BaseFormatter):
         semgrep_structured_errors: Sequence[SemgrepError],
         cli_output_extra: out.CliOutputExtra,
         extra: Mapping[str, Any],
+        is_ci_invocation: bool,
     ) -> str:
-        reachable = []
-        unreachable = []
-        first_party_blocking = []
-        first_party_nonblocking = []
-        first_party_blocking_rules = []
+        # all output in this function is captured and returned as a string
+        with force_quiet_off(console), console.capture() as captured_output:
+            grouped_matches: Dict[Tuple[RuleProduct, str], List[RuleMatch]] = {
+                # ordered most important to least important
+                (RuleProduct.sast, "blocking"): [],
+                (RuleProduct.sca, "reachable"): [],
+                (RuleProduct.sca, "undetermined"): [],
+                (RuleProduct.sca, "unreachable"): [],
+                (RuleProduct.sast, "nonblocking"): [],
+            }
+
         for match in rule_matches:
-            if "sca_info" not in match.extra:
-                if match.is_blocking:
-                    first_party_blocking.append(match)
-                    rule_id = match.match.rule_id.value
-                    # When ephemeral rules are run with the -e or --pattern flag in the command-line, the rule_id is set to -.
-                    # The rule is ran in the command-line and has no associated rule_id
-                    if rule_id != "-":
-                        first_party_blocking_rules.append(rule_id)
-                else:
-                    first_party_nonblocking.append(match)
-            elif match.extra["sca_info"].reachable:
-                reachable.append(match)
+                if match.product == RuleProduct.sast:
+                    subgroup = "blocking" if match.is_blocking else "nonblocking"
             else:
-                unreachable.append(match)
+                    subgroup = match.exposure_type or "undetermined"
 
-        timing_output = (
-            self._build_summary(
-                cli_output_extra.time,
-                semgrep_structured_errors,
-                extra.get("color_output", False),
-            )
-            if cli_output_extra.time
-            else iter([])
-        )
-
-        findings_output = []
-        if reachable or unreachable:
-            findings_output.append(
-                f"\n{with_color(Colors.foreground, 'Semgrep Supply Chain Summary')}: {with_color(Colors.red,unit_str(len(reachable),'Reachable finding'))}, {with_color(Colors.yellow,unit_str(len(unreachable),'Unreachable finding'))}\n"
-            )
-        if reachable:
-            reachable_output = self._build_text_output(
-                reachable,
-                extra.get("color_output", False),
-                extra["per_finding_max_lines_limit"],
-                extra["per_line_max_chars_limit"],
-                extra["dataflow_traces"],
-            )
+                grouped_matches[match.product, subgroup].append(match)
 
-            findings_output.append(
-                f"\n{with_color(Colors.red, 'Reachable Supply Chain Findings:')}\n"
-                + "\n".join(reachable_output)
-            )
+            first_party_blocking_rules = {
+                match.match.rule_id.value
+                for match in grouped_matches[RuleProduct.sast, "blocking"]
+            }
 
-        if unreachable:
-            unreachable_output = self._build_text_output(
-                unreachable,
-                extra.get("color_output", False),
-                extra["per_finding_max_lines_limit"],
-                extra["per_line_max_chars_limit"],
-                extra["dataflow_traces"],
-            )
+            # When ephemeral rules are run with the -e or --pattern flag in the command-line, the rule_id is set to -.
+            # The rule is ran in the command-line and has no associated rule_id
+            first_party_blocking_rules.discard("-")
 
-            findings_output.append(
-                f"\n{with_color(Colors.yellow, 'Unreachable Supply Chain Findings:')}\n"
-                + "\n".join(unreachable_output)
-            )
+            if not is_ci_invocation:
+                grouped_matches[(RuleProduct.sast, "merged")] = [
+                    *grouped_matches.pop((RuleProduct.sast, "nonblocking")),
+                    *grouped_matches.pop((RuleProduct.sast, "blocking")),
+                ]
 
-        if first_party_nonblocking:
-            first_party_nonblocking_output = self._build_text_output(
-                first_party_nonblocking,
-                extra.get("color_output", False),
-                extra["per_finding_max_lines_limit"],
-                extra["per_line_max_chars_limit"],
-                extra["dataflow_traces"],
-            )
-            findings_output.append(
-                "\nFirst-Party Non-Blocking Findings:\n"
-                + "\n".join(first_party_nonblocking_output)
-            ) if (reachable or unreachable) else findings_output.append(
-                "\nNon-Blocking Findings:\n" + "\n".join(first_party_nonblocking_output)
-            )
-        if first_party_blocking:
-            first_party_blocking_output = self._build_text_output(
-                first_party_blocking,
+            for group, matches in grouped_matches.items():
+                if not matches:
+                    continue
+                console.print(Title(unit_str(len(matches), GROUP_TITLES[group])))
+                print_text_output(
+                    matches,
                 extra.get("color_output", False),
                 extra["per_finding_max_lines_limit"],
                 extra["per_line_max_chars_limit"],
                 extra["dataflow_traces"],
             )
-            findings_output.append(
-                "\nFirst-Party Blocking Findings:\n"
-                + "\n".join(first_party_blocking_output)
-            ) if (reachable or unreachable) else findings_output.append(
-                "\nBlocking Findings:\n" + "\n".join(first_party_blocking_output)
-            )
 
-        first_party_blocking_rules_output = []
-        if first_party_blocking_rules:
-            formatted_first_party_blocking_rules = [
-                with_color(Colors.foreground, rule_id, bold=True)
-                for rule_id in sorted(
-                    set(first_party_blocking_rules),
-                    key=first_party_blocking_rules.index,
-                )
-            ]
-            first_party_blocking_rules_output = (
-                [
-                    "\nFirst-Party Blocking Rules Fired:\n   "
-                    + "   \n   ".join(formatted_first_party_blocking_rules)
+            if first_party_blocking_rules and is_ci_invocation:
+                console.print(Title("Blocking Code Rules Fired:", order=2))
+                for rule_id in sorted(first_party_blocking_rules):
+                    console.print(f"  {rule_id}")
+                console.reset_title(order=1)
+
+            if cli_output_extra.time:
+                print_time_summary(cli_output_extra.time, semgrep_structured_errors)
+
+            rules_by_engine = (
+                cli_output_extra.rules_by_engine
+                if cli_output_extra.rules_by_engine
+                else []
+            )
+
+            oss_rules = [
+                with_color(Colors.foreground, rule.value[0].value, bold=True)
+                for rule in rules_by_engine
+                if isinstance(rule.value[1].value, out.OSS)
                 ]
-                if (reachable or unreachable)
-                else [
-                    "\nBlocking Rules Fired:\n   "
-                    + "   \n   ".join(formatted_first_party_blocking_rules)
-                ]
-            )
 
-        return "\n".join(
-            [
-                *findings_output,
-                *first_party_blocking_rules_output,
-                *timing_output,
-            ]
+            if (extra["engine_requested"].is_interfile) and oss_rules:
+                console.print(
+                    "Some rules were run as OSS rules because `interfile: true` was not specified."
+                )
+                if extra.get("verbose_errors"):
+                    console.print("These rules were:\n   " + "   \n   ".join(oss_rules))
+                else:
+                    console.print(
+                        f"{unit_str(len(oss_rules), 'rule')} ran with OSS engine (--verbose to see which)"
         )
+
+        return captured_output.get()
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/formatter/vim.py /home/pad/yy/cli/src/semgrep/formatter/vim.py
--- /tmp/semgrep/cli/src/semgrep/formatter/vim.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/formatter/vim.py	2022-12-06 09:32:02.560492621 +0100
@@ -3,3 +3,3 @@ from typing import Iterable
 from typing import Mapping
 from typing import Sequence
 
@@ -35,5 +35,6 @@ class VimFormatter(BaseFormatter):
         semgrep_structured_errors: Sequence[SemgrepError],
         cli_output_extra: out.CliOutputExtra,
         extra: Mapping[str, Any],
+        is_ci_invocation: bool,
     ) -> str:
         return "\n".join(":".join(self._get_parts(rm)) for rm in rule_matches)
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/git.py /home/pad/yy/cli/src/semgrep/git.py
--- /tmp/semgrep/cli/src/semgrep/git.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/git.py	2023-01-21 17:25:53.730017268 +0100
@@ -57,9 +57,13 @@ class StatusCode:
 class BaselineHandler:
     """
     base_commit: Git ref to compare against
+
+    is_mergebase: Is it safe to assume that the given commit is the mergebase?
+    If not, we have to compute the mergebase ourselves, which can be impossible
+    on shallow checkouts.
     """
 
-    def __init__(self, base_commit: str) -> None:
+    def __init__(self, base_commit: str, is_mergebase: bool = False) -> None:
         """
         Raises Exception if
         - cwd is not in git repo
@@ -68,6 +72,7 @@ class BaselineHandler:
         - there are untracked files that will be overwritten by a file in the base commit
         """
         self._base_commit = base_commit
+        self._is_mergebase = is_mergebase
         self._dirty_paths_by_status: Optional[Dict[str, List[Path]]] = None
 
         try:
@@ -111,9 +116,13 @@ class BaselineHandler:
             self._base_commit,
         ]
         try:
+            if self._is_mergebase:
+                cmd = status_cmd
+            else:
+                cmd = [*status_cmd, "--merge-base"]
             # nosemgrep: python.lang.security.audit.dangerous-subprocess-use.dangerous-subprocess-use
             raw_output = subprocess.run(
-                [*status_cmd, "--merge-base"],
+                cmd,
                 timeout=env.git_command_timeout,
                 capture_output=True,
                 encoding="utf-8",
@@ -187,6 +196,16 @@ class BaselineHandler:
 
         return GitStatus(added, modified, removed, unmerged, renamed)
 
+    def _get_git_merge_base(self) -> str:
+        # If we already know that the base commit is the merge base, just return
+        # the base commit. This allows us to operate on shallow checkouts where
+        # we might not have the information locally to compute the merge base.
+        # In this case, calling `git merge-base` may fail.
+        if self._is_mergebase:
+            return self._base_commit
+        else:
+            return git_check_output(["git", "merge-base", self._base_commit, "HEAD"])
+
     def _get_dirty_paths_by_status(self) -> Dict[str, List[Path]]:
         """
         Returns all paths that have a git status, grouped by change type.
@@ -287,10 +306,7 @@ class BaselineHandler:
 
         current_head = git_check_output(["git", "rev-parse", "HEAD"])
         try:
-            merge_base_sha = git_check_output(
-                ["git", "merge-base", self._base_commit, "HEAD"]
-            )
-
+            merge_base_sha = self._get_git_merge_base()
             logger.debug("Running git checkout for baseline context")
             git_check_output(["git", "reset", "--hard", merge_base_sha])
             logger.debug("Finished git checkout for baseline context")
@@ -329,10 +345,10 @@ class BaselineHandler:
 
     def print_git_log(self) -> None:
         base_commit_sha = git_check_output(["git", "rev-parse", self._base_commit])
-        merge_base_sha = git_check_output(
-            ["git", "merge-base", self._base_commit, "HEAD"]
+        merge_base_sha = self._get_git_merge_base()
+        logger.info(
+            "  Will report findings introduced by these commits (may be incomplete for shallow checkouts):"
         )
-        logger.info("  Will report findings introduced by these commits:")
         log = git_check_output(
             ["git", "log", "--oneline", "--graph", f"{merge_base_sha}..HEAD"]
         )
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/ignores.py /home/pad/yy/cli/src/semgrep/ignores.py
--- /tmp/semgrep/cli/src/semgrep/ignores.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/ignores.py	2022-11-09 10:19:47.750022630 +0100
@@ -145,6 +145,7 @@ class Parser:
     # Parser steps are each represented as Generators. This allows us to chain
     # steps, whether the step is a transformation, a filter, an expansion, or any combination thereof.
 
+    file_path: Path
     base_path: Path
 
     @staticmethod
@@ -176,7 +177,7 @@ class Parser:
             if include_path.is_file():
                 with include_path.open() as include_lines:
                     sub_base = include_path.parent.resolve()
-                    sub_parser = Parser(sub_base)
+                    sub_parser = Parser(file_path=include_path, base_path=sub_base)
                     metrics.add_feature("semgrepignore", "include")
                     return sub_parser.parse(include_lines)
             else:
@@ -184,9 +185,11 @@ class Parser:
                     f"Skipping `:include {include_path}` directive, file not found"
                 )
                 return []
+        elif line == ":":
+            return []
         elif CONTROL_REGEX.match(line):
             raise SemgrepError(
-                f"Unknown ignore directive in Semgrep ignore file at {self.base_path}: '{line}'"
+                f"While parsing .semgrepignore: unknown ignore directive in {self.file_path}: '{line}'"
             )
         else:
             return (line for _ in range(1))
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/join_rule.py /home/pad/yy/cli/src/semgrep/join_rule.py
--- /tmp/semgrep/cli/src/semgrep/join_rule.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/join_rule.py	2023-02-14 10:28:57.440045881 +0100
@@ -369,18 +369,48 @@ def cli_intermediate_vars_to_core_interm
     return [core.CoreMatchIntermediateVar(location=v.location) for v in i_vars]
 
 
+def cli_call_trace_to_core_call_trace(
+    trace: core.CliMatchCallTrace,
+) -> core.CoreMatchCallTrace:
+    value = trace.value
+    if isinstance(value, core.CliLoc):
+        return core.CoreMatchCallTrace(core.CoreLoc(value.value[0]))
+    elif isinstance(value, core.CliCall):
+        data, intermediate_vars, call_trace = value.value
+
+        return core.CoreMatchCallTrace(
+            core.CoreCall(
+                (
+                    data[0],
+                    cli_intermediate_vars_to_core_intermediate_vars(intermediate_vars),
+                    cli_call_trace_to_core_call_trace(call_trace),
+                )
+            )
+        )
+    else:
+        # unreachable, theoretically
+        logger.error("Reached unreachable code in cli call trace to core call trace")
+        return None
+
+
 def cli_trace_to_core_trace(
     trace: core.CliMatchDataflowTrace,
 ) -> core.CoreMatchDataflowTrace:
-    taint_source = trace.taint_source.location if trace.taint_source else None
+    taint_source = trace.taint_source if trace.taint_source else None
+    taint_sink = trace.taint_sink if trace.taint_sink else None
     intermediate_vars = (
         cli_intermediate_vars_to_core_intermediate_vars(trace.intermediate_vars)
         if trace.intermediate_vars
         else None
     )
     return core.CoreMatchDataflowTrace(
-        taint_source=taint_source,
+        taint_source=cli_call_trace_to_core_call_trace(taint_source)
+        if taint_source
+        else None,
         intermediate_vars=intermediate_vars,
+        taint_sink=cli_call_trace_to_core_call_trace(taint_sink)
+        if taint_sink
+        else None,
     )
 
 
@@ -400,6 +430,9 @@ def json_to_rule_match(join_rule: Dict[s
         # not. This is unsafe, but before it was just implicitly unsafe.
         metavars=cli_match_extra.metavars,  # type: ignore[arg-type]
         dataflow_trace=dataflow_trace,
+        engine_kind=cli_match_extra.engine_kind
+        if cli_match_extra.engine_kind
+        else core.EngineKind(core.OSS()),
     )
     return RuleMatch(
         message=join_rule.get(
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/lsp/config.py /home/pad/yy/cli/src/semgrep/lsp/config.py
--- /tmp/semgrep/cli/src/semgrep/lsp/config.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/lsp/config.py	2023-03-17 09:21:14.529881197 +0100
@@ -14,23 +14,23 @@ from typing import Tuple
 from typing import Union
 
 import semgrep.commands.ci
-import semgrep.output_from_core as core
 import semgrep.semgrep_main
 from semgrep.app.scans import ScanHandler
 from semgrep.config_resolver import get_config
 from semgrep.constants import OutputFormat
 from semgrep.constants import RuleSeverity
+from semgrep.engine import EngineType
 from semgrep.error import SemgrepError
 from semgrep.meta import generate_meta_from_environment
 from semgrep.metrics import MetricsState
 from semgrep.output import OutputHandler
 from semgrep.output import OutputSettings
-from semgrep.parsing_data import ParsingData
+from semgrep.output_extra import OutputExtra
 from semgrep.profile_manager import ProfileManager
-from semgrep.profiling import ProfilingData
 from semgrep.project import get_project_url
 from semgrep.rule import Rule
 from semgrep.rule_match import RuleMatchMap
+from semgrep.semgrep_interfaces.semgrep_output_v1 import FoundDependency
 from semgrep.state import get_state
 from semgrep.target_manager import FileTargetingLog
 from semgrep.target_manager import TargetManager
@@ -124,7 +124,7 @@ class LSPConfig:
 
     @property
     def scan_url(self) -> str:
-        scan_handler = ScanHandler(True)
+        scan_handler = ScanHandler(dry_run=True)
         metadata = generate_meta_from_environment(self.baseline_commit)
         state = get_state()
         to_server = (
@@ -236,15 +236,12 @@ class LSPConfig:
             RuleMatchMap,
             List[SemgrepError],
             Set[Path],
-            Set[Path],
             FileTargetingLog,
             List[Rule],
             ProfileManager,
-            ProfilingData,
-            ParsingData,
-            Optional[List[core.MatchingExplanation]],
+            OutputExtra,
             Collection[RuleSeverity],
-            Dict[str, int],
+            Dict[str, List[FoundDependency]],
         ],
     ]:
         """Generate a scanner according to the config"""
@@ -253,7 +250,7 @@ class LSPConfig:
         get_state().metrics.configure(self.metrics, None)
         return partial(
             semgrep.semgrep_main.main,
-            deep=False,
+            engine_type=EngineType.OSS,
             configs=configs,
             severity=self.severity,
             exclude=self.exclude,
@@ -281,15 +278,12 @@ class LSPConfig:
             RuleMatchMap,
             List[SemgrepError],
             Set[Path],
-            Set[Path],
             FileTargetingLog,
             List[Rule],
             ProfileManager,
-            ProfilingData,
-            ParsingData,
-            Optional[List[core.MatchingExplanation]],
+            OutputExtra,
             Collection[RuleSeverity],
-            Dict[str, int],
+            Dict[str, List[FoundDependency]],
         ],
     ]:
         return self._scanner(configs=self.configs)
@@ -303,15 +297,12 @@ class LSPConfig:
             RuleMatchMap,
             List[SemgrepError],
             Set[Path],
-            Set[Path],
             FileTargetingLog,
             List[Rule],
             ProfileManager,
-            ProfilingData,
-            ParsingData,
-            Optional[List[core.MatchingExplanation]],
+            OutputExtra,
             Collection[RuleSeverity],
-            Dict[str, int],
+            Dict[str, List[FoundDependency]],
         ],
     ]:
         return self._scanner(configs=[self.scan_url])
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/lsp/convert.py /home/pad/yy/cli/src/semgrep/lsp/convert.py
--- /tmp/semgrep/cli/src/semgrep/lsp/convert.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/lsp/convert.py	2022-11-02 22:44:28.737000525 +0100
@@ -10,3 +10,3 @@ from semgrep.lsp.types import Diagnostic
 from semgrep.rule import Rule
 from semgrep.rule_match import RuleMatch
 from semgrep.rule_match import RuleMatchMap
@@ -119,7 +119,9 @@ def rule_match_to_diagnostic(rule_match:
         "code": rule_match.rule_id,
         "codeDescription": {"href": rule_url},
         "data": {
-            "matchSource": rule_match.syntactic_context,
+            # Use match_formula_string here since it'll always be more accurate
+            # for fix_regex
+            "matchSource": rule_match.match_formula_string,
             "uri": f"file://{rule_match.path}",
         },
         "relatedInformation": rule_match_get_related(rule_match),
@@ -132,7 +134,7 @@ def rule_match_to_diagnostic(rule_match:
         diagnostic["data"]["fix"] = rule_match.fix
         fix_message = rule_match.fix
     if rule_match.fix_regex:
-        diagnostic["data"]["fix_regex"] = rule_match.fix_regex
+        diagnostic["data"]["fix_regex"] = rule_match.fix_regex.to_json()
         fix_message = rule_match.fix
 
     if fix_message is not None:
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/lsp/run_semgrep.py /home/pad/yy/cli/src/semgrep/lsp/run_semgrep.py
--- /tmp/semgrep/cli/src/semgrep/lsp/run_semgrep.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/lsp/run_semgrep.py	2023-01-31 10:50:32.500002514 +0100
@@ -16,19 +16,16 @@ def run_rules(
     (
         filtered_matches_by_rule,
         _,
-        all_targets,
-        _,
-        _,
-        _,
         _,
         _,
         _,
         _,
+        output_extra,
         _,
         _,
     ) = config.scanner(target=targets)
     # ignore this type since we're doing weird things with partial :O
-    return (filtered_matches_by_rule, all_targets)
+    return (filtered_matches_by_rule, output_extra.all_targets)
 
 
 def run_rules_ci(
@@ -37,16 +34,13 @@ def run_rules_ci(
     (
         filtered_matches_by_rule,
         _,
-        all_targets,
-        _,
-        _,
-        _,
         _,
         _,
         _,
         _,
+        output_extra,
         _,
         _,
     ) = config.scanner_ci(target=targets)
     # ignore this type since we're doing weird things with partial :O
-    return (filtered_matches_by_rule, all_targets)
+    return (filtered_matches_by_rule, output_extra.all_targets)
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/lsp/server.py /home/pad/yy/cli/src/semgrep/lsp/server.py
--- /tmp/semgrep/cli/src/semgrep/lsp/server.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/lsp/server.py	2022-11-02 22:44:28.737000525 +0100
@@ -39,3 +39,3 @@ from semgrep.lsp.types import CodeAction
 from semgrep.lsp.types import Diagnostic
 from semgrep.lsp.types import Range
 from semgrep.lsp.types import TextDocumentItem
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/meta.py /home/pad/yy/cli/src/semgrep/meta.py
--- /tmp/semgrep/cli/src/semgrep/meta.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/meta.py	2023-02-20 09:00:19.780001987 +0100
@@ -9,6 +9,7 @@ from typing import Any
 from typing import Dict
 from typing import Optional
 
+import requests
 from boltons.cacheutils import cachedproperty
 from glom import glom
 from glom import T
@@ -39,6 +40,15 @@ def get_url_from_sstp_url(sstp_url: Opti
         # let's just pick https
         protocol = "https"
 
+    # We need to parse the URL into a clickable format, a la these supported formats:
+    # https://stackoverflow.com/questions/31801271/what-are-the-supported-git-url-formats
+    # We only support a few, though, so we may get `None` if we run into a format we do
+    # not support.
+    # So if we know that this URL is going to be unclickable, we should return
+    # the original URL
+    if None in [protocol, result.resource, result.owner, result.name]:
+        return sstp_url
+
     return f"{protocol}://{result.resource}/{result.owner}/{result.name}"
 
 
@@ -89,7 +99,24 @@ class GitMeta:
 
     @property
     def repo_url(self) -> Optional[str]:
-        return get_url_from_sstp_url(os.getenv("SEMGREP_REPO_URL"))
+        env = get_state().env
+        repo_url = os.getenv("SEMGREP_REPO_URL")
+        if not repo_url:
+            # if the repo URL was not explicitly provided, try getting it from git
+            # nosem: use-git-check-output-helper
+            git_parse = subprocess.run(
+                ["git", "remote", "get-url", "origin"],
+                capture_output=True,
+                encoding="utf-8",
+                timeout=env.git_command_timeout,
+            )
+            if git_parse.returncode != 0:
+                logger.warn(
+                    f"Unable to infer repo_url. Set SEMGREP_REPO_URL environment variable or run in a valid git project with remote origin defined"
+                )
+            repo_url = git_parse.stdout.strip()
+
+        return get_url_from_sstp_url(repo_url)
 
     @property
     def commit_sha(self) -> Optional[str]:
@@ -137,6 +164,10 @@ class GitMeta:
         """
         return git_check_output(["git", "show", "-s", "--format=%ct"])
 
+    @property
+    def is_full_scan(self) -> bool:
+        return self.merge_base_ref is None
+
     def to_dict(self) -> Dict[str, Any]:
         commit_title = git_check_output(["git", "show", "-s", "--format=%B"])
         commit_author_email = git_check_output(["git", "show", "-s", "--format=%ae"])
@@ -162,7 +193,7 @@ class GitMeta:
             "pull_request_id": self.pr_id,
             "pull_request_title": self.pr_title,
             "scan_environment": self.environment,
-            "is_full_scan": self.merge_base_ref is None,
+            "is_full_scan": self.is_full_scan,
         }
 
 
@@ -185,6 +216,10 @@ class GithubMeta(GitMeta):
         return {}
 
     @property
+    def gh_token(self) -> Optional[str]:
+        return os.getenv("GH_TOKEN")
+
+    @property
     def is_pull_request_event(self) -> bool:
         """Return if running on a PR, even for variant types such as `pull_request_target`."""
         return self.event_name in {"pull_request", "pull_request_target"}
@@ -206,6 +241,10 @@ class GithubMeta(GitMeta):
         return None
 
     @property
+    def api_url(self) -> Optional[str]:
+        return os.getenv("GITHUB_API_URL")
+
+    @property
     def commit_sha(self) -> Optional[str]:
         if self.is_pull_request_event:
             # https://github.community/t/github-sha-not-the-same-as-the-triggering-commit/18286/2
@@ -231,6 +270,26 @@ class GithubMeta(GitMeta):
             ]
         )
 
+    def _shallow_fetch_commit(self, commit_hash: str) -> None:
+        """
+        Split out shallow fetch so we can mock it away in tests
+
+        Different from _shallow_fetch_branch because it does not assign a local
+        name to the commit. It just does the fetch.
+        """
+        logger.debug(f"Trying to shallow fetch commit {commit_hash} from origin")
+        git_check_output(
+            [
+                "git",
+                "fetch",
+                "origin",
+                "--depth=1",
+                "--force",
+                "--update-head-ok",
+                commit_hash,
+            ]
+        )
+
     def _get_latest_commit_hash_in_branch(self, branch_name: str) -> str:
         """
         Return sha hash of latest commit in a given branch
@@ -348,6 +407,62 @@ class GithubMeta(GitMeta):
                 ]
             )
 
+        # By default, the GitHub Actions checkout action gives you a shallow
+        # clone of the repository. In order to get the merge base, we need to
+        # fetch the history of the head branch and the base branch, all the way
+        # back to the point where the head branch diverged. In a large
+        # repository, this can be a lot of commits, and this fetching can
+        # dramatically impact performance.
+        #
+        # To avoid this, on the first attempt to find the merge base, we try to
+        # use the GitHub REST API instead of fetching enough history to compute
+        # it locally. We only do this if the `GH_TOKEN` environment variable is
+        # provided. GitHub Actions provides that token to workflows, but the
+        # workflow needs to explicitly make it available to Semgrep via an
+        # environment variable like this:
+        #
+        # env:
+        #   GH_TOKEN: ${{ github.token }}
+        #
+        # This will allow Semgrep to make this API request even for private
+        # repositories.
+        if (
+            attempt_count == 0
+            and self.gh_token is not None
+            and self.api_url is not None
+            and self.repo_name is not None
+        ):
+            logger.debug("Trying to get merge base using GitHub API")
+            try:
+                headers = {"Authorization": f"Bearer {self.gh_token}"}
+                req = requests.get(
+                    f"{self.api_url}/repos/{self.repo_name}/compare/{self.base_branch_hash}...{self.head_branch_hash}",
+                    headers=headers,
+                )
+                if req.status_code == 200:
+                    compare_json = json.loads(req.text)
+                    base = glom(
+                        compare_json, T["merge_base_commit"]["sha"], default=None
+                    )
+                    if type(base) == str:
+                        logger.debug(f"Got merge base using GitHub API: {base}")
+                        # Normally, we fetch commits until we can compute the
+                        # merge base locally. That guarantees that the merge
+                        # base itself has been fetched. However, when we just
+                        # query the GH API, we don't necessarily have anything
+                        # locally. Later steps will check out the merge base, so
+                        # we need to make sure it is available locally or those
+                        # steps will error.
+                        self._shallow_fetch_commit(base)
+                        return base
+            except Exception as e:
+                # We're relying on an external service here. If something goes
+                # wrong, just log the exception and continue with the ordinary
+                # method of computing the merge base.
+                logger.debug(
+                    f"Encountered error while getting merge base using GitHub API: {repr(e)}"
+                )
+
         try:  # check if both branches connect to the yet-unknown branch-off point now
             # nosem: use-git-check-output-helper
             process = subprocess.run(
@@ -543,6 +658,10 @@ class CircleCIMeta(GitMeta):
 
     @property
     def repo_name(self) -> str:
+        repo_name = os.getenv("SEMGREP_REPO_NAME")
+        if repo_name:
+            return repo_name
+
         project_name = os.getenv("CIRCLE_PROJECT_USERNAME", "")
         repo_name = os.getenv("CIRCLE_PROJECT_REPONAME", "")
         if repo_name == "" and project_name == "":
@@ -553,24 +672,44 @@ class CircleCIMeta(GitMeta):
 
     @property
     def repo_url(self) -> Optional[str]:
+        repo_url = os.getenv("SEMGREP_REPO_URL")
+        if repo_url:
+            return repo_url
+
         # may be in SSH url format
         url = get_url_from_sstp_url(os.getenv("CIRCLE_REPOSITORY_URL"))
         return url if url else super().repo_url
 
     @property
     def branch(self) -> Optional[str]:
+        branch = os.getenv("SEMGREP_BRANCH")
+        if branch:
+            return branch
+
         return os.getenv("CIRCLE_BRANCH")
 
     @property
     def ci_job_url(self) -> Optional[str]:
+        job_url = os.getenv("SEMGREP_JOB_URL")
+        if job_url:
+            return job_url
+
         return os.getenv("CIRCLE_BUILD_URL")
 
     @property
     def commit_sha(self) -> Optional[str]:
+        commit = os.getenv("SEMGREP_COMMIT")
+        if commit:
+            return commit
+
         return os.getenv("CIRCLE_SHA1")
 
     @property
     def pr_id(self) -> Optional[str]:
+        pr_id = os.getenv("SEMGREP_PR_ID")
+        if pr_id:
+            return pr_id
+
         # have to use the pull request url to get the id
         return os.getenv("CIRCLE_PULL_REQUEST", "").split("/")[-1]
 
@@ -638,6 +777,10 @@ class BitbucketMeta(GitMeta):
 
     @property
     def repo_name(self) -> str:
+        repo_name = os.getenv("SEMGREP_REPO_NAME")
+        if repo_name:
+            return repo_name
+
         name = os.getenv("BITBUCKET_REPO_FULL_NAME")
         if name is None:
             # try pulling from url
@@ -646,15 +789,29 @@ class BitbucketMeta(GitMeta):
 
     @property
     def repo_url(self) -> Optional[str]:
-        url = get_url_from_sstp_url(os.getenv("BITBUCKET_GIT_HTTP_ORIGIN"))
+        repo_url = os.getenv("SEMGREP_REPO_URL")
+        if repo_url:
+            return repo_url
+
+        # Bitbucket Cloud URLs should be in the format: http://bitbucket.org/<workspace>/<repo>
+        # Bitbucker Server URLs should be in the format: https://bitbucket<company>.com/projects/<PROJECT>/repos/<REPO_NAME>
+        url = os.getenv("BITBUCKET_GIT_HTTP_ORIGIN")
         return url if url else super().repo_url
 
     @property
     def branch(self) -> Optional[str]:
+        branch = os.getenv("SEMGREP_BRANCH")
+        if branch:
+            return branch
+
         return os.getenv("BITBUCKET_BRANCH")
 
     @property
     def ci_job_url(self) -> Optional[str]:
+        job_url = os.getenv("SEMGREP_JOB_URL")
+        if job_url:
+            return job_url
+
         url = "{}/addon/pipelines/home#!/results/{}".format(
             os.getenv("BITBUCKET_GIT_HTTP_ORIGIN"), os.getenv("BITBUCKET_PIPELINE_UUID")
         )
@@ -662,10 +819,17 @@ class BitbucketMeta(GitMeta):
 
     @property
     def commit_sha(self) -> Optional[str]:
+        commit = os.getenv("SEMGREP_COMMIT")
+        if commit:
+            return commit
         return os.getenv("BITBUCKET_COMMIT")
 
     @property
     def pr_id(self) -> Optional[str]:
+        pr_id = os.getenv("SEMGREP_PR_ID")
+        if pr_id:
+            return pr_id
+
         return os.getenv("BITBUCKET_PR_ID")
 
 
@@ -744,6 +908,14 @@ class AzurePipelinesMeta(GitMeta):
             "BUILD_SOURCEVERSION"
         )
 
+    @property
+    def pr_id(self) -> Optional[str]:
+        pr_id = os.getenv("SEMGREP_PR_ID")
+        if pr_id:
+            return pr_id
+
+        return os.getenv("SYSTEM_PULLREQUEST_PULLREQUESTNUMBER")
+
 
 @dataclass
 class BuildkiteMeta(GitMeta):
@@ -753,30 +925,54 @@ class BuildkiteMeta(GitMeta):
 
     @property
     def repo_name(self) -> str:
+        repo_name = os.getenv("SEMGREP_REPO_NAME")
+        if repo_name:
+            return repo_name
+
         name = get_repo_name_from_repo_url(os.getenv("BUILDKITE_REPO"))
         return name if name else super().repo_name
 
     @property
     def repo_url(self) -> Optional[str]:
+        repo_url = os.getenv("SEMGREP_REPO_URL")
+        if repo_url:
+            return repo_url
+
         url = get_url_from_sstp_url(os.getenv("BUILDKITE_REPO"))
         return url if url else super().repo_url
 
     @property
     def branch(self) -> Optional[str]:
+        branch = os.getenv("SEMGREP_BRANCH")
+        if branch:
+            return branch
+
         return os.getenv("BUILDKITE_BRANCH")
 
     @property
     def ci_job_url(self) -> Optional[str]:
+        job_url = os.getenv("SEMGREP_JOB_URL")
+        if job_url:
+            return job_url
+
         return "{}#{}".format(
             os.getenv("BUILDKITE_BUILD_URL"), os.getenv("BUILDKITE_JOB_ID")
         )
 
     @property
     def commit_sha(self) -> Optional[str]:
+        commit = os.getenv("SEMGREP_COMMIT")
+        if commit:
+            return commit
+
         return os.getenv("BUILDKITE_COMMIT")
 
     @property
     def pr_id(self) -> Optional[str]:
+        pr_id = os.getenv("SEMGREP_PR_ID")
+        if pr_id:
+            return pr_id
+
         # might be "false" if there is no PR id
         pr_id = os.getenv("BUILDKITE_PULL_REQUEST")
         return None if pr_id == "false" else pr_id
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/metrics.py /home/pad/yy/cli/src/semgrep/metrics.py
--- /tmp/semgrep/cli/src/semgrep/metrics.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/metrics.py	2023-03-17 09:21:14.529881197 +0100
@@ -16,2 +16,2 @@ from typing import NewType
 
 METRICS_ENDPOINT = "https://metrics.semgrep.dev"
@@ -71,12 +75,15 @@ class FileStats(TypedDict, total=False):
     runTime: Optional[float]
 
 
-class EnvironmentSchema(TypedDict, total=False):
+class EnvironmentRequiredSchema(TypedDict):
     version: str
+
+
+class EnvironmentSchema(EnvironmentRequiredSchema, total=False):
+    ci: Optional[str]
     projectHash: Optional[Sha256Hash]
     configNamesHash: Sha256Hash
     rulesHash: Sha256Hash
-    ci: Optional[str]
     isAuthenticated: bool
 
 
@@ -87,6 +94,7 @@ class PerformanceSchema(TypedDict, total
     numRules: Optional[int]
     numTargets: Optional[int]
     totalBytesScanned: Optional[int]
+    maxMemoryBytes: Optional[int]
 
 
 class ErrorsSchema(TypedDict, total=False):
@@ -99,6 +107,7 @@ class ValueRequiredSchema(TypedDict):
 
 
 class ValueSchema(ValueRequiredSchema, total=False):
+    engineRequested: str
     numFindings: int
     numIgnored: int
     ruleHashesWithFindings: Dict[str, int]
@@ -117,10 +126,13 @@ class ParseStatSchema(TypedDict, total=F
     num_bytes: int
 
 
-class TopLevelSchema(TypedDict, total=False):
+class TopLevelRequiredSchema(TypedDict):
     event_id: uuid.UUID
-    anonymous_user_id: str
     started_at: datetime
+
+
+class TopLevelSchema(TopLevelRequiredSchema, total=False):
+    anonymous_user_id: str
     sent_at: datetime
 
 
@@ -175,20 +187,19 @@ class Metrics:
     metrics_state: MetricsState = MetricsState.OFF
     payload: PayloadSchema = Factory(
         lambda: PayloadSchema(
-            environment=EnvironmentSchema(),
+            environment=EnvironmentSchema(version=__VERSION__),
             errors=ErrorsSchema(),
             performance=PerformanceSchema(),
             value=ValueSchema(features=set()),
             fix_rate=FixRateSchema(),
             parse_rate=dict(),
+            started_at=datetime.now(),
+            event_id=uuid.uuid4(),
         )
     )
 
     def __attrs_post_init__(self) -> None:
-        self.payload["started_at"] = datetime.now()
-        self.payload["environment"]["version"] = __VERSION__
         self.payload["environment"]["ci"] = os.getenv("CI")
-        self.payload["event_id"] = uuid.uuid4()
 
     def configure(
         self,
@@ -215,17 +226,19 @@ class Metrics:
             )
         self.metrics_state = metrics_state or legacy_state or MetricsState.AUTO
 
+    @suppress_errors
+    def add_engine_type(self, engineType: "EngineType") -> None:
+        """
+        Assumes configs is list of arguments passed to semgrep using --config
+        """
+        self.payload["value"]["engineRequested"] = engineType.name
+
     @property
     def is_using_registry(self) -> bool:
         return self._is_using_registry
 
     @is_using_registry.setter
     def is_using_registry(self, value: bool) -> None:
-        if self.is_using_registry is False and value is True:
-            logger.info(
-                f"Semgrep rule registry URL is {os.environ.get('SEMGREP_URL', 'https://semgrep.dev/registry')}."
-            )
-
         self._is_using_registry = value
 
     @suppress_errors
@@ -281,6 +294,12 @@ class Metrics:
         ]
 
     @suppress_errors
+    def add_max_memory_bytes(self, profiling_data: ProfilingData) -> None:
+        self.payload["performance"][
+            "maxMemoryBytes"
+        ] = profiling_data.get_max_memory_bytes()
+
+    @suppress_errors
     def add_findings(self, findings: FilteredMatches) -> None:
         self.payload["value"]["ruleHashesWithFindings"] = {
             r.full_hash: len(f) for r, f in findings.kept.items()
@@ -391,8 +410,20 @@ class Metrics:
           - on, sends
           - off, doesn't send
         """
+        # import here to prevent circular import
+        from semgrep.state import get_state
+
+        state = get_state()
+
         if self.metrics_state == MetricsState.AUTO:
-            return self.is_using_registry
+            # When running logged in with `semgrep ci`, configs are
+            # resolved before `self.is_using_registry` is set.
+            # However, these scans are still pulling from the registry
+            using_app = (
+                state.command.get_subcommand() == "ci"
+                and state.app_session.is_authenticated
+            )
+            return self.is_using_registry or using_app
         return self.metrics_state == MetricsState.ON
 
     @suppress_errors
@@ -409,6 +440,22 @@ class Metrics:
             if source == click.core.ParameterSource.PROMPT:
                 self.add_feature("cli-prompt", param)
 
+    # Posting the metrics is separated out so that our tests can check
+    # for it
+    # TODO it's a bit unfortunate that our tests are going to post
+    # metrics...
+    def _post_metrics(self, user_agent: str) -> None:
+        r = requests.post(
+            METRICS_ENDPOINT,
+            data=self.as_json(),
+            headers={
+                "Content-Type": "application/json",
+                "User-Agent": user_agent,
+            },
+            timeout=3,
+        )
+        r.raise_for_status()
+
     @suppress_errors
     def send(self) -> None:
         """
@@ -430,13 +477,4 @@ class Metrics:
         self.payload["sent_at"] = datetime.now()
         self.payload["anonymous_user_id"] = state.settings.get("anonymous_user_id")
 
-        r = requests.post(
-            METRICS_ENDPOINT,
-            data=self.as_json(),
-            headers={
-                "Content-Type": "application/json",
-                "User-Agent": str(state.app_session.user_agent),
-            },
-            timeout=3,
-        )
-        r.raise_for_status()
+        self._post_metrics(str(state.app_session.user_agent))
Only in /home/pad/yy/cli/src/semgrep: output_extra.py
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/output_from_core.py /home/pad/yy/cli/src/semgrep/output_from_core.py
--- /tmp/semgrep/cli/src/semgrep/output_from_core.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/output_from_core.py	2022-11-02 22:44:28.737000525 +0100
@@ -1,0 +1,0 @@
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/output.py /home/pad/yy/cli/src/semgrep/output.py
--- /tmp/semgrep/cli/src/semgrep/output.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/output.py	2023-03-25 17:10:17.159558674 +0100
@@ -20,9 +20,12 @@ from typing import Type
 import requests
 from boltons.iterutils import partition
 
+from semgrep.console import console
+from semgrep.console import Title
 from semgrep.constants import Colors
 from semgrep.constants import OutputFormat
 from semgrep.constants import RuleSeverity
+from semgrep.engine import EngineType
 from semgrep.error import FINDINGS_EXIT_CODE
 from semgrep.error import Level
 from semgrep.error import SemgrepCoreError
@@ -127,6 +130,7 @@ def _build_time_json(
             for target, num_bytes in zip(targets, target_bytes)
         ],
         total_bytes=sum(n for n in target_bytes),
+        max_memory_bytes=profiling_data.get_max_memory_bytes(),
     )
 
 
@@ -173,12 +177,15 @@ class OutputHandler:
         self.semgrep_structured_errors: List[SemgrepError] = []
         self.error_set: Set[SemgrepError] = set()
         self.has_output = False
+        self.is_ci_invocation = False
         self.filtered_rules: List[Rule] = []
         self.profiling_data: ProfilingData = (
             ProfilingData()
         )  # (rule, target) -> duration
         self.severities: Collection[RuleSeverity] = DEFAULT_SHOWN_SEVERITIES
         self.explanations: Optional[List[out.MatchingExplanation]] = None
+        self.rules_by_engine: Optional[List[out.RuleIdAndEngineKind]] = None
+        self.engine_type: EngineType = EngineType.OSS
 
         self.final_error: Optional[Exception] = None
         formatter_type = FORMATTERS.get(self.settings.output_format)
@@ -253,7 +260,10 @@ class OutputHandler:
         if ex is None:
             return
         if isinstance(ex, SemgrepError):
-            if ex.level == Level.ERROR:
+            if ex.level == Level.ERROR and not (
+                isinstance(ex, SemgrepCoreError)
+                and ex.is_special_interfile_analysis_error
+            ):
                 raise ex
             elif self.settings.strict:
                 raise ex
@@ -289,8 +299,11 @@ class OutputHandler:
         profiler: Optional[ProfileManager] = None,
         profiling_data: Optional[ProfilingData] = None,  # (rule, target) -> duration
         explanations: Optional[List[out.MatchingExplanation]] = None,
+        rules_by_engine: Optional[List[out.RuleIdAndEngineKind]] = None,
         severities: Optional[Collection[RuleSeverity]] = None,
         print_summary: bool = False,
+        is_ci_invocation: bool = False,
+        engine_type: EngineType = EngineType.OSS,
     ) -> None:
         state = get_state()
         self.has_output = True
@@ -304,6 +317,8 @@ class OutputHandler:
         self.all_targets = all_targets
         self.filtered_rules = filtered_rules
 
+        self.engine_type = engine_type
+
         if ignore_log:
             self.ignore_log = ignore_log
         else:
@@ -315,9 +330,13 @@ class OutputHandler:
             self.profiling_data = profiling_data
         if explanations:
             self.explanations = explanations
+        if rules_by_engine:
+            self.rules_by_engine = rules_by_engine
         if severities:
             self.severities = severities
 
+        self.is_ci_invocation = is_ci_invocation
+
         final_error = None
         any_findings_not_ignored = any(not rm.is_ignored for rm in self.rule_matches)
 
@@ -350,6 +369,8 @@ class OutputHandler:
             else:
                 if output:
                     try:
+                        # console.print() would go to stderr; here we print() directly to stdout
+                        # the output string is already pre-formatted by semgrep.console
                         print(output)
                     except UnicodeEncodeError as ex:
                         raise Exception(
@@ -381,15 +402,20 @@ class OutputHandler:
                 stats_line = f"\nRan {unit_str(num_rules, 'rule')} on {unit_str(num_targets, 'file')}: {unit_str(num_findings, 'finding')}."
             if ignore_log is not None:
                 logger.verbose(ignore_log.verbose_output())
-            output_text = "\n" + ignores_line + suggestion_line + stats_line
+
+            output_text = ignores_line + suggestion_line + stats_line
+            console.print(Title("Scan Summary"))
             logger.info(output_text)
 
         self._final_raise(final_error)
 
     def _save_output(self, destination: str, output: str) -> None:
+        metrics = get_state().metrics
         if is_url(destination):
+            metrics.add_feature("output", "url")
             self._post_output(destination, output)
         else:
+            metrics.add_feature("output", "path")
             save_path = Path(destination)
             # create the folders if not exists
             save_path.parent.mkdir(parents=True, exist_ok=True)
@@ -406,9 +432,7 @@ class OutputHandler:
         except requests.exceptions.Timeout:
             raise SemgrepError(f"posting output to {output_url} timed out")
 
-    def _build_output(
-        self,
-    ) -> str:
+    def _build_output(self) -> str:
         # CliOutputExtra members
         cli_paths = out.CliPaths(
             scanned=[str(path) for path in sorted(self.all_targets)],
@@ -416,7 +440,10 @@ class OutputHandler:
             skipped=None,
         )
         cli_timing: Optional[out.CliTiming] = None
+
         explanations: Optional[List[out.MatchingExplanation]] = self.explanations
+        rules_by_engine: Optional[List[out.RuleIdAndEngineKind]] = self.rules_by_engine
+
         # Extra, extra! This just in! ðŸ—žï¸
         # The extra dict is for blatantly skipping type checking and function signatures.
         # - The text formatter uses it to store settings
@@ -441,6 +468,7 @@ class OutputHandler:
                     for x in skipped
                 ],
             )
+            extra["verbose_errors"] = True
         else:
             cli_paths = dataclasses.replace(
                 cli_paths, _comment="<add --verbose for a list of skipped paths>"
@@ -456,6 +484,11 @@ class OutputHandler:
                 "per_line_max_chars_limit"
             ] = self.settings.output_per_line_max_chars_limit
             extra["dataflow_traces"] = self.settings.dataflow_traces
+        if self.settings.output_format == OutputFormat.SARIF:
+            extra["dataflow_traces"] = self.settings.dataflow_traces
+
+        # as opposed to below, we need to distinguish the various kinds of pro engine
+        extra["engine_requested"] = self.engine_type
 
         # the rules are used only by the SARIF formatter
         return self.formatter.output(
@@ -463,8 +496,14 @@ class OutputHandler:
             self.rule_matches,
             self.semgrep_structured_errors,
             out.CliOutputExtra(
-                paths=cli_paths, time=cli_timing, explanations=explanations
+                paths=cli_paths,
+                time=cli_timing,
+                explanations=explanations,
+                rules_by_engine=rules_by_engine,
+                # this flattens the information into just distinguishing "pro" and "not-pro"
+                engine_requested=self.engine_type.to_engine_kind(),
             ),
             extra,
             self.severities,
+            is_ci_invocation=self.is_ci_invocation,
         )
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/profiling.py /home/pad/yy/cli/src/semgrep/profiling.py
--- /tmp/semgrep/cli/src/semgrep/profiling.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/profiling.py	2022-12-06 09:32:02.560492621 +0100
@@ -30,6 +30,8 @@ class ProfilingData:
         self._file_match_times: Dict[Path, float] = defaultdict(float)
         self._file_num_times_scanned: Dict[Path, int] = defaultdict(int)
 
+        self._max_memory_bytes: Optional[int] = None
+
     def get_run_times(self, rule: Rule, target: Path) -> Times:
         return self._match_time_matrix[Semgrep_run(rule=rule.id2, target=target)]
 
@@ -84,6 +86,13 @@ class ProfilingData:
         """
         return self._file_num_times_scanned[target]
 
+    def get_max_memory_bytes(self) -> Optional[int]:
+        """
+        Returns the amount of bytes of memory used during Semgrep's
+        run on the OCaml side.
+        """
+        return self._max_memory_bytes
+
     def set_file_times(
         self, target: Path, times: Dict[core.RuleId, Times], run_time: float
     ) -> None:
@@ -109,3 +118,6 @@ class ProfilingData:
 
     def set_rules_parse_time(self, parse_time: float) -> None:
         self._rules_parse_time = parse_time
+
+    def set_max_memory_bytes(self, max_memory_bytes: int) -> None:
+        self._max_memory_bytes = max_memory_bytes
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/rule_lang.py /home/pad/yy/cli/src/semgrep/rule_lang.py
--- /tmp/semgrep/cli/src/semgrep/rule_lang.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/rule_lang.py	2023-02-14 10:28:57.440045881 +0100
@@ -26,3 +26,3 @@ from ruamel.yaml import Node
 from ruamel.yaml import RoundTripConstructor
 from ruamel.yaml import YAML
 
@@ -439,6 +439,7 @@ class RuleValidation:
     REQUIRE_REGEX = re.compile(r"'(.*)' is a required property")
     PATTERN_KEYS = {
         "match",
+        "taint",  # for new-syntax taint mode rules
         "pattern",
         "pattern-either",
         "pattern-regex",
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/rule_match.py /home/pad/yy/cli/src/semgrep/rule_match.py
--- /tmp/semgrep/cli/src/semgrep/rule_match.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/rule_match.py	2023-03-25 17:10:17.159558674 +0100
@@ -1,6 +1,5 @@
 import binascii
 import hashlib
-import itertools
 import textwrap
 from collections import Counter
 from datetime import datetime
@@ -22,7 +21,8 @@ from attrs import field
 from attrs import frozen
 
 import semgrep.output_from_core as core
 from semgrep.rule import Rule
+from semgrep.rule import RuleProduct
 from semgrep.util import get_lines
 
 if TYPE_CHECKING:
@@ -100,6 +100,10 @@ class RuleMatch:
     match_based_key: Tuple = field(init=False, repr=False)
     syntactic_id: str = field(init=False, repr=False)
     match_based_id: str = field(init=False, repr=False)
+    code_hash: str = field(init=False, repr=False)
+    pattern_hash: str = field(init=False, repr=False)
+    start_line_hash: str = field(init=False, repr=False)
+    end_line_hash: str = field(init=False, repr=False)
 
     # TODO: return a out.RuleId
     @property
@@ -118,6 +122,17 @@ class RuleMatch:
     def end(self) -> core.Position:
         return self.match.location.end
 
+    @property
+    def product(self) -> RuleProduct:
+        return RuleProduct.sca if "sca_info" in self.extra else RuleProduct.sast
+
+    def get_individual_line(self, line_number: int) -> str:
+        line_array = get_lines(self.path, line_number, line_number)
+        if len(line_array) == 0:
+            return ""
+        else:
+            return line_array[0]
+
     @lines.default
     def get_lines(self) -> List[str]:
         """
@@ -135,26 +150,10 @@ class RuleMatch:
         """Return the line preceding the match, if any.
 
         This is meant for checking for the presence of a nosemgrep comment.
-        This implementation was derived from the 'lines' method below.
-        Refer to it for relevant comments.
-        IT feels like a lot of duplication. Feel free to improve.
-        """
-        # see comments in '_get_lines' method
-        start_line = self.start.line - 2
-        end_line = start_line + 1
-        is_empty_file = self.end.line <= 0
-
-        if start_line < 0 or is_empty_file:
-            # no previous line
-            return ""
-
-        with self.path.open(buffering=1, errors="replace") as fd:
-            res = list(itertools.islice(fd, start_line, end_line))
-
-        if res:
-            return res[0]
-        else:
-            return ""
+        """
+        return (
+            self.get_individual_line(self.start.line - 1) if self.start.line > 1 else ""
+        )
 
     @syntactic_context.default
     def get_syntactic_context(self) -> str:
@@ -190,6 +189,12 @@ class RuleMatch:
             self.start.offset,
             self.end.offset,
             self.message,
+            # TODO: Bring this back.
+            # This is necessary so we don't deduplicate taint findings which have different sources.
+            # self.match.extra.dataflow_trace.to_json_string
+            # if self.match.extra.dataflow_trace
+            # else None,
+            None,
         )
 
     @ci_unique_key.default
@@ -242,7 +247,7 @@ class RuleMatch:
         """
         A 32-character hash representation of ci_unique_key.
 
-        This value is sent to semgrep.dev and used as a unique key in the database.
+        This value is sent to semgrep.dev and used to track findings across branches
         """
         # Upon reviewing an old decision,
         # there's no good reason for us to use MurmurHash3 here,
@@ -284,6 +289,60 @@ class RuleMatch:
         match_id_str = str(match_id)
         return f"{hashlib.blake2b(str.encode(match_id_str)).hexdigest()}_{str(self.match_based_index)}"
 
+    @code_hash.default
+    def get_code_hash(self) -> str:
+        """
+        A 32-character hash representation of syntactic_context.
+
+        We started collecting this in addition to syntactic_id because the syntactic_id changes
+        whenever the file path or rule name or index changes, but all of those can be sent in
+        plaintext, so it does not make sense to include them inside a hash.
+
+        By sending the hash of ONLY the code contents, we can determine whether a finding
+        has moved files (path changed), or moved down a file (index changed) and handle that
+        logic in the app. This also gives us more flexibility for changing the definition of
+        a unique finding in the future, since we can analyze things like code, file, index all
+        independently of one another.
+        """
+        return hashlib.sha256(self.syntactic_context.encode()).hexdigest()
+
+    @pattern_hash.default
+    def get_pattern_hash(self) -> str:
+        """
+        A 32-character hash representation of syntactic_context.
+
+        We started collecting this in addition to match_based_id because the match_based_id will
+        change when the file path or rule name or index changes, but all of those can be passed
+        in plaintext, so it does not make sense to include them in the hash.
+
+        By sending the hash of ONLY the pattern contents, we can determine whether a finding
+        has only changed because e.g. the file path changed
+        """
+        match_formula_str = self.match_formula_string
+        if self.extra.get("metavars") is not None:
+            metavars = self.extra["metavars"]
+            for metavar in metavars:
+                match_formula_str = match_formula_str.replace(
+                    metavar, metavars[metavar]["abstract_content"]
+                )
+        return hashlib.sha256(match_formula_str.encode()).hexdigest()
+
+    @start_line_hash.default
+    def get_start_line_hash(self) -> str:
+        """
+        A 32-character hash of the first line of the code in the match
+        """
+        first_line = self.get_individual_line(self.start.line)
+        return hashlib.sha256(first_line.encode()).hexdigest()
+
+    @end_line_hash.default
+    def get_end_line_hash(self) -> str:
+        """
+        A 32-character hash of the last line of the code in the match
+        """
+        last_line = self.get_individual_line(self.end.line)
+        return hashlib.sha256(last_line.encode()).hexdigest()
+
     @property
     def uuid(self) -> UUID:
         """
@@ -304,19 +363,46 @@ class RuleMatch:
 
     @property
     def dataflow_trace(self) -> Optional[core.CliMatchDataflowTrace]:
+        # We need this to quickly get augment a Location with the contents of the location
+        # Convenient to just have it as a separate function
+        def translate_loc(location: core.Location) -> Tuple[core.Location, str]:
+            with open(location.path, errors="replace") as fd:
+                content = util.read_range(
+                    fd, location.start.offset, location.end.offset
+                )
+            return (location, content)
+
+        def translate_core_match_call_trace(
+            call_trace: core.CoreMatchCallTrace,
+        ) -> core.CliMatchCallTrace:
+            if isinstance(call_trace.value, core.CoreLoc):
+                return core.CliMatchCallTrace(
+                    core.CliLoc(translate_loc(call_trace.value.value))
+                )
+            elif isinstance(call_trace.value, core.CoreCall):
+                intermediate_vars = [
+                    core.CliMatchIntermediateVar(*translate_loc(var.location))
+                    for var in call_trace.value.value[1]
+                ]
+
+                return core.CliMatchCallTrace(
+                    core.CliCall(
+                        (
+                            translate_loc(call_trace.value.value[0]),
+                            intermediate_vars,
+                            translate_core_match_call_trace(call_trace.value.value[2]),
+                        )
+                    )
+                )
+
         dataflow_trace = self.match.extra.dataflow_trace
         if dataflow_trace:
             taint_source = None
             intermediate_vars = None
+            taint_sink = None
             if dataflow_trace.taint_source:
-                location = dataflow_trace.taint_source
-                with open(location.path, errors="replace") as fd:
-                    content = util.read_range(
-                        fd, location.start.offset, location.end.offset
-                    )
-                taint_source = core.CliMatchTaintSource(
-                    location=location,
-                    content=content,
+                taint_source = translate_core_match_call_trace(
+                    dataflow_trace.taint_source
                 )
             if dataflow_trace.intermediate_vars:
                 intermediate_vars = []
@@ -335,12 +421,35 @@ class RuleMatch:
                             content=content,
                         )
                     )
+            if dataflow_trace.taint_sink:
+                taint_sink = translate_core_match_call_trace(dataflow_trace.taint_sink)
             return core.CliMatchDataflowTrace(
                 taint_source=taint_source,
                 intermediate_vars=intermediate_vars,
+                taint_sink=taint_sink,
             )
         return None
 
+    @property
+    def exposure_type(self) -> Optional[str]:
+        """
+        Mimic the exposure categories on semgrep.dev for supply chain.
+
+        "reachable": dependency is used in the codebase or is vulnerable even without usage
+        "unreachable": dependency is not used in the codebase
+        "undetermined": rule for dependency doesn't look for reachability
+        None: not a supply chain rule
+        """
+        if "sca_info" not in self.extra:
+            return None
+
+        if self.metadata.get("sca-kind") == "upgrade-only":
+            return "reachable"
+        elif self.metadata.get("sca-kind") == "legacy":
+            return "undetermined"
+        else:
+            return "reachable" if self.extra["sca_info"].reachable else "unreachable"
+
     def to_app_finding_format(self, commit_date: str) -> out.Finding:
         """
         commit_date here for legacy reasons.
@@ -358,6 +467,13 @@ class RuleMatch:
         else:
             app_severity = 0
 
+        hashes = out.FindingHashes(
+            start_line_hash=self.start_line_hash,
+            end_line_hash=self.end_line_hash,
+            code_hash=self.code_hash,
+            pattern_hash=self.pattern_hash,
+        )
+
         ret = out.Finding(
             check_id=out.RuleId(self.rule_id),
             path=str(self.path),
@@ -371,6 +487,7 @@ class RuleMatch:
             commit_date=commit_date_app_format,
             syntactic_id=self.syntactic_id,
             match_based_id=self.match_based_id,
+            hashes=hashes,
             metadata=out.RawJson(self.metadata),
             is_blocking=self.is_blocking,
             dataflow_trace=self.dataflow_trace,
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/rule.py /home/pad/yy/cli/src/semgrep/rule.py
--- /tmp/semgrep/cli/src/semgrep/rule.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/rule.py	2023-03-17 09:21:14.529881197 +0100
@@ -1,5 +1,7 @@
 import hashlib
 import json
+from enum import auto
+from enum import Enum
 from typing import Any
 from typing import AnyStr
 from typing import cast
@@ -18,9 +20,14 @@ from semgrep.rule_lang import RuleValida
 from semgrep.rule_lang import Span
 from semgrep.rule_lang import YamlMap
 from semgrep.rule_lang import YamlTree
 from semgrep.semgrep_types import SEARCH_MODE
 
 
+class RuleProduct(Enum):
+    sast = auto()
+    sca = auto()
+
+
 class Rule:
     def __init__(
         self, raw: Dict[str, Any], yaml: Optional[YamlTree[YamlMap]] = None
@@ -125,7 +132,7 @@ class Rule:
 
     @property
     def message(self) -> str:
-        return str(self._raw["message"])
+        return str(self._raw.get("message"))
 
     @property
     def metadata(self) -> Dict[str, Any]:
@@ -235,6 +242,14 @@ class Rule:
         return any(key in RuleValidation.PATTERN_KEYS for key in self._raw)
 
     @property
+    def product(self) -> RuleProduct:
+        return (
+            RuleProduct.sca
+            if "r2c-internal-project-depends-on" in self._raw
+            else RuleProduct.sast
+        )
+
+    @property
     def formula_string(self) -> str:
         """
         Used to calculate a pattern based ID, works through DFS of all
@@ -254,10 +269,10 @@ class Rule:
                 for key in sorted(raw.keys()):
                     next_raw = raw.get(key)
                     if next_raw is not None:
-                        patterns_to_add.append(get_subrules(next_raw))
+                        patterns_to_add.append(get_subrules(next_raw))  # type: ignore[arg-type]
             elif isinstance(raw, list):
                 for p in raw:
-                    patterns_to_add.append(get_subrules(p))
+                    patterns_to_add.append(get_subrules(p))  # type: ignore[arg-type]
             else:
                 raise ValueError(
                     f"This rule contains an unexpected pattern key: {self.id}:\n {str(raw)}"
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/semgrep_core.py /home/pad/yy/cli/src/semgrep/semgrep_core.py
--- /tmp/semgrep/cli/src/semgrep/semgrep_core.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/semgrep_core.py	2023-02-20 09:00:19.790001987 +0100
@@ -3,6 +3,7 @@ import os
 import shutil
 import sys
 import types
+from pathlib import Path
 from typing import Optional
 
 from semgrep.verbose_logging import getLogger
@@ -48,7 +49,7 @@ class SemgrepCore:
     # yet.
     _SEMGREP_PATH_: Optional[str] = None
 
-    _DEEP_PATH_: Optional[str] = None
+    _PRO_PATH_: Optional[str] = None
 
     # Reference to the bridge module if we are using it.
     _bridge_module: Optional[types.ModuleType] = None
@@ -70,7 +71,7 @@ class SemgrepCore:
         return ret
 
     @classmethod
-    def path(cls) -> str:
+    def path(cls) -> Path:
         """
         Return the path to the semgrep binary, either the Python module
         or the stand-alone program.  Raise Exception if neither is
@@ -113,7 +114,7 @@ class SemgrepCore:
 
                 cls._SEMGREP_PATH_ = cls.executable_path()
 
-        return cls._SEMGREP_PATH_
+        return Path(cls._SEMGREP_PATH_)
 
     @classmethod
     def get_bridge_module(cls) -> types.ModuleType:
@@ -136,7 +137,8 @@ class SemgrepCore:
         return cls._bridge_module is not None
 
     @classmethod
-    def deep_path(cls) -> Optional[str]:
-        if cls._DEEP_PATH_ is None:
-            cls._DEEP_PATH_ = compute_executable_path("deep-semgrep")
-        return cls._DEEP_PATH_
+    def pro_path(cls) -> Optional[Path]:
+        if cls._PRO_PATH_ is None:
+            cls._PRO_PATH_ = compute_executable_path("semgrep-core-proprietary")
+
+        return Path(cls._PRO_PATH_) if cls._PRO_PATH_ is not None else None
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/semgrep_main.py /home/pad/yy/cli/src/semgrep/semgrep_main.py
--- /tmp/semgrep/cli/src/semgrep/semgrep_main.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/semgrep_main.py	2023-03-30 10:07:29.759984029 +0200
@@ -3,6 +3,8 @@ import time
 from io import StringIO
 from os import environ
 from pathlib import Path
+from sys import getrecursionlimit
+from sys import setrecursionlimit
 from typing import Any
 from typing import Collection
 from typing import Dict
@@ -13,16 +15,22 @@ from typing import Set
 from typing import Tuple
 from typing import Union
 
+from boltons.iterutils import get_path
 from boltons.iterutils import partition
+from rich.padding import Padding
 
-import semgrep.output_from_core as out
+from semdep.parse_lockfile import parse_lockfile_path
 from semgrep import __VERSION__
 from semgrep.autofix import apply_fixes
 from semgrep.config_resolver import get_config
+from semgrep.console import console
+from semgrep.console import Title
 from semgrep.constants import DEFAULT_TIMEOUT
 from semgrep.constants import OutputFormat
 from semgrep.constants import RuleSeverity
 from semgrep.core_runner import CoreRunner
+from semgrep.core_runner import Plan
+from semgrep.engine import EngineType
 from semgrep.error import FilesNotFoundError
 from semgrep.error import MISSING_CONFIG_EXIT_CODE
 from semgrep.error import SemgrepError
@@ -35,15 +43,17 @@ from semgrep.nosemgrep import process_ig
 from semgrep.output import DEFAULT_SHOWN_SEVERITIES
 from semgrep.output import OutputHandler
 from semgrep.output import OutputSettings
-from semgrep.parsing_data import ParsingData
+from semgrep.output_extra import OutputExtra
 from semgrep.profile_manager import ProfileManager
-from semgrep.profiling import ProfilingData
 from semgrep.project import get_project_url
 from semgrep.rule import Rule
+from semgrep.rule import RuleProduct
 from semgrep.rule_match import RuleMatchMap
 from semgrep.rule_match import RuleMatchSet
+from semgrep.semgrep_interfaces.semgrep_output_v1 import FoundDependency
 from semgrep.semgrep_types import JOIN_MODE
 from semgrep.state import get_state
+from semgrep.target_manager import ECOSYSTEM_TO_LOCKFILES
 from semgrep.target_manager import FileTargetingLog
 from semgrep.target_manager import TargetManager
 from semgrep.util import unit_str
@@ -82,7 +92,7 @@ def get_file_ignore() -> FileIgnore:
     with semgrepignore_path.open() as f:
         file_ignore = FileIgnore.from_unprocessed_patterns(
             base_path=workdir,
-            patterns=Parser(workdir).parse(f),
+            patterns=Parser(file_path=semgrepignore_path, base_path=workdir).parse(f),
         )
 
     return file_ignore
@@ -108,12 +118,9 @@ def invoke_semgrep(
         _,
         _,
         _,
-        _,
         filtered_rules,
         profiler,
-        profiling_data,
-        _,
-        explanations,
+        output_extra,
         shown_severities,
         _,
     ) = main(
@@ -130,28 +137,79 @@ def invoke_semgrep(
         m for ms in filtered_matches_by_rule.values() for m in ms
     ]
     output_handler.profiler = profiler
-    output_handler.profiling_data = profiling_data
+    output_handler.profiling_data = output_extra.profiling_data
     output_handler.severities = shown_severities
-    output_handler.explanations = explanations
+    output_handler.explanations = output_extra.explanations
+    output_handler.rules_by_engine = output_extra.rules_by_engine
 
     return json.loads(output_handler._build_output())  # type: ignore
 
 
+def print_summary_line(
+    target_manager: TargetManager, sast_plan: Plan, sca_plan: Plan
+) -> None:
+    file_count = len(target_manager.get_all_files())
+    summary_line = f"Scanning {unit_str(file_count, 'file')}"
+    if target_manager.respect_git_ignore:
+        summary_line += " tracked by git"
+
+    sast_rule_count = len(sast_plan.rules)
+    summary_line += f" with {unit_str(sast_rule_count, 'Code rule')}"
+
+    sca_rule_count = len(sca_plan.rules)
+    if sca_rule_count:
+        summary_line += f", {unit_str(sca_rule_count, 'Supply Chain rule')}"
+
+    pro_rule_count = sum(
+        1
+        for rule in sast_plan.rules
+        if get_path(rule.metadata, ("semgrep.dev", "rule", "origin"), default=None)
+        == "pro_rules"
+    )
+    if pro_rule_count:
+        summary_line += f", {unit_str(pro_rule_count, 'Pro rule')}"
+
+    console.print(summary_line + ":")
+
+
+def print_scan_status(rules: Sequence[Rule], target_manager: TargetManager) -> None:
+    """Print a section like:"""
+    console.print(Title("Scan Status"))
+
+    sast_plan = CoreRunner.plan_core_run(
+        [rule for rule in rules if rule.product == RuleProduct.sast],
+        target_manager,
+    )
+    sca_plan = CoreRunner.plan_core_run(
+        [rule for rule in rules if rule.product == RuleProduct.sca],
+        target_manager,
+    )
+
+    print_summary_line(target_manager, sast_plan, sca_plan)
+
+    if not sca_plan.rules:
+        # just print these tables without the section headers
+        sast_plan.print(with_tables_for=RuleProduct.sast)
+        return
+
+    console.print(Padding(Title("Code Rules", order=2), (1, 0, 0, 0)))
+    sast_plan.print(with_tables_for=RuleProduct.sast)
+    console.print(Title("Supply Chain Rules", order=2))
+    sca_plan.print(with_tables_for=RuleProduct.sca)
+
+
 def run_rules(
     filtered_rules: List[Rule],
     target_manager: TargetManager,
     core_runner: CoreRunner,
     output_handler: OutputHandler,
     dump_command_for_core: bool,
-    deep: bool,
+    engine_type: EngineType,
 ) -> Tuple[
-    RuleMatchMap,
-    List[SemgrepError],
-    Set[Path],
-    ProfilingData,
-    ParsingData,
-    Optional[List[out.MatchingExplanation]],
+    RuleMatchMap, List[SemgrepError], OutputExtra, Dict[str, List[FoundDependency]]
 ]:
+    print_scan_status(filtered_rules, target_manager)
+
     join_rules, rest_of_the_rules = partition(
         filtered_rules, lambda rule: rule.mode == JOIN_MODE
     )
@@ -160,15 +218,8 @@ def run_rules(
         rest_of_the_rules, lambda rule: not rule.should_run_on_semgrep_core
     )
 
-    (
-        rule_matches_by_rule,
-        semgrep_errors,
-        all_targets,
-        profiling_data,
-        parsing_data,
-        explanations,
-    ) = core_runner.invoke_semgrep(
-        target_manager, rest_of_the_rules, dump_command_for_core, deep
+    (rule_matches_by_rule, semgrep_errors, output_extra,) = core_runner.invoke_semgrep(
+        target_manager, rest_of_the_rules, dump_command_for_core, engine_type
     )
 
     if join_rules:
@@ -187,6 +238,7 @@ def run_rules(
             rule_matches_by_rule.update(join_rule_matches_by_rule)
             output_handler.handle_semgrep_errors(join_rule_errors)
 
+    dependencies = {}
     if len(dependency_aware_rules) > 0:
         from semgrep.dependency_aware_rule import (
             generate_unreachable_sca_findings,
@@ -203,7 +255,7 @@ def run_rules(
                 (
                     dep_rule_matches,
                     dep_rule_errors,
-                    matched_lockfiles,
+                    already_reachable,
                 ) = generate_reachable_sca_findings(
                     rule_matches_by_rule.get(rule, []),
                     rule,
@@ -214,30 +266,33 @@ def run_rules(
                 (
                     dep_rule_matches,
                     dep_rule_errors,
-                    targeted_lockfiles,
                 ) = generate_unreachable_sca_findings(
-                    rule, target_manager, matched_lockfiles
+                    rule, target_manager, already_reachable
                 )
                 rule_matches_by_rule[rule].extend(dep_rule_matches)
                 output_handler.handle_semgrep_errors(dep_rule_errors)
-                all_targets.union(targeted_lockfiles)
             else:
                 (
                     dep_rule_matches,
                     dep_rule_errors,
-                    targeted_lockfiles,
-                ) = generate_unreachable_sca_findings(rule, target_manager, set())
+                ) = generate_unreachable_sca_findings(
+                    rule, target_manager, lambda p, d: False
+                )
                 rule_matches_by_rule[rule] = dep_rule_matches
                 output_handler.handle_semgrep_errors(dep_rule_errors)
-                all_targets.union(targeted_lockfiles)
+
+        # Generate stats per lockfile:
+        for ecosystem in ECOSYSTEM_TO_LOCKFILES.keys():
+            for lockfile in target_manager.get_lockfiles(ecosystem):
+                # Add lockfiles as a target that was scanned
+                output_extra.all_targets.add(lockfile)
+                dependencies[str(lockfile)] = parse_lockfile_path(lockfile)
 
     return (
         rule_matches_by_rule,
         semgrep_errors,
-        all_targets,
-        profiling_data,
-        parsing_data,
-        explanations,
+        output_extra,
+        dependencies,
     )
 
 
@@ -277,14 +332,14 @@ def main(
     *,
     core_opts_str: Optional[str] = None,
     dump_command_for_core: bool = False,
-    deep: bool = False,
+    engine_type: EngineType = EngineType.OSS,
     output_handler: OutputHandler,
     target: Sequence[str],
     pattern: Optional[str],
     lang: Optional[str],
     configs: Sequence[str],
     no_rewrite_rule_ids: bool = False,
-    jobs: int = 1,
+    jobs: Optional[int] = None,
     include: Optional[Sequence[str]] = None,
     exclude: Optional[Sequence[str]] = None,
     exclude_rule: Optional[Sequence[str]] = None,
@@ -296,27 +351,39 @@ def main(
     no_git_ignore: bool = False,
     timeout: int = DEFAULT_TIMEOUT,
     max_memory: int = 0,
+    interfile_timeout: int = 0,
     max_target_bytes: int = 0,
     timeout_threshold: int = 0,
     skip_unknown_extensions: bool = False,
     severity: Optional[Sequence[str]] = None,
     optimizations: str = "none",
     baseline_commit: Optional[str] = None,
+    baseline_commit_is_mergebase: bool = False,
 ) -> Tuple[
     RuleMatchMap,
     List[SemgrepError],
     Set[Path],
-    Set[Path],
     FileTargetingLog,
     List[Rule],
     ProfileManager,
-    ProfilingData,
-    ParsingData,
-    Optional[List[out.MatchingExplanation]],
+    OutputExtra,
     Collection[RuleSeverity],
-    Dict[str, int],
+    Dict[str, List[FoundDependency]],
 ]:
     logger.debug(f"semgrep version {__VERSION__}")
+
+    # Some of the lockfile parsers are defined recursively
+    # This does not play well with python's conservative recursion limit, so we manually increase
+
+    if "SEMGREP_PYTHON_RECURSION_LIMIT_INCREASE" in environ:
+        recursion_limit_increase = int(
+            environ["SEMGREP_PYTHON_RECURSION_LIMIT_INCREASE"]
+        )
+    else:
+        recursion_limit_increase = 500
+
+    setrecursionlimit(getrecursionlimit() + recursion_limit_increase)
+
     if include is None:
         include = []
 
@@ -336,6 +403,15 @@ def main(
     all_rules = configs_obj.get_rules(no_rewrite_rule_ids)
     profiler.save("config_time", rule_start_time)
 
+    # Metrics send part 1: add environment information
+    # Must happen after configs are resolved because it is determined
+    # then whether metrics are sent or not
+    metrics = get_state().metrics
+    if metrics.is_enabled:
+        metrics.add_project_url(project_url)
+        metrics.add_configs(configs)
+        metrics.add_engine_type(engine_type)
+
     if not severity:
         shown_severities = DEFAULT_SHOWN_SEVERITIES
         filtered_rules = all_rules
@@ -380,7 +456,9 @@ def main(
     baseline_handler = None
     if baseline_commit:
         try:
-            baseline_handler = BaselineHandler(baseline_commit)
+            baseline_handler = BaselineHandler(
+                baseline_commit, is_mergebase=baseline_commit_is_mergebase
+            )
         # TODO better handling
         except Exception as e:
             raise SemgrepError(e)
@@ -403,31 +481,35 @@ def main(
     core_start_time = time.time()
     core_runner = CoreRunner(
         jobs=jobs,
+        engine_type=engine_type,
         timeout=timeout,
         max_memory=max_memory,
+        interfile_timeout=interfile_timeout,
         timeout_threshold=timeout_threshold,
         optimizations=optimizations,
         core_opts_str=core_opts_str,
     )
 
+    experimental_rules, unexperimental_rules = partition(
+        filtered_rules, lambda rule: rule.severity == RuleSeverity.EXPERIMENT
+    )
+
     logger.verbose("Rules:")
-    for ruleid in sorted(rule.id for rule in filtered_rules):
+    for ruleid in sorted(rule.id for rule in unexperimental_rules):
         logger.verbose(f"- {ruleid}")
 
-    (
-        rule_matches_by_rule,
-        semgrep_errors,
-        all_targets,
-        profiling_data,
-        parsing_data,
-        explanations,
-    ) = run_rules(
+    if len(experimental_rules) > 0:
+        logger.verbose("Experimental Rules:")
+        for ruleid in sorted(rule.id for rule in experimental_rules):
+            logger.verbose(f"- {ruleid}")
+
+    (rule_matches_by_rule, semgrep_errors, output_extra, dependencies) = run_rules(
         filtered_rules,
         target_manager,
         core_runner,
         output_handler,
         dump_command_for_core,
-        deep,
+        engine_type,
     )
     profiler.save("core_time", core_start_time)
     output_handler.handle_semgrep_errors(semgrep_errors)
@@ -477,10 +559,8 @@ def main(
                     (
                         baseline_rule_matches_by_rule,
                         baseline_semgrep_errors,
-                        baseline_targets,
-                        baseline_profiling_data,
-                        baseline_parsing_data,
-                        _explanations,
+                        _,
+                        _,
                     ) = run_rules(
                         # only the rules that had a match
                         [
@@ -492,7 +572,7 @@ def main(
                         core_runner,
                         output_handler,
                         dump_command_for_core,
-                        deep,
+                        engine_type,
                     )
                     rule_matches_by_rule = remove_matches_in_baseline(
                         rule_matches_by_rule,
@@ -513,16 +593,15 @@ def main(
 
     profiler.save("total_time", rule_start_time)
 
-    metrics = get_state().metrics
+    # Metrics send part 2: send results
     if metrics.is_enabled:
-        metrics.add_project_url(project_url)
-        metrics.add_configs(configs)
-        metrics.add_rules(filtered_rules, profiling_data)
-        metrics.add_targets(all_targets, profiling_data)
+        metrics.add_rules(filtered_rules, output_extra.profiling_data)
+        metrics.add_max_memory_bytes(output_extra.profiling_data)
+        metrics.add_targets(output_extra.all_targets, output_extra.profiling_data)
         metrics.add_findings(filtered_matches_by_rule)
         metrics.add_errors(semgrep_errors)
         metrics.add_profiling(profiler)
-        metrics.add_parse_rates(parsing_data)
+        metrics.add_parse_rates(output_extra.parsing_data)
 
     if autofix:
         apply_fixes(filtered_matches_by_rule.kept, dryrun)
@@ -533,14 +612,11 @@ def main(
     return (
         filtered_matches_by_rule.kept,
         semgrep_errors,
-        all_targets,
         renamed_targets,
         target_manager.ignore_log,
         filtered_rules,
         profiler,
-        profiling_data,
-        parsing_data,
-        explanations,
+        output_extra,
         shown_severities,
-        target_manager.lockfile_scan_info,
+        dependencies,
     )
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/state.py /home/pad/yy/cli/src/semgrep/state.py
--- /tmp/semgrep/cli/src/semgrep/state.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/state.py	2023-03-17 09:21:14.529881197 +0100
@@ -3,6 +3,7 @@ from attrs import Factory
 from attrs import frozen
 
 from semgrep.app.session import AppSession
+from semgrep.command import Command
 from semgrep.env import Env
 from semgrep.error_handler import ErrorHandler
 from semgrep.metrics import Metrics
@@ -24,6 +25,7 @@ class SemgrepState:
     error_handler: ErrorHandler = Factory(ErrorHandler)
     settings: Settings = Factory(Settings)
     terminal: Terminal = Factory(Terminal)
+    command: Command = Factory(Command)
 
 
 def get_state() -> SemgrepState:
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/target_manager.py /home/pad/yy/cli/src/semgrep/target_manager.py
--- /tmp/semgrep/cli/src/semgrep/target_manager.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/target_manager.py	2023-04-12 12:55:37.489634222 +0200
@@ -22,10 +22,7 @@ from typing import Set
 from typing import Tuple
 from typing import Union
 
-from semdep.find_lockfiles import ECOSYSTEM_TO_LOCKFILES
-from semdep.find_lockfiles import LOCKFILE_TO_MANIFEST
-from semdep.parse_lockfile import parse_lockfile_str
 from semgrep.git import BaselineHandler
 
 # usually this would be a try...except ImportError
 # but mypy understands only this
@@ -58,6 +53,12 @@ from semgrep.util import path_has_permis
 from semgrep.util import with_color
 from semgrep.verbose_logging import getLogger
 
+from semgrep.semgrep_interfaces.semgrep_output_v1 import Gem
+from semgrep.semgrep_interfaces.semgrep_output_v1 import Gomod
+from semgrep.semgrep_interfaces.semgrep_output_v1 import Maven
+from semgrep.semgrep_interfaces.semgrep_output_v1 import Npm
+from semgrep.semgrep_interfaces.semgrep_output_v1 import Pypi
+
 logger = getLogger(__name__)
 
 MAX_CHARS_TO_READ_FOR_SHEBANG = 255
@@ -70,6 +73,20 @@ ALL_EXTENSIONS: Collection[FileExtension
     if ext != FileExtension("")
 }
 
+ECOSYSTEM_TO_LOCKFILES = {
+    Ecosystem(Pypi()): [
+        "Pipfile.lock",
+        "poetry.lock",
+        "requirements.txt",
+        "requirements3.txt",
+    ],
+    Ecosystem(Npm()): ["package-lock.json", "yarn.lock", "pnpm-lock.yaml"],
+    Ecosystem(Gem()): ["Gemfile.lock"],
+    Ecosystem(Gomod()): ["go.mod"],
+    Ecosystem(Cargo()): ["Cargo.lock"],
+    Ecosystem(Maven()): ["maven_dep_tree.txt", "gradle.lockfile"],
+}
+
 
 def write_pipes_to_disk(targets: Sequence[str], temp_dir: Path) -> Sequence[str]:
     """
@@ -306,6 +323,8 @@ class FileTargetingLog:
         return output
 
     def yield_json_objects(self) -> Iterable[Dict[str, Any]]:
+        # coupling: if you add a reason here,
+        # add it also to semgrep_output_v1.atd.
         for path in self.always_skipped:
             yield {"path": str(path), "reason": "always_skipped"}
         for path in self.semgrepignored:
@@ -492,7 +511,6 @@ class TargetManager:
     baseline_handler: Optional[BaselineHandler] = None
     allow_unknown_extensions: bool = False
     file_ignore: Optional[FileIgnore] = None
-    lockfile_scan_info: Dict[str, int] = {}
     ignore_log: FileTargetingLog = Factory(FileTargetingLog, takes_self=True)
     targets: Sequence[Target] = field(init=False)
 
@@ -734,25 +752,47 @@ class TargetManager:
 
         return paths.kept
 
+    def get_all_lockfiles(self) -> Dict[Ecosystem, FrozenSet[Path]]:
+        """
+        Return a dict mapping each ecosystem to the set of lockfiles for that ecosystem
+        """
+        ALL_ECOSYSTEMS: Set[Ecosystem] = {
+            Ecosystem(Npm()),
+            Ecosystem(Pypi()),
+            Ecosystem(Gem()),
+            Ecosystem(Gomod()),
+            Ecosystem(Cargo()),
+            Ecosystem(Maven()),
+        }
+
+        return {
+            ecosystem: self.get_lockfiles(ecosystem) for ecosystem in ALL_ECOSYSTEMS
+        }
+
     @lru_cache(maxsize=None)
-    def get_lockfile_dependencies(
-        self, ecosystem: Ecosystem
-    ) -> Dict[Path, List[FoundDependency]]:
-        lockfiles = self.get_files_for_language(ecosystem).kept
-        parsed: Dict[Path, List[FoundDependency]] = {}
-        for lockfile in lockfiles:
-            path, lockfile_pattern = lockfile.parent, lockfile.parts[-1]
-            manifest_pattern = LOCKFILE_TO_MANIFEST[lockfile_pattern]
-            manifest_path = path / manifest_pattern if manifest_pattern else None
-            deps = parse_lockfile_str(
-                lockfile.read_text(encoding="utf8"),
-                lockfile,
-                manifest_path.read_text(encoding="utf8")
-                if manifest_path and manifest_path.exists()
-                else None,
-            )
-            if lockfile not in self.lockfile_scan_info:
-                # We haven't seen this file during reachable finding generation
-                self.lockfile_scan_info[str(lockfile)] = len(deps)
-            parsed[lockfile] = deps
-        return parsed
+    def get_lockfiles(self, ecosystem: Ecosystem) -> FrozenSet[Path]:
+        """
+        Return set of paths to lockfiles for a given ecosystem
+
+        Respects semgrepignore/exclude flag
+        """
+        return self.get_files_for_language(ecosystem).kept
+
+    def find_single_lockfile(self, p: Path, ecosystem: Ecosystem) -> Optional[Path]:
+        """
+        Find the nearest lockfile in a given ecosystem to P
+        Searches only up the directory tree
+
+        If lockfile not in self.get_lockfiles(ecosystem) then return None
+        this would happen if the lockfile is ignored by a .semgrepignore or --exclude
+        """
+        candidates = self.get_lockfiles(ecosystem)
+
+        for path in p.parents:
+            for lockfile_pattern in ECOSYSTEM_TO_LOCKFILES[ecosystem]:
+                lockfile_path = path / lockfile_pattern
+                if lockfile_path in candidates and lockfile_path.exists():
+                    return lockfile_path
+                else:
+                    continue
+        return None
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/templates/.semgrepignore /home/pad/yy/cli/src/semgrep/templates/.semgrepignore
--- /tmp/semgrep/cli/src/semgrep/templates/.semgrepignore	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/templates/.semgrepignore	2022-10-14 07:53:26.950002293 +0200
@@ -7,6 +7,7 @@ vendor/
 .venv/
 .tox/
 *.min.js
+.npm/
 
 # Common test paths
 test/
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/terminal.py /home/pad/yy/cli/src/semgrep/terminal.py
--- /tmp/semgrep/cli/src/semgrep/terminal.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/terminal.py	2023-02-14 10:28:57.440045881 +0100
@@ -4,6 +4,8 @@ import sys
 
 from attr import define
 
+from semgrep.console import console
+from semgrep.constants import OutputFormat
 from semgrep.env import Env
 
 
@@ -34,8 +36,22 @@ class Terminal:
         debug: bool = False,
         quiet: bool = True,
         force_color: bool = False,
+        # our conservative default is JSON, as it is the most restrictive
+        output_format: OutputFormat = OutputFormat.JSON,
     ) -> None:
-        """Set the relevant logging levels"""
+        """Set the relevant logging levels
+
+        Affects also the configuration of the rich console."""
+
+        # GitHub Actions mixes stdout and stderr: https://github.com/isaacs/github/issues/1981
+        # As a workaround, we limit output in GHA to a single stream
+        # which guarantees lines are shown in the intended order.
+        # We don't apply the workaround if output format is not text,
+        # because mixed output is better than maybe breaking custom integrations relying on JSON output.
+        multiple_streams_available = True
+        if os.getenv("GITHUB_ACTIONS") == "true" and output_format == OutputFormat.TEXT:
+            multiple_streams_available = False
+
         # Assumes only one of verbose, debug, quiet is True
         logger = logging.getLogger("semgrep")
         logger.handlers = []  # Reset to no handlers
@@ -48,8 +64,10 @@ class Terminal:
         elif quiet:
             stdout_level = logging.CRITICAL
 
-        # Setup stdout logging
-        stdout_handler = logging.StreamHandler()
+        # Setup output stream logging
+        stdout_handler = logging.StreamHandler(
+            stream=sys.stderr if multiple_streams_available else sys.stdout
+        )
         stdout_formatter = logging.Formatter("%(message)s")
         stdout_handler.setFormatter(stdout_formatter)
         stdout_handler.setLevel(stdout_level)
@@ -82,6 +100,15 @@ class Terminal:
         ):
             self.force_color_off = True
 
+        self.configure_rich_console(quiet, multiple_streams_available)
+
+    def configure_rich_console(
+        self, quiet: bool, multiple_streams_available: bool
+    ) -> None:
+        console.quiet = quiet
+        console.stderr = multiple_streams_available
+        console.width = min(console.width, 120)
+
     @property
     def is_quiet(self) -> bool:
         """
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/test.py /home/pad/yy/cli/src/semgrep/test.py
--- /tmp/semgrep/cli/src/semgrep/test.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/test.py	2023-02-20 09:00:19.790001987 +0100
@@ -34,6 +34,7 @@ from boltons.iterutils import partition
 from ruamel.yaml import YAML
 
 from semgrep.constants import BREAK_LINE
+from semgrep.engine import EngineType
 from semgrep.semgrep_main import invoke_semgrep
 from semgrep.util import final_suffix_matches
 from semgrep.util import is_config_fixtest_suffix
@@ -210,7 +211,9 @@ def get_expected_and_reported_lines(
                         effective_line_num,
                         test_file_resolved,
                     )
-            except ValueError:  # comment looked like a test annotation but couldn't parse
+            except (
+                ValueError
+            ):  # comment looked like a test annotation but couldn't parse
                 logger.warning(
                     f"Could not parse {line} as a test annotation in file {test_file_resolved}. Skipping this line"
                 )
@@ -309,16 +312,21 @@ def create_temporary_copy(path: Path) ->
     return temp_path
 
 
-def relatively_eq(parent1: Path, child1: Path, parent2: Path, child2: Path) -> bool:
-    def remove_all_suffixes(p: Path) -> Path:
-        result = p.with_suffix("")
-        while result != result.with_suffix(""):
-            result = result.with_suffix("")
-        return result
-
-    rel1 = remove_all_suffixes(child1.relative_to(parent1))
-    rel2 = remove_all_suffixes(child2.relative_to(parent2))
-    return rel1 == rel2
+def relatively_eq(
+    parent_target: Path, target: Path, parent_config: Path, config: Path
+) -> bool:
+    def remove_all_suffixes(p: str) -> str:
+        return p.split(".", 1)[0]
+
+    rel1 = target.relative_to(parent_target).parts
+    rel2 = config.relative_to(parent_config).parts
+    s = len(rel2)
+    if len(rel1) < s:
+        return False
+    s -= 1
+    return rel1[:s] == rel2[:s] and remove_all_suffixes(rel1[s]) == remove_all_suffixes(
+        rel2[s]
+    )
 
 
 def get_config_filenames(original_config: Path) -> List[Path]:
@@ -408,7 +415,7 @@ def config_contains_fix_key(config: Path
     with open(config) as file:
         yaml = YAML(typ="safe")  # default, if not specfied, is 'rt' (round-trip)
         rule = yaml.load(file)
-        if "rules" in rule:
+        if rule.get("rules"):
             return "fix" in rule["rules"][0]
         else:
             return False
@@ -445,7 +451,7 @@ def generate_test_results(
     config: Path,
     strict: bool,
     json_output: bool,
-    deep: bool,
+    engine_type: EngineType,
     optimizations: str = "none",
 ) -> None:
     config_filenames = get_config_filenames(config)
@@ -466,7 +472,7 @@ def generate_test_results(
 
     invoke_semgrep_fn = functools.partial(
         invoke_semgrep_multi,
-        deep=deep,
+        engine_type=engine_type,
         no_git_ignore=True,
         no_rewrite_rule_ids=True,
         strict=strict,
@@ -554,7 +560,7 @@ def generate_test_results(
 
     # This is the invocation of semgrep for testing autofix.
     #
-    # TODO: should 'deep' be set to 'deep=deep' or always 'deep=False'?
+    # TODO: should 'engine' be set to 'engine=engine' or always 'engine=EngineType.OSS'?
     invoke_semgrep_with_autofix_fn = functools.partial(
         invoke_semgrep_multi,
         no_git_ignore=True,
@@ -681,9 +687,8 @@ def test_main(
     strict: bool,
     json: bool,
     optimizations: str,
-    deep: bool,
+    engine_type: EngineType,
 ) -> None:
-
     if len(target) != 1:
         raise Exception("only one target directory allowed for tests")
     target_path = Path(target[0])
@@ -702,6 +707,6 @@ def test_main(
         config=config_path,
         strict=strict,
         json_output=json,
-        deep=deep,
+        engine_type=engine_type,
         optimizations=optimizations,
     )
diff -u -p -b -B -r -x .semantic.cache -x .depend -x CVS -x .hg -x .svn -x .git -x _darcs /tmp/semgrep/cli/src/semgrep/util.py /home/pad/yy/cli/src/semgrep/util.py
--- /tmp/semgrep/cli/src/semgrep/util.py	2023-04-13 11:00:52.889774289 +0200
+++ /home/pad/yy/cli/src/semgrep/util.py	2023-02-20 09:00:19.790001987 +0100
@@ -14,5 +14,5 @@ from typing import List
 
-def sub_check_output(cmd: List[str], **kwargs: Any) -> Any:
+def sub_check_output(cmd: Sequence[Union[str, Path]], **kwargs: Any) -> Any:
     """A simple proxy function to minimize and centralize subprocess usage."""
     from semgrep.state import get_state  # avoiding circular imports
 

Diff finished.  Thu Apr 13 11:35:49 2023
