#! /usr/bin/env python3
#
# Run semgrep on a series of pairs (rules, repo) with different options,
# and report the time it takes. Optionally upload the results to the semgrep
# dashboard.
#
# With the --semgrep_core option, instead run semgrep-core on a series of
# pairs (rules, repo) with options chosen to test semgrep-core performance.
# Note that semgrep-core can currently be run for one language only, so
# these benchmarks only include corpuses that are primarily one language.
# This allows them to be compared to the semgrep runtimes. Can also upload
# the results to the dashboard, and use a dummy set instead
#
import argparse
import copy
import json
import os
import re
import subprocess
import time
import urllib.request
from collections import defaultdict
from contextlib import contextmanager
from typing import Iterator, List, Optional
from typing import Tuple
from pathlib import Path
from RepositoryTimePerRule import RepositoryTimePerRule
from config import SemgrepBenchmarkConfig, BenchmarkRunSetupData
import logging
import sys

logger = logging.getLogger(__file__)
logger.setLevel(logging.DEBUG)
handler = logging.StreamHandler(stream=sys.stderr)
handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
logger.addHandler(handler)
from Corpuses import Corpus, SMALL_CORPUSES, MEDIUM_CORPUSES, LARGE_CORPUSES, INTERNAL_CORPUSES, GITLAB_CORPUSES, DUMMY_CORPUSES

import requests
import semgrep_core_benchmark  # type: ignore

DASHBOARD_URL = "https://dashboard.semgrep.dev"
STATS_URL = "https://stats.semgrep.dev"
BPS_ENDPOINT = "semgrep.perf.bps"
LPS_ENDPOINT = "semgrep.perf.lps"

STD = "std"

SEMGREP_USER_AGENT = "Semgrep/0.0.0-benchmark"
RULE_CONFIG_CACHE_DIR = Path("rules_cache")
PREP_FILE_TEMPLATE = """#! /bin/sh
#
# Fetch rules and targets prior for the "r2c" benchmark.
#
# rule_dir: input/rules
# target_dir: input/socket.io
#
# See ../r2c-rules for centralized copy of the rule
# Uses sh because bash is not installed within the semgrep docker container.
#
set -eu

mkdir -p input
mkdir -p input/rules

cp -r {rule_cache_dir} input/rules
cd input

# Obtain a shallow clone of a git repo for a specific commit
shallow_clone() {{
  if [ -d "$name" ]; then
    echo "Reusing repo '$name'"
  else
    echo "Obtaining a shallow clone of git repo '$name', commit $commit"
    mkdir -p "$name"
    (
      cd "$name"
      git init
      git remote add origin "$url"
      git fetch --depth 1 origin "$commit"
      git checkout FETCH_HEAD -b tmp
    )
  fi
}}

# Targets using other repos we run in CI
name="{name}"
url="{url}"
commit="{commit_hash}"
shallow_clone
"""

class SemgrepVariant:
    def __init__(self, name: str, semgrep_core_extra: str, semgrep_extra: str = ""):
        # name for the input corpus (rules and targets)
        self.name = name

        # space-separated extra arguments to pass to semgrep-core
        # command via SEMGREP_CORE_EXTRA environment variable
        self.semgrep_core_extra = semgrep_core_extra

        # space-separated extra arguments to pass to the default semgrep
        # command
        self.semgrep_extra = semgrep_extra


# Feel free to create new variants. The idea is to use the default set
# of options as the baseline and we see what happens when we enable or
# disable this or that optimization.
#
SEMGREP_VARIANTS = [
    # default settings
    SemgrepVariant(STD, ""),
    # removing optimisations
    SemgrepVariant("no-cache", "-no_opt_cache"),
    SemgrepVariant("max-cache", "-opt_max_cache"),
    SemgrepVariant("no-bloom", "-no_bloom_filter"),
    SemgrepVariant("no-gc-tuning", "-no_gc_tuning"),
    # alternate optimisations
    SemgrepVariant("set_filters", "-set_filter"),
    SemgrepVariant("experimental", "-no_filter_irrelevant_rules", "--optimizations"),
    SemgrepVariant(
        "experimental_and_fast", "-filter_irrelevant_rules", "--optimizations"
    ),
]

# For when you just want to test a single change
STD_VARIANTS = [SemgrepVariant(STD, "")]

GITLAB_VARIANTS = [
    SemgrepVariant(STD, ""),
    SemgrepVariant("experimental", "-no_filter_irrelevant_rules", "--optimizations"),
]

# This class allows us to put semgrep results in a set and compute set
# differences while saving the original JSON dictionary
class SemgrepResult:
    def __init__(self, dict: dict) -> None:
        self.res = dict

        # We use self.str to compare dicts, so change this
        # to abstract away differences
        if "extra" in dict:
            dict2 = copy.deepcopy(dict)
            # TODO: full-rule does not correctly update message with metavars
            if "message" in dict2["extra"]:
                dict2["extra"]["message"] = ""
            # TODO: spacegrep.py calls dedent() on lines (not sure why)
            if "lines" in dict2["extra"]:
                dict2["extra"]["lines"] = ""
            if "metavars" in dict2["extra"]:
                # TODO: spacegrep/../Semgrep.ml uses a different unique_id
                for _, v in dict2["extra"]["metavars"].items():
                    if "unique_id" in v:
                        v["unique_id"] = ""
                # TODO: core_runner.py dedup_output() depends on the order
                # of the elements in the list to remove similar findings
                # but with different metavars
                dict2["extra"]["metavars"] = ""

            self.str = json.dumps(dict2, sort_keys=True)
        else:
            self.str = json.dumps(dict, sort_keys=True)

    def __eq__(self, other: object) -> bool:
        if not isinstance(other, SemgrepResult):
            raise NotImplementedError

        return self.str == other.str

    def __lt__(self, other: object) -> bool:
        if not isinstance(other, SemgrepResult):
            raise NotImplementedError

        return self.str > other.str

    def __hash__(self) -> int:
        return hash(self.str)


# Add support for: with chdir(DIR): ...
@contextmanager
def chdir(dir: str) -> Iterator[None]:
    old_dir = os.getcwd()
    os.chdir(dir)
    try:
        yield
    finally:
        os.chdir(old_dir)


def upload(url: str, value: str) -> None:
    print(f"Uploading to {url}")
    r = urllib.request.urlopen(  # nosem
        url=url,
        data=value.encode("ascii"),
    )
    print(r.read().decode())


def upload_result(
    variant_name: str, metric_name: str, value: float, timings: dict
) -> None:
    # Upload overall runtime of the benchmark
    metric_url = f"{DASHBOARD_URL}/api/metric/{metric_name}"
    upload(metric_url, str(value))

    if variant_name == STD and timings is not None:
        # Compute bps from semgrep timing data
        assert "rules" in timings
        assert "total_time" in timings
        assert "total_bytes" in timings
        num_rules = len(timings["rules"])
        bps = timings["total_bytes"] / (timings["total_time"] * num_rules)

        bps_url = f"{DASHBOARD_URL}/api/metric/{BPS_ENDPOINT}"
        upload(bps_url, str(bps))

        # Similarly, compute lps
        assert "targets" in timings
        num_lines = 0
        for target in timings["targets"]:
            assert "path" in target
            with open(target["path"]) as f:
                try:
                    num_lines += sum(1 for _ in f)
                except UnicodeDecodeError:
                    pass
        lps = num_lines / (timings["total_time"] * num_rules)

        lps_url = f"{DASHBOARD_URL}/api/metric/{LPS_ENDPOINT}"
        upload(lps_url, str(lps))

        # Upload timing data as a json
        print(f"Uploading timing data to {STATS_URL}")
        headers = {"content-type": "application/json"}
        r = requests.post(
            STATS_URL, data=json.dumps(timings), headers=headers, timeout=30
        )
        print(r.content)


def standardize_findings(findings: dict, include_time: bool) -> Tuple[dict, dict]:
    if "errors" not in findings:
        msg = json.dumps(findings, indent=4) + "\n\nDid not find expected key 'errors'"
        raise Exception(msg)
    if "results" not in findings:
        msg = json.dumps(findings, indent=4) + "\n\nDid not find expected key 'results'"
        raise Exception(msg)
    if include_time and "time" not in findings:
        msg = json.dumps(findings, indent=4) + "\n\nDid not find expected key 'time'"
        raise Exception(msg)
    results = {
        "errors": findings["errors"],
        "results": {SemgrepResult(i) for i in findings["results"]},
    }
    timings = findings["time"] if include_time else None
    return results, timings


def output_differences(
    findings: set, std_findings: set, variant: str
) -> Tuple[int, int]:
    def output_diff(diff: set) -> None:
        for d in sorted(diff):
            print(json.dumps(d.res, sort_keys=True, indent=4))

    f_diff = findings.difference(std_findings)
    s_diff = std_findings.difference(findings)
    fd_len = len(f_diff)
    sd_len = len(s_diff)
    print("In", variant, "but not std", fd_len, "findings :")
    output_diff(f_diff)
    print("In std but not", variant, sd_len, "findings :")
    output_diff(s_diff)
    return fd_len, sd_len

def run_semgrep(
    docker: str, corpus: Corpus, variant: SemgrepVariant, include_time: bool
) -> Tuple[float, bytes]:
    args = []
    common_args = [
        "--strict",
        "--json",
        "--timeout",
        "0",
        "--verbose",
        "--no-git-ignore",  # because files in bench/*/input/ are git-ignored
    ]
    if docker:
        # Absolute paths are required by docker for mounting volumes, otherwise
        # they end up empty inside the container.
        args = [
            "docker",
            "run",
            "-v",
            os.path.abspath(corpus.rule_dir) + ":/rules",
            "-v",
            os.path.abspath(corpus.target_dir) + ":/targets",
            "-t",
            docker,
            "--config",
            "/rules",
            "/targets",
        ]
    else:
        # Absolute paths for rules and targets are required by semgrep
        # when running within the semgrep docker container.
        args = [
            "semgrep",
            "--config",
            os.path.abspath(corpus.rule_dir),
            os.path.abspath(corpus.target_dir),
        ]
    args.extend(common_args)
    if variant.semgrep_extra != "":
        args.extend([variant.semgrep_extra])
    if include_time:
        args.extend(["--time"])

    print(f"current directory: {os.getcwd()}")
    print("semgrep command: {}".format(" ".join(args)))
    os.environ["SEMGREP_CORE_EXTRA"] = variant.semgrep_core_extra
    print(f"extra arguments for semgrep-core: '{variant.semgrep_core_extra}'")

    t1 = time.time()
    res = subprocess.run(args, capture_output=True)  # nosem
    t2 = time.time()

    status = res.returncode
    print(f"semgrep exit status: {status}")
    if status == 0:
        print("success")
    elif status == 3:
        print("warning: some files couldn't be parsed")
    else:
        print("************* Semgrep stdout *************")
        print(res.stdout)
        print("************* Semgrep stderr *************")
        print(res.stderr)
        res.check_returncode()

    return t2 - t1, res.stdout

def normalize_rule_config_name(rule_config_str: str) -> str:
    return rule_config_str.replace("/", ".") + ".yaml"


def prepare_rule_cache_for_this_run(setup_data: BenchmarkRunSetupData)-> Tuple[Path, List[Path]]:
    rule_cache_for_this_run: Path = RULE_CONFIG_CACHE_DIR / setup_data.run_name
    rule_cache_for_this_run.mkdir(parents=True, exist_ok=True)
    logger.debug(f"Rule cache for run {setup_data.run_name} created at {rule_cache_for_this_run}")
    rule_config_paths = list()
    for rule_config in setup_data.rule_configs:
        logger.debug(f"Checking for rule config '{rule_config}' in cache")
        normalized_rule_config = normalize_rule_config_name(rule_config)
        rule_config_path = rule_cache_for_this_run / normalized_rule_config
        # check if on filesystem
        # if not, fetch from URL
        if not rule_config_path.exists():
            try:
                rule_config_url = f"https://semgrep.dev/{rule_config}"
                logger.debug(f"Fetching config from '{rule_config_url}'")
                r = requests.get(
                    rule_config_url,
                    headers={"User-Agent": SEMGREP_USER_AGENT},
                    timeout=60,
                )
            except requests.Timeout:
                print(f"There was a timeout error trying to download the rule config at '{rule_config_url}'")
                continue
            rule_config_path.write_text(r.text)
            logger.debug(f"Rule config '{rule_config}' has been written to '{rule_config_path}'")
        else:
            logger.debug(f"Rule config '{rule_config_path}' already exists, using this one")
        rule_config_paths.append(rule_config_path)
    return rule_cache_for_this_run, rule_config_paths

def prepare_benchmark_run(benchmark_config: SemgrepBenchmarkConfig) -> List[Corpus]:
    """
    Sets up the relevant data for a benchmarking run
    - downloads rule configs from an endpoint if necessary
    - generates a 'prep' file which clones a repo from a URL and checks out the commit
    """
    corpuses = list()
    for setup_data in benchmark_config.benchmark_setup_data:
        logger.debug(f"Setting up benchmark run for run '{setup_data.run_name}'")
        rule_cache_dir, _ = prepare_rule_cache_for_this_run(setup_data)
        for repository in setup_data.repositories:
            repo_name = repository.url.split('/')[-1]
            generate_prep_file(repo_name, rule_cache_dir.absolute(), repository.url, repository.commit_hash)
            corpuses.append(Corpus(
                repo_name,
                rule_cache_dir.absolute(),
                Path("input") / repo_name,
            ))
    
    return corpuses

def generate_prep_file(name: str, rule_cache_dir: Path, url: str, commit_hash: str, ) -> bool:
    # make dirs
    benchdir = Path(name)
    benchdir.mkdir(parents=True, exist_ok=True)
    prep_file = benchdir / "prep"
    # generate prep file
    logger.debug(f"Generating 'prep' file at {prep_file}")
    prep_file.touch(0o755, exist_ok=True)
    prep_file.write_text(PREP_FILE_TEMPLATE.format(
        name=name,
        rule_cache_dir=rule_cache_dir,
        url=url,
        commit_hash=commit_hash
    ))


def run_benchmarks(
    docker: str,
    dummy: bool,
    small_only: bool,
    all: bool,
    internal: bool,
    gitlab: bool,
    std_only: bool,
    config_file: Optional[Path],
    filter_corpus: str,
    filter_variant: str,
    output_time_per_rule_json: str,
    plot_benchmarks: bool,
    upload: bool,
    include_time: bool,
    summary_file_path: str,
    called_dir: str,
) -> None:

    variants = SEMGREP_VARIANTS
    if std_only or output_time_per_rule_json:
        variants = STD_VARIANTS
    if filter_variant:
        variants = [x for x in variants if re.search(filter_variant, x.name) != None]
    # TODO: make benchmarking time-per-rule data work with all variants.
    if output_time_per_rule_json:
        logger.info("Running only the standard variants.")
        variants = STD_VARIANTS

    results_msgs = []
    durations = ""
    results: dict = {variant.name: [] for variant in variants}
    output_per_rule_json_results = RepositoryTimePerRule(output_file=output_time_per_rule_json)

    corpuses = SMALL_CORPUSES + MEDIUM_CORPUSES
    if dummy:
        corpuses = DUMMY_CORPUSES
    elif internal:
        corpuses = INTERNAL_CORPUSES
    elif gitlab:
        corpuses = GITLAB_CORPUSES
        variants = GITLAB_VARIANTS
    elif small_only:
        corpuses = SMALL_CORPUSES
    elif all:
        corpuses = SMALL_CORPUSES + MEDIUM_CORPUSES + LARGE_CORPUSES
    elif config_file:
        corpuses = prepare_benchmark_run(SemgrepBenchmarkConfig.parse_config(config_file.absolute()))
    if filter_corpus:
        corpuses = [x for x in corpuses if re.search(filter_corpus, x.name) != None]

    # Multithread here. Dedicate one core to each benchmark
    for corpus in corpuses:
        with chdir(corpus.name):
            corpus.prep()
            std_findings = {}
            for variant in variants:

                # Run variant
                name = ".".join(["semgrep", "bench", corpus.name, variant.name])
                metric_name = ".".join([name, "duration"])
                print(f"------ {name} ------")
                duration, findings_bytes = run_semgrep(
                    docker, corpus, variant, include_time
                )   
                
                if not output_time_per_rule_json is None:
                    output_per_rule_json_results.times_per_file_to_times_per_rule(corpus.name, findings_bytes)

                # Report results
                msg = f"{metric_name} = {duration:.3f} s"
                print(msg)
                results_msgs.append(msg)
                durations += f"{duration:.3f}\n"
                results[variant.name].append(duration)

                findings, timings = standardize_findings(
                    json.loads(findings_bytes), include_time
                )

                if upload:
                    upload_result(variant.name, metric_name, duration, timings)

                # Check correctness
                num_results = len(findings["results"])
                num_errors = len(findings["errors"])
                print(f"Result: {num_results} findings, {num_errors} parse errors")

                if variant.name == STD:
                    std_findings = findings
                elif findings["results"] ^ std_findings["results"] != set():
                    fd_len, sd_len = output_differences(
                        findings["results"], std_findings["results"], variant.name
                    )
                    results_msgs[
                        -1
                    ] += f" ERROR: {fd_len} extra findings, {sd_len} missing findings"
                elif len(findings["errors"]) > len(std_findings["errors"]):
                    results_msgs[-1] += " WARNING: more errors than std"

    # Show summary data
    print("\n".join(results_msgs))

    if summary_file_path:
        with chdir(called_dir):
            summary_file = open(summary_file_path, "w+")
            summary_file.write(durations)
            summary_file.close()

    if plot_benchmarks:
        import matplotlib.pyplot as plt
        import pandas as pd

        indexes = [corpus.name for corpus in corpuses]
        plotdata = pd.DataFrame(results, index=indexes)
        plotdata.plot(kind="bar")
        plt.show()

    if not output_time_per_rule_json is None:
        with chdir(called_dir):
            output_per_rule_json_results.print_repo_to_times_per_rule()

def main() -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--docker",
        metavar="DOCKER_IMAGE",
        type=str,
        help="use the specified docker image for semgrep, such as returntocorp/semgrep:develop",
    )
    parser.add_argument(
        "--dummy",
        help="run quick, fake benchmarks for development purposes",
        action="store_true",
    )
    parser.add_argument(
        "--internal",
        help="run internal benchmarks for development purposes",
        action="store_true",
    )
    parser.add_argument(
        "--gitlab",
        help="run gitlab benchmarks for development purposes",
        action="store_true",
    )
    parser.add_argument(
        "--small-only",
        help="only run small benchmarks (20s per run or less)",
        action="store_true",
    )
    parser.add_argument(
        "--all",
        help="run all benchmarks (takes >1 day)",
        action="store_true",
    )
    parser.add_argument(
        "--config",
        "-f",
        default=None,
        help="run benchmarks according to a config file",
    )
    parser.add_argument(
        "--std-only", help="only run the default semgrep", action="store_true"
    )
    parser.add_argument(
        "--filter-corpus",
        metavar="REGEXP",
        type=str,
        help="run the corpus only if it satisfies the regexp (e.g., 'std|exp')",
    )
    parser.add_argument(
        "--filter-variant",
        metavar="REGEXP",
        type=str,
        help="run the variant only if it satisfies the regexp (e.g., 'dr.*') ",
    )
    parser.add_argument(
        "--upload", help="upload results to semgrep dashboard", action="store_true"
    )
    parser.add_argument(
        "--save-to",
        metavar="FILE_NAME",
        type=str,
        help="save timing summary to the file given by the argument",
    )
    parser.add_argument(
        "--plot-benchmarks",
        help="display a graph of the benchmark results",
        action="store_true",
    )
    parser.add_argument(
        "--semgrep-core", help="run semgrep-core benchmarks", action="store_true"
    )
    parser.add_argument(
        "--output-time-per-rule-json",
        nargs='?',
        default=None,
        const = 'repo_to_rule_time.json',        
        help="output a json file that shows timing information for each rule."
    )
    parser.add_argument("--no-time", help="disable time-checking", action="store_true")
    args = parser.parse_args()

    cur_dir = os.path.dirname(os.path.abspath(__file__))
    called_dir = os.getcwd()

    with chdir(cur_dir + "/bench"):
        if args.semgrep_core:
            semgrep_core_benchmark.run_benchmarks(args.dummy, args.upload)
        else:
            run_benchmarks(
                args.docker,
                args.dummy,
                args.small_only,
                args.all,
                args.internal,
                args.gitlab,
                args.std_only,
                (Path(called_dir) / args.config) if args.config else None,
                args.filter_corpus,
                args.filter_variant,
                args.output_time_per_rule_json,
                args.plot_benchmarks,
                args.upload,
                not args.no_time,
                args.save_to,
                called_dir,
            )


if __name__ == "__main__":
    main()