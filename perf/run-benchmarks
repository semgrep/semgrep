#! /usr/bin/env python3
#
# Run semgrep on a series of pairs (rules, repo) with different options,
# and report the time it takes. Optionally upload the results to the semgrep
# dashboard.
#
# With the --semgrep_core option, instead run semgrep-core on a series of
# pairs (rules, repo) with options chosen to test semgrep-core performance.
# Note that semgrep-core can currently be run for one language only, so
# these benchmarks only include corpuses that are primarily one language.
# This allows them to be compared to the semgrep runtimes. Can also upload
# the results to the dashboard, and use a dummy set instead
#
import argparse
import copy
import json
import os
import re
import subprocess
import time
import urllib.request
from contextlib import contextmanager
from typing import Iterator
from typing import Tuple

import requests
import semgrep_core_benchmark  # type: ignore

DASHBOARD_URL = "https://dashboard.semgrep.dev"
STATS_URL = "https://stats.semgrep.dev"
BPS_ENDPOINT = "semgrep.perf.bps"
LPS_ENDPOINT = "semgrep.perf.lps"

STD = "std"

# Run command and propagate errors
def cmd(*args: str) -> None:
    subprocess.run(args, check=True)  # nosem


class Corpus:
    def __init__(self, name: str, rule_dir: str, target_dir: str):
        # name for the input corpus (rules and targets)
        self.name = name

        # folder containing the semgrep rules
        self.rule_dir = rule_dir

        # folder containing the target source files
        self.target_dir = target_dir

    # Fetch rules and targets is delegated to an ad-hoc script named 'prep'.
    def prep(self) -> None:
        cmd("./prep")


# Naming conventions:
#
# Save for a few repos (currently zulip, big-js, njsbox),
# the tests below all run r2c rulepacks (in r2c-rules) on public repos
# For command Corpus("$X", ..., "input/$Y"), you can find the repo by
# going to github.com/$X/$Y
#

SMALL_CORPUSES = [
    # Run zulip custom Python rules on zulip itself
    Corpus("zulip", "input/semgrep.yml", "input/zulip"),
    # Run our flask rulepack on a python repo
    Corpus("dropbox", "input/flask.yml", "input/pytest-flakefinder"),
    # Run our r2c-ci and r2c-security-audit packs on a go/ruby repo
    Corpus("coinbase", "input/rules", "input/bifrost"),
    # Run our django rulepack on a large python repo
    Corpus("apache", "input/django.yml", "input/libcloud"),
    # Run our golang rulepack on a go/html repo
    Corpus("0c34", "input/golang.yml", "input/govwa"),
    # Run our javascript and eslint-plugin-security packs on a large JS repo
    Corpus("lodash", "input/rules", "input/lodash"),
    # Run old nodejsscan rules on vulnerable app (was in calculate_ci_perf.py)
    Corpus("njs-old-dvna", "input/njsscan/njsscan/rules/semantic_grep", "input/dvna"),
    #
    # Run our r2c-ci packs (but not r2c-security-audit) on vulnerable apps
    # See https://owasp.org/www-project-vulnerable-web-applications-directory/
    # for a full list of such apps
    #
    Corpus("DVWA", "input/rules", "input/DVWA"),
    Corpus("juice-shop", "input/rules", "input/juice-shop"),
    Corpus("Vulnerable-Flask-App", "input/rules", "input/Vulnerable-Flask-App"),
    # (Gitlab small) Run our python and flask rules on a python repo
    Corpus("pallets", "input/rules", "input/flask"),
    # (Gitlab small) Run our javascript rules on a JS repo
    Corpus("socketio", "input/javascript.yml", "input/socket"),
]

MEDIUM_CORPUSES = [
    # Single rule bench at the origin of the --filter-irrelevant-rules opti
    Corpus("big-js", "input/semgrep.yml", "input/big-js"),
    # Some nodejsscan bench
    Corpus(
        "njs-box", "input/njsscan/njsscan/rules/semantic_grep", "input/dropbox-sdk-js"
    ),
    # Run old nodejsscan rules on vulnerable app (was in calculate_ci_perf.py)
    Corpus(
        "njs-old-juice", "input/njsscan/njsscan/rules/semantic_grep", "input/juice-shop"
    ),
    # Run our r2c-ci and r2c-security audit packs on a python/JS repo
    Corpus("netflix", "input/rules", "input/lemur"),
    # Run our r2c-ci and r2c-security audit packs on a JS/other repo
    Corpus("draios", "input/rules", "input/sysdig-inspect"),
    # Run our ruby rulepack on a large ruby repo
    Corpus("rails", "input/ruby.yml", "input/rails"),
    # (Gitlab medium) Run our python and flask packs on a python repo
    Corpus("django", "input/rules", "input/django"),
    # (Gitlab medium) Run our r2c-ci and r2c-security audit packs on a java repo
    Corpus("dropwizard", "input/rules", "input/dropwizard"),
]

# By default, these will not run
LARGE_CORPUSES = [
    #
    # Run Ajin's nodejsscan rules on some repo containing javascript files.
    # This takes something like 4 hours or more. Maybe we could run it
    # on fewer targets.
    #
    Corpus(
        "njs-juice", "input/njsscan/njsscan/rules/semantic_grep", "input/juice-shop"
    ),
    # (Gitlab large) Run our javascript and r2c-security audit packs on a js/ruby repo
    Corpus("gitlab", "input/rules", "input/gitlab"),
    # (Gitlab large) Run our security-audit pack on a c repo
    Corpus("smacker", "input/r2c-security-audit.yml", "input/gotree"),
    # (Gitlab large) Run our java pack on a java repo
    Corpus("spring-projects", "input/java.yml", "input/spring"),
    # (Gitlab medium) Run our r2c-ci and r2c-security audit packs on a java repo
    Corpus("pmd", "input/rules", "input/pmd"),
]

DUMMY_CORPUSES = [Corpus("dummy", "input/dummy/rules", "input/dummy/targets")]

# For corpuses that cannot be run in CI because they use private repos
INTERNAL_CORPUSES = [
    Corpus("dogfood", "input/semgrep.yml", "input/"),
]

# Due to the semgrep-gitlab partnership, we want to benchmark the
# speed of the rules gitlab runs on repos they have suggested
GITLAB_CORPUSES = [
    # Semgrep-app
    Corpus("semgrep-app", "../gitlab-rules", "input/semgrep-app"),
    # Gitlab small
    Corpus("pallets", "../gitlab-rules", "input/flask"),
    # Gitlab small
    Corpus("socketio", "../gitlab-rules", "input/socket"),
    # (Gitlab medium) python repo
    Corpus("django", "../gitlab-rules", "input/django"),
    # (Gitlab medium) java repo
    Corpus("dropwizard", "../gitlab-rules", "input/dropwizard"),
    # (Gitlab large) js/ruby repo
    Corpus("gitlab", "../gitlab-rules", "input/gitlab"),
    # (Gitlab large) c repo
    Corpus("smacker", "../gitlab-rules", "input/gotree"),
    # (Gitlab large) java repo
    Corpus("spring-projects", "../gitlab-rules", "input/spring"),
    # (Gitlab medium) java repo
    Corpus("pmd", "../gitlab-rules", "input/pmd"),
]


class SemgrepVariant:
    def __init__(self, name: str, semgrep_core_extra: str, semgrep_extra: str = ""):
        # name for the input corpus (rules and targets)
        self.name = name

        # space-separated extra arguments to pass to semgrep-core
        # command via SEMGREP_CORE_EXTRA environment variable
        self.semgrep_core_extra = semgrep_core_extra

        # space-separated extra arguments to pass to the default semgrep
        # command
        self.semgrep_extra = semgrep_extra


# Feel free to create new variants. The idea is to use the default set
# of options as the baseline and we see what happens when we enable or
# disable this or that optimization.
#
SEMGREP_VARIANTS = [
    # default settings
    SemgrepVariant(STD, ""),
    # removing optimisations
    SemgrepVariant("no-cache", "-no_opt_cache"),
    SemgrepVariant("max-cache", "-opt_max_cache"),
    SemgrepVariant("no-bloom", "-no_bloom_filter"),
    SemgrepVariant("no-gc-tuning", "-no_gc_tuning"),
    # alternate optimisations
    SemgrepVariant("set_filters", "-set_filter"),
    SemgrepVariant("experimental", "-no_filter_irrelevant_rules", "--optimizations"),
    SemgrepVariant(
        "experimental_and_fast", "-filter_irrelevant_rules", "--optimizations"
    ),
]

# For when you just want to test a single change
STD_VARIANTS = [SemgrepVariant(STD, "")]

GITLAB_VARIANTS = [
    SemgrepVariant(STD, ""),
    SemgrepVariant("experimental", "-no_filter_irrelevant_rules", "--optimizations"),
]

# This class allows us to put semgrep results in a set and compute set
# differences while saving the original JSON dictionary
class SemgrepResult:
    def __init__(self, dict: dict) -> None:
        self.res = dict

        # We use self.str to compare dicts, so change this
        # to abstract away differences
        if "extra" in dict:
            dict2 = copy.deepcopy(dict)
            # TODO: full-rule does not correctly update message with metavars
            if "message" in dict2["extra"]:
                dict2["extra"]["message"] = ""
            # TODO: spacegrep.py calls dedent() on lines (not sure why)
            if "lines" in dict2["extra"]:
                dict2["extra"]["lines"] = ""
            if "metavars" in dict2["extra"]:
                # TODO: spacegrep/../Semgrep.ml uses a different unique_id
                for _, v in dict2["extra"]["metavars"].items():
                    if "unique_id" in v:
                        v["unique_id"] = ""
                # TODO: core_runner.py dedup_output() depends on the order
                # of the elements in the list to remove similar findings
                # but with different metavars
                dict2["extra"]["metavars"] = ""

            self.str = json.dumps(dict2, sort_keys=True)
        else:
            self.str = json.dumps(dict, sort_keys=True)

    def __eq__(self, other: object) -> bool:
        if not isinstance(other, SemgrepResult):
            raise NotImplementedError

        return self.str == other.str

    def __lt__(self, other: object) -> bool:
        if not isinstance(other, SemgrepResult):
            raise NotImplementedError

        return self.str > other.str

    def __hash__(self) -> int:
        return hash(self.str)


# Add support for: with chdir(DIR): ...
@contextmanager
def chdir(dir: str) -> Iterator[None]:
    old_dir = os.getcwd()
    os.chdir(dir)
    try:
        yield
    finally:
        os.chdir(old_dir)


def upload(url: str, value: str) -> None:
    print(f"Uploading to {url}")
    r = urllib.request.urlopen(  # nosem
        url=url,
        data=value.encode("ascii"),
    )
    print(r.read().decode())


def upload_result(
    variant_name: str, metric_name: str, value: float, timings: dict
) -> None:
    # Upload overall runtime of the benchmark
    metric_url = f"{DASHBOARD_URL}/api/metric/{metric_name}"
    upload(metric_url, str(value))

    if variant_name == STD and timings is not None:
        # Compute bps from semgrep timing data
        assert "rules" in timings
        assert "total_time" in timings
        assert "total_bytes" in timings
        num_rules = len(timings["rules"])
        bps = timings["total_bytes"] / (timings["total_time"] * num_rules)

        bps_url = f"{DASHBOARD_URL}/api/metric/{BPS_ENDPOINT}"
        upload(bps_url, str(bps))

        # Similarly, compute lps
        assert "targets" in timings
        num_lines = 0
        for target in timings["targets"]:
            assert "path" in target
            with open(target["path"]) as f:
                try:
                    num_lines += sum(1 for _ in f)
                except UnicodeDecodeError:
                    pass
        lps = num_lines / (timings["total_time"] * num_rules)

        lps_url = f"{DASHBOARD_URL}/api/metric/{LPS_ENDPOINT}"
        upload(lps_url, str(lps))

        # Upload timing data as a json
        print(f"Uploading timing data to {STATS_URL}")
        headers = {"content-type": "application/json"}
        r = requests.post(
            STATS_URL, data=json.dumps(timings), headers=headers, timeout=30
        )
        print(r.content)


def standardize_findings(findings: dict, include_time: bool) -> Tuple[dict, dict]:
    if "errors" not in findings:
        msg = json.dumps(findings, indent=4) + "\n\nDid not find expected key 'errors'"
        raise Exception(msg)
    if "results" not in findings:
        msg = json.dumps(findings, indent=4) + "\n\nDid not find expected key 'results'"
        raise Exception(msg)
    if include_time and "time" not in findings:
        msg = json.dumps(findings, indent=4) + "\n\nDid not find expected key 'time'"
        raise Exception(msg)
    results = {
        "errors": findings["errors"],
        "results": {SemgrepResult(i) for i in findings["results"]},
    }
    timings = findings["time"] if include_time else None
    return results, timings


def output_differences(
    findings: set, std_findings: set, variant: str
) -> Tuple[int, int]:
    def output_diff(diff: set) -> None:
        for d in sorted(diff):
            print(json.dumps(d.res, sort_keys=True, indent=4))

    f_diff = findings.difference(std_findings)
    s_diff = std_findings.difference(findings)
    fd_len = len(f_diff)
    sd_len = len(s_diff)
    print("In", variant, "but not std", fd_len, "findings :")
    output_diff(f_diff)
    print("In std but not", variant, sd_len, "findings :")
    output_diff(s_diff)
    return fd_len, sd_len


def run_semgrep(
    docker: str, corpus: Corpus, variant: SemgrepVariant, include_time: bool
) -> Tuple[float, bytes]:
    args = []
    common_args = [
        "--strict",
        "--json",
        "--timeout",
        "0",
        "--verbose",
        "--no-git-ignore",  # because files in bench/*/input/ are git-ignored
    ]
    if docker:
        # Absolute paths are required by docker for mounting volumes, otherwise
        # they end up empty inside the container.
        args = [
            "docker",
            "run",
            "-v",
            os.path.abspath(corpus.rule_dir) + ":/rules",
            "-v",
            os.path.abspath(corpus.target_dir) + ":/targets",
            "-t",
            docker,
            "--config",
            "/rules",
            "/targets",
        ]
    else:
        # Absolute paths for rules and targets are required by semgrep
        # when running within the semgrep docker container.
        args = [
            "semgrep",
            "--config",
            os.path.abspath(corpus.rule_dir),
            os.path.abspath(corpus.target_dir),
        ]
    args.extend(common_args)
    if variant.semgrep_extra != "":
        args.extend([variant.semgrep_extra])
    if include_time:
        args.extend(["--time"])

    print(f"current directory: {os.getcwd()}")
    print("semgrep command: {}".format(" ".join(args)))
    os.environ["SEMGREP_CORE_EXTRA"] = variant.semgrep_core_extra
    print(f"extra arguments for semgrep-core: '{variant.semgrep_core_extra}'")

    t1 = time.time()
    res = subprocess.run(args, capture_output=True)  # nosem
    t2 = time.time()

    status = res.returncode
    print(f"semgrep exit status: {status}")
    if status == 0:
        print("success")
    elif status == 3:
        print("warning: some files couldn't be parsed")
    else:
        print("************* Semgrep stdout *************")
        print(res.stdout)
        print("************* Semgrep stderr *************")
        print(res.stderr)
        res.check_returncode()

    return t2 - t1, res.stdout


def run_benchmarks(
    docker: str,
    dummy: bool,
    small_only: bool,
    all: bool,
    internal: bool,
    gitlab: bool,
    std_only: bool,
    filter_corpus: str,
    filter_variant: str,
    plot_benchmarks: bool,
    upload: bool,
    include_time: bool,
    summary_file_path: str,
    called_dir: str,
) -> None:

    variants = SEMGREP_VARIANTS
    if std_only:
        variants = STD_VARIANTS
    if filter_variant:
        variants = [x for x in variants if re.search(filter_variant, x.name) != None]

    results_msgs = []
    durations = ""
    results: dict = {variant.name: [] for variant in variants}

    corpuses = SMALL_CORPUSES + MEDIUM_CORPUSES
    if dummy:
        corpuses = DUMMY_CORPUSES
    if internal:
        corpuses = INTERNAL_CORPUSES
    if gitlab:
        corpuses = GITLAB_CORPUSES
        variants = GITLAB_VARIANTS
    if small_only:
        corpuses = SMALL_CORPUSES
    if all:
        corpuses = SMALL_CORPUSES + MEDIUM_CORPUSES + LARGE_CORPUSES
    if filter_corpus:
        corpuses = [x for x in corpuses if re.search(filter_corpus, x.name) != None]

    for corpus in corpuses:
        with chdir(corpus.name):
            corpus.prep()
            std_findings = {}
            for variant in variants:

                # Run variant
                name = ".".join(["semgrep", "bench", corpus.name, variant.name])
                metric_name = ".".join([name, "duration"])
                print(f"------ {name} ------")
                duration, findings_bytes = run_semgrep(
                    docker, corpus, variant, include_time
                )

                # Report results
                msg = f"{metric_name} = {duration:.3f} s"
                print(msg)
                results_msgs.append(msg)
                durations += f"{duration:.3f}\n"
                results[variant.name].append(duration)

                findings, timings = standardize_findings(
                    json.loads(findings_bytes), include_time
                )

                if upload:
                    upload_result(variant.name, metric_name, duration, timings)

                # Check correctness
                num_results = len(findings["results"])
                num_errors = len(findings["errors"])
                print(f"Result: {num_results} findings, {num_errors} parse errors")

                if variant.name == STD:
                    std_findings = findings
                elif findings["results"] ^ std_findings["results"] != set():
                    fd_len, sd_len = output_differences(
                        findings["results"], std_findings["results"], variant.name
                    )
                    results_msgs[
                        -1
                    ] += f" ERROR: {fd_len} extra findings, {sd_len} missing findings"
                elif len(findings["errors"]) > len(std_findings["errors"]):
                    results_msgs[-1] += " WARNING: more errors than std"

    # Show summary data
    print("\n".join(results_msgs))

    if summary_file_path:
        with chdir(called_dir):
            summary_file = open(summary_file_path, "w+")
            summary_file.write(durations)
            summary_file.close()

    if plot_benchmarks:
        import matplotlib.pyplot as plt
        import pandas as pd

        indexes = [corpus.name for corpus in corpuses]
        plotdata = pd.DataFrame(results, index=indexes)
        plotdata.plot(kind="bar")
        plt.show()


def main() -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--docker",
        metavar="DOCKER_IMAGE",
        type=str,
        help="use the specified docker image for semgrep, such as returntocorp/semgrep:develop",
    )
    parser.add_argument(
        "--dummy",
        help="run quick, fake benchmarks for development purposes",
        action="store_true",
    )
    parser.add_argument(
        "--internal",
        help="run internal benchmarks for development purposes",
        action="store_true",
    )
    parser.add_argument(
        "--gitlab",
        help="run gitlab benchmarks for development purposes",
        action="store_true",
    )
    parser.add_argument(
        "--small-only",
        help="only run small benchmarks (20s per run or less)",
        action="store_true",
    )
    parser.add_argument(
        "--all",
        help="run all benchmarks (takes >1 day)",
        action="store_true",
    )
    parser.add_argument(
        "--std-only", help="only run the default semgrep", action="store_true"
    )
    parser.add_argument(
        "--filter-corpus",
        metavar="REGEXP",
        type=str,
        help="run the corpus only if it satisfies the regexp (e.g., 'std|exp')",
    )
    parser.add_argument(
        "--filter-variant",
        metavar="REGEXP",
        type=str,
        help="run the variant only if it satisfies the regexp (e.g., 'dr.*') ",
    )
    parser.add_argument(
        "--upload", help="upload results to semgrep dashboard", action="store_true"
    )
    parser.add_argument(
        "--save-to",
        metavar="FILE_NAME",
        type=str,
        help="save timing summary to the file given by the argument",
    )
    parser.add_argument(
        "--plot-benchmarks",
        help="display a graph of the benchmark results",
        action="store_true",
    )
    parser.add_argument(
        "--semgrep-core", help="run semgrep-core benchmarks", action="store_true"
    )
    parser.add_argument("--no-time", help="disable time-checking", action="store_true")
    args = parser.parse_args()

    cur_dir = os.path.dirname(os.path.abspath(__file__))
    called_dir = os.getcwd()
    with chdir(cur_dir + "/bench"):
        if args.semgrep_core:
            semgrep_core_benchmark.run_benchmarks(args.dummy, args.upload)
        else:
            run_benchmarks(
                args.docker,
                args.dummy,
                args.small_only,
                args.all,
                args.internal,
                args.gitlab,
                args.std_only,
                args.filter_corpus,
                args.filter_variant,
                args.plot_benchmarks,
                args.upload,
                not args.no_time,
                args.save_to,
                called_dir,
            )


if __name__ == "__main__":
    main()
