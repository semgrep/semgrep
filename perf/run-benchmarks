#! /usr/bin/env python3
#
# Run semgrep on a series of pairs (rules, repo) with different options,
# and report the time it takes. Optionally upload the results to the semgrep
# dashboard.
#
# With the --semgrep_core option, instead run semgrep-core on a series of
# pairs (rules, repo) with options chosen to test semgrep-core performance.
# Note that semgrep-core can currently be run for one language only, so
# these benchmarks only include corpuses that are primarily one language.
# This allows them to be compared to the semgrep runtimes. Can also upload
# the results to the dashboard, and use a dummy set instead
#
import argparse
import os
import subprocess
import time
import urllib.request
from contextlib import contextmanager
from typing import Iterator

import semgrep_core_benchmark  # type: ignore

DASHBOARD_URL = "https://dashboard.semgrep.dev"

# Run command and propagate errors
def cmd(*args: str) -> None:
    subprocess.run(args, check=True)  # nosem


class Corpus:
    def __init__(self, name: str, rule_dir: str, target_dir: str):
        # name for the input corpus (rules and targets)
        self.name = name

        # folder containing the semgrep rules
        self.rule_dir = rule_dir

        # folder containing the target source files
        self.target_dir = target_dir

    # Fetch rules and targets is delegated to an ad-hoc script named 'prep'.
    def prep(self) -> None:
        cmd("./prep")


CORPUSES = [
    # Run Ajin's nodejsscan rules on some repo containing javascript files.
    # This takes something like 4 hours or more. Maybe we could run it
    # on fewer targets.
    # Corpus("njs", "input/njsscan/njsscan/rules/semantic_grep", "input/juice-shop"),
    Corpus("big-js", "input/semgrep.yml", "input/big-js"),
    Corpus(
        "njsbox", "input/njsscan/njsscan/rules/semantic_grep", "input/dropbox-sdk-js"
    ),
    Corpus("zulip", "input/semgrep.yml", "input/zulip"),
    # The tests below all run r2c rulepacks (in r2c-rules) on public repos
    # For command Corpus("$X", ..., "input/$Y"), you can find the repo by
    # going to github.com/$X/$Y
    #
    # Run our django rulepack on a large python repo
    Corpus("apache", "input/django.yml", "input/libcloud"),
    # Run our flask rulepack on a python repo
    Corpus("dropbox", "input/flask.yml", "input/pytest-flakefinder"),
    # Run our r2c-ci and r2c-security-audit packs on a go/ruby repo
    Corpus("coinbase", "input/rules", "input/bifrost"),
    # Run our r2c-ci and r2c-security audit packs on a python/JS repo
    Corpus("netflix", "input/rules", "input/lemur"),
    # Run our r2c-ci and r2c-security audit packs on a JS/other repo
    Corpus("draios", "input/rules", "input/sysdig-inspect"),
    # Run our golang rulepack on a go/html repo
    Corpus("0c34", "input/golang.yml", "input/govwa"),
    # Run our ruby rulepack on a large ruby repo
    Corpus("rails", "input/ruby.yml", "input/rails"),
    # Run our javascript and eslint-plugin-security packs on a large JS repo
    Corpus("lodash", "input/rules", "input/lodash"),
]

DUMMY_CORPUSES = [Corpus("dummy", "input/dummy/rules", "input/dummy/targets")]

# Projects GitLab is using to benchmark us on perf
GITLAB_CORPUSES = [
    # (Gitlab large) Run our javascript and r2c-secuirty audit packs on a js/ruby repo
    Corpus("gitlab", "input/rules", "input/gitlab"),
    # (Gitlab large) Run our security-audit pack on a c repo
    Corpus("smacker", "input/r2c-security-audit.yml", "input/gotree"),
    # (Gitlab large) Run our java pack on a java repo
    Corpus("spring-projects", "input/java.yml", "input/spring"),
    # (Gitlab medium) Run our python and flask packs on a python repo
    Corpus("django", "input/rules", "input/django"),
    # (Gitlab medium) Run our r2c-ci and r2c-security audit packs on a java repo
    Corpus("pmd", "input/rules", "input/pmd"),
    # (Gitlab medium) Run our r2c-ci and r2c-security audit packs on a java repo
    Corpus("dropwizard", "input/rules", "input/dropwizard"),
    # (Gitlab small) Run our python and flask rules on a python repo
    Corpus("pallets", "input/rules", "input/flask"),
    # (Gitlab small) Run our javascript rules on a JS repo
    Corpus("socketio", "input/javascript.yml", "input/socket"),
]

# For corpuses that cannot be run in CI because they use private repos
INTERNAL_CORPUSES = [
    Corpus("dogfood", "input/semgrep.yml", "input/"),
]


class SemgrepVariant:
    def __init__(self, name: str, semgrep_core_extra: str, semgrep_extra: str = ""):
        # name for the input corpus (rules and targets)
        self.name = name

        # space-separated extra arguments to pass to semgrep-core
        # command via SEMGREP_CORE_EXTRA environment variable
        self.semgrep_core_extra = semgrep_core_extra

        # space-separated extra arguments to pass to the default semgrep
        # command
        self.semgrep_extra = semgrep_extra


# Feel free to create new variants. The idea is to use the default set
# of options as the baseline and we see what happens when we enable or
# disable this or that optimization.
#
SEMGREP_VARIANTS = [
    # default settings
    SemgrepVariant("std", ""),
    SemgrepVariant("no-cache", "-no_opt_cache"),
    SemgrepVariant("max-cache", "-opt_max_cache"),
    SemgrepVariant("no-bloom", "-no_bloom_filter"),
    SemgrepVariant("no-gc-tuning", "-no_gc_tuning"),
    SemgrepVariant("experimental", "-fast", "--experimental"),
]

# Add support for: with chdir(DIR): ...
@contextmanager
def chdir(dir: str) -> Iterator[None]:
    old_dir = os.getcwd()
    os.chdir(dir)
    try:
        yield
    finally:
        os.chdir(old_dir)


def upload_result(metric_name: str, value: float) -> None:
    url = f"{DASHBOARD_URL}/api/metric/{metric_name}"
    print(f"Uploading to {url}")
    r = urllib.request.urlopen(  # nosem
        url=url,
        data=str(value).encode("ascii"),
    )
    print(r.read().decode())


def run_semgrep(docker: str, corpus: Corpus, variant: SemgrepVariant) -> float:
    args = []
    common_args = [
        "--strict",
        "--timeout",
        "0",
        "--verbose",
        "--no-git-ignore",  # because files in bench/*/input/ are git-ignored
    ]
    if docker:
        # Absolute paths are required by docker for mounting volumes, otherwise
        # they end up empty inside the container.
        args = [
            "docker",
            "run",
            "-v",
            os.path.abspath(corpus.rule_dir) + ":/rules",
            "-v",
            os.path.abspath(corpus.target_dir) + ":/targets",
            "-t",
            docker,
            "--config",
            "/rules",
            "/targets",
        ]
    else:
        # Absolute paths for rules and targets are required by semgrep
        # when running within the semgrep docker container.
        args = [
            "semgrep",
            "--config",
            os.path.abspath(corpus.rule_dir),
            os.path.abspath(corpus.target_dir),
        ]
    args.extend(common_args)
    if variant.semgrep_extra != "":
        args.extend([variant.semgrep_extra])

    print(f"current directory: {os.getcwd()}")
    print("semgrep command: {}".format(" ".join(args)))
    os.environ["SEMGREP_CORE_EXTRA"] = variant.semgrep_core_extra
    print(f"extra arguments for semgrep-core: '{variant.semgrep_core_extra}'")

    t1 = time.time()
    res = subprocess.run(args)  # nosem
    t2 = time.time()

    status = res.returncode
    print(f"semgrep exit status: {status}")
    if status == 0:
        print("success")
    elif status == 3:
        print("warning: some files couldn't be parsed")
    else:
        res.check_returncode()

    return t2 - t1


def run_benchmarks(
    docker: str, dummy: bool, gitlab: bool, internal: bool, upload: bool
) -> None:
    results = []
    corpuses = CORPUSES
    if dummy:
        corpuses = DUMMY_CORPUSES
    if internal:
        corpuses = INTERNAL_CORPUSES
    if gitlab:
        corpuses = GITLAB_CORPUSES
    for corpus in corpuses:
        with chdir(corpus.name):
            corpus.prep()
            for variant in SEMGREP_VARIANTS:
                name = ".".join(["semgrep", "bench", corpus.name, variant.name])
                metric_name = ".".join([name, "duration"])
                print(f"------ {name} ------")
                duration = run_semgrep(docker, corpus, variant)
                msg = f"{metric_name} = {duration:.3f} s"
                print(msg)
                results.append(msg)
                if upload:
                    upload_result(metric_name, duration)
    for msg in results:
        print(msg)


def main() -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--docker",
        metavar="DOCKER_IMAGE",
        type=str,
        help="use the specified docker image for semgrep, such as returntocorp/semgrep:develop",
    )
    parser.add_argument(
        "--dummy",
        help="run quick, fake benchmarks for development purposes",
        action="store_true",
    )
    parser.add_argument(
        "--internal",
        help="run internal benchmarks for development purposes",
        action="store_true",
    )
    parser.add_argument(
        "--gitlab",
        help="run gitlab benchmarks for development purposes",
        action="store_true",
    )
    parser.add_argument(
        "--upload", help="upload results to semgrep dashboard", action="store_true"
    )
    parser.add_argument(
        "--semgrep_core", help="run semgrep-core benchmarks", action="store_true"
    )
    args = parser.parse_args()
    with chdir("bench"):
        if args.semgrep_core:
            semgrep_core_benchmark.run_benchmarks(args.dummy, args.upload)
        else:
            run_benchmarks(
                args.docker, args.dummy, args.gitlab, args.internal, args.upload
            )


if __name__ == "__main__":
    main()
